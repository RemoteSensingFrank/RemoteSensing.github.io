<!DOCTYPE html><html lang="zh-CN"><head><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description" content="随感，遥感，机器学习....想到什么写什么"><title>tensorflow-二十八弹 | 吴蔚</title><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/normalize/5.0.0/normalize.min.css"><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/pure/0.6.2/pure-min.css"><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/pure/0.6.2/grids-responsive-min.css"><link rel="stylesheet" type="text/css" href="/css/style.css?v=0.0.0"><link rel="stylesheet" href="//cdn.bootcss.com/font-awesome/4.7.0/css/font-awesome.min.css"><script type="text/javascript" src="//cdn.bootcss.com/jquery/3.1.1/jquery.min.js"></script><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.ico"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/apple-touch-icon.png"><link rel="alternate" type="application/atom+xml" href="/atom.xml"></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">tensorflow-二十八弹</h1><a id="logo" href="/.">吴蔚</a><p class="description">生命不息，折腾不止！</p></div><div id="nav-menu"><a href="/." class="current"><i class="fa fa-home"> 首页</i></a><a href="/archives/"><i class="fa fa-archive"> 归档</i></a><a href="/about/"><i class="fa fa-user"> 关于</i></a><a href="/atom.xml"><i class="fa fa-rss"> 订阅</i></a></div></div><div id="layout" class="pure-g"><div class="pure-u-1 pure-u-md-3-4"><div class="content_container"><div class="post"><h1 class="post-title">tensorflow-二十八弹</h1><div class="post-meta">Aug 22, 2018<span> | </span><span class="category"><a href="/categories/学习/">学习</a></span><script src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js" async></script><span id="busuanzi_container_page_pv"> | <span id="busuanzi_value_page_pv"></span><span> Hits</span></span></div><div class="post-content"><p>&nbsp;&nbsp;&nbsp;&nbsp;上一次的学习过程中介绍了RNN,然后列举了它的流程以及理解，然后运行代码试了一下，有了一个直观的体会，但是代码的逻辑实际上还是比较奇怪，而且列出的公式也不直观，实际上在27弹中所提到了公式是进行简化了的公式，所以看起来会比较复杂一些，这次在深入理解之后重新梳理了一下，然后把公式串起来。  </p>
<center><img src="https://blogimage-1251632003.cos.ap-guangzhou.myqcloud.com/rnn.JPG"></center><br> &nbsp;&nbsp;&nbsp;&nbsp;还是上面这张图，这张图其实描述得有点含糊，为了更清晰得说明情况我再加上一张图进行说明：<br> <center><img src="https://timgsa.baidu.com/timg?image&quality=80&size=b9999_10000&sec=1546757744614&di=4926294d374c6a407cf7834d94e55872&imgtype=0&src=http%3A%2F%2Fcrawler-fs.intsig.net%2Fcamfs%2Fdownload%3Ffilename%3D10005_d3ec5886a98e02c3a4077f619ac99734_4.gif"></center>  

<p> &nbsp;&nbsp;&nbsp;&nbsp;上图来自网络，<a href="wuweiBlog.com">侵删联系</a>。把两张图结合起来看就比较好了，实际上对于一个序列输入$[x_1,x_2…x_n]$来说再训练得时候其实不必要关心每次得输入对应得输出，我们更关心的是最后的输入和输出结果。所以对于每一步实际上最重要的是中间层，从两个图中我们可以得到中间层的计算方法,为了统一我们公式表示都参考第一张图，其中$s_t$为中间层：<br>$$<br>s_t=f(W<em>s_{t-1}+U</em>x_t+b)(1)<br>$$<br>&nbsp;&nbsp;&nbsp;&nbsp;公式(1)中$s_t$为当前状态的隐含层，$W$为上一个隐含层到当前隐含层的权重，$U$为输入到隐含层的权重。这样我们就得到了从输入到隐含层的计算方式，然后我们从隐含层到输出：<br>$$<br>o_t=g(V*s_t+c)(2)<br>$$<br>&nbsp;&nbsp;&nbsp;&nbsp;在公式(2)中$o_t$为输出结果，$V$为输入到输出的权重，然后我们就得到了整个卷积神经网络的经典公式。对比上次给出的公式：<br><strong>二十七弹中的公式为</strong>：</p>
<p>$$<br>\begin{aligned}<br>&amp;S_t=f(W(X<em>t@ S</em>{t-1})+b)(3)\<br>&amp;O_t=g(US_t,+c)(4)<br>\end{aligned}<br>$$<br>&nbsp;&nbsp;&nbsp;&nbsp;从比较中可以看出实际上主要的差别集中在(1)和(3)上，实际上在二十七弹中给出的公式是简化公式，我们可以把式(1)写成如下形式:<br>$$<br>s_t=f([W,U]\cdot[s_{t-1},x_t]’+b))(5)<br>$$<br>&nbsp;&nbsp;&nbsp;&nbsp;对于式(5)其实就是将$W$与$U$合并为一个矩阵再将$s_{t-1},x_t$合并，其中$@$符号就是矩阵合并的符号，由于python矩阵合并计算相对比较简单因此采用简化公式能够减小编码量，但是却增大的理解的负担；实际上序列不可能无限长，因此每次都是截取一定长度的序列进行计算，由此产生了step参数。<br>&nbsp;&nbsp;&nbsp;&nbsp;实际上经典的RNN已经了解了，但是在应用过程中RNN存在很多变化比如以下几种图的变化：<br> <center><img src="https://pic1.zhimg.com/v2-6caa75392fe47801e605d5e8f2d3a100_r.jpg"><br> 图1.多个输入对应一个输出<br> <img src="https://pic3.zhimg.com/80/v2-87ebd6a82e32e81657682ffa0ba084ee_hd.jpg"><br> 图2.一个输入对应多个输出<br>  <img src="https://pic4.zhimg.com/80/v2-77e8a977fc3d43bec8b05633dc52ff9f_hd.jpg"><br>图3.自编码解码器<br> </center><br> &nbsp;&nbsp;&nbsp;&nbsp;以上三种是RNN中除了输入与输出等长之外最常见的三种模式了，实际上前两种都比较简单，对于多个输入对应一个输出的模式，只需要在最后一个输出上对应进行变换就好了。而对于多个输出对应一个输入的只需要对最开始的输入进行变换。整个结构没有变化过程也比较简单，单独把最后或者第一次的输入或者输出抽取出来就好了；对于多个输入对应一个输出的情况一般都是在语义的识别中，比如输入一个语句判断其情感倾向等。一个输入对应多个输出情况就比较多，比如输入第一画自动绘制，输入描述自动生成文字图片等都属于这一类。第三类是最重要的一个部分，我们叫自编码器；实际上就是根据输入得到一个输入的编码对于不同的输入都能够得到统一的编码形式使得输入数据能够统一，另外编码也能够包含输入的所有信息。编码的形式有很多种，可以直接输出最后一个隐藏状态，也可以对最后一个隐藏状态进行变换或者对所有隐藏状态进行变换得到结果。得到这个结果我们就认为是对序列输入的编码结果，而过程中的参数实际上就是编码器。有编码过程就一定有一个解码过程，解码过程就是由c得到各个输出的过程。解码实际上也是一个RNN网络，这个网络的结构就是图2中提到的多个输入对应一个输出的结构。然后训练出一个网络。这样就得到了两个网络，这一组网络我们就称为编码-解码器。编码器的优点在于不对输入和输出的长度进行限制，因此被广泛的应用。<a href="https://zhuanlan.zhihu.com/p/28054589" target="_blank" rel="noopener">参考内容</a><br> &nbsp;&nbsp;&nbsp;&nbsp;通过这次学习解决了上次遗留的一些问题，同时对RNN的结构进行了归纳，同时对编码器进行了理解，实际上通过这个结构对自编码器就能够很好的理解了。通过学习编码形式得到输出和其本身对接近的编码器和解码器从而实现对数据的抽象，对于小样本的学习来说通过自编码的形式能够对数据进行更高层次的抽象，在此基础上通过少量的样本可以达到很好的学习效果。</p>
</div><script type="text/javascript" src="/js/share.js?v=0.0.0" async></script><a data-url="http://www.wuweiblog.com/2018/08/22/tensorflow-二十八弹/" data-id="cjqki2co60020eonn1lnj01oc" class="article-share-link">分享到</a><div class="tags"><a href="/tags/tensorflow学习/">tensorflow学习</a></div><div class="post-nav"><a href="/2018/08/22/tensorflow-二十七弹/" class="pre">tensorflow-二十七弹</a><a href="/2018/08/13/tensorflow-二十六弹/" class="next">tensorflow-二十六弹</a></div></div></div></div><div class="pure-u-1-4 hidden_mid_and_down"><div id="sidebar"><div class="widget"><form action="//www.google.com/search" method="get" accept-charset="utf-8" target="_blank" class="search-form"><input type="text" name="q" maxlength="20" placeholder="Search"/><input type="hidden" name="sitesearch" value="http://www.wuweiblog.com"/></form></div><div class="widget"><div class="widget-title"><i class="fa fa-folder-o"> 分类</i></div><ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/书评/">书评</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/图像处理/">图像处理</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/学习/">学习</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/小说/">小说</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/影评/">影评</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/数学/">数学</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/游戏/">游戏</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/算法/">算法</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/随感/">随感</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-star-o"> 标签</i></div><div class="tagcloud"><a href="/tags/openMVG-openMVS学习/" style="font-size: 15px;">openMVG openMVS学习</a> <a href="/tags/ArcGIS环境配置/" style="font-size: 15px;">ArcGIS环境配置</a> <a href="/tags/linux-学习/" style="font-size: 15px;">linux 学习</a> <a href="/tags/学习/" style="font-size: 15px;">学习</a> <a href="/tags/数学/" style="font-size: 15px;">数学</a> <a href="/tags/图像处理数学原理/" style="font-size: 15px;">图像处理数学原理</a> <a href="/tags/效率/" style="font-size: 15px;">效率</a> <a href="/tags/系统架构-学习/" style="font-size: 15px;">系统架构 学习</a> <a href="/tags/算法/" style="font-size: 15px;">算法</a> <a href="/tags/机器学习/" style="font-size: 15px;">机器学习</a> <a href="/tags/机器学习，图像处理/" style="font-size: 15px;">机器学习，图像处理</a> <a href="/tags/tensorflow学习/" style="font-size: 15px;">tensorflow学习</a> <a href="/tags/随感/" style="font-size: 15px;">随感</a> <a href="/tags/沉默的大多数，书评/" style="font-size: 15px;">沉默的大多数，书评</a> <a href="/tags/Mary-and-Max，影评/" style="font-size: 15px;">Mary and Max，影评</a> <a href="/tags/断舍离，书评/" style="font-size: 15px;">断舍离，书评</a> <a href="/tags/将夜-书评/" style="font-size: 15px;">将夜,书评</a> <a href="/tags/未来简史-书评/" style="font-size: 15px;">未来简史,书评</a> <a href="/tags/潜规则-书评/" style="font-size: 15px;">潜规则,书评</a> <a href="/tags/呼兰河传-书评/" style="font-size: 15px;">呼兰河传,书评</a> <a href="/tags/雪中悍刀行-书评/" style="font-size: 15px;">雪中悍刀行,书评</a> <a href="/tags/你好疯子，影评/" style="font-size: 15px;">你好疯子，影评</a> <a href="/tags/图像处理/" style="font-size: 15px;">图像处理</a> <a href="/tags/开发/" style="font-size: 15px;">开发</a> <a href="/tags/喜剧之王，影评/" style="font-size: 15px;">喜剧之王，影评</a> <a href="/tags/The-Legend-of-1900/" style="font-size: 15px;">The Legend of 1900</a> <a href="/tags/随感-摄影测量/" style="font-size: 15px;">随感-摄影测量</a> <a href="/tags/随感，毕业/" style="font-size: 15px;">随感，毕业</a> <a href="/tags/校正方法，控制点，光束法平差/" style="font-size: 15px;">校正方法，控制点，光束法平差</a> <a href="/tags/linux学习/" style="font-size: 15px;">linux学习</a> <a href="/tags/月亮与六便士/" style="font-size: 15px;">月亮与六便士</a> <a href="/tags/电影十二公民/" style="font-size: 15px;">电影十二公民</a> <a href="/tags/小说/" style="font-size: 15px;">小说</a> <a href="/tags/V字仇杀队-浪潮，影评/" style="font-size: 15px;">V字仇杀队,浪潮，影评</a> <a href="/tags/随感－代码重构/" style="font-size: 15px;">随感－代码重构</a> <a href="/tags/R-学习/" style="font-size: 15px;">R 学习</a> <a href="/tags/爱乐之城，影评/" style="font-size: 15px;">爱乐之城，影评</a> <a href="/tags/狗子日记/" style="font-size: 15px;">狗子日记</a> <a href="/tags/一个叫欧维的男人决定去死，书评/" style="font-size: 15px;">一个叫欧维的男人决定去死，书评</a> <a href="/tags/星际穿越，影评/" style="font-size: 15px;">星际穿越，影评</a> <a href="/tags/图像处理的数学原理/" style="font-size: 15px;">图像处理的数学原理</a> <a href="/tags/社交网络，影评/" style="font-size: 15px;">社交网络，影评</a> <a href="/tags/秒速五厘米/" style="font-size: 15px;">秒速五厘米</a> <a href="/tags/饥荒/" style="font-size: 15px;">饥荒</a> <a href="/tags/白日梦想家，影评/" style="font-size: 15px;">白日梦想家，影评</a> <a href="/tags/海涛之声，影评/" style="font-size: 15px;">海涛之声，影评</a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-file-o"> 最新文章</i></div><ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/2019/01/06/猝死/">猝死</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/12/31/我有两盏灯/">我有两盏灯</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/12/31/concul服务注册与发现/">concul服务注册与发现</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/12/24/原来不如此/">原来不如此</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/12/17/关于激光点云数据处理/">关于激光点云数据处理</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/12/08/工作狂日记/">工作狂日记</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/11/26/我曾经历过沧海桑田/">我曾经历过沧海桑田</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/11/11/kd树的构建/">kd树的构建</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/11/08/选择/">选择</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/10/31/不读书(七)/">不读书(七)</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-external-link"> 友情链接</i></div><ul></ul><a href="https://github.com/RemoteSensingFrank" title="Github" target="_blank">Github</a><ul></ul><a href="http://www.example2.com/" title="site-name2" target="_blank">site-name2</a><ul></ul><a href="http://www.example3.com/" title="site-name3" target="_blank">site-name3</a></div></div></div><div class="pure-u-1 pure-u-md-3-4"><div id="footer">© <a href="/." rel="nofollow">吴蔚.转载请注明</a></div></div></div><a id="rocket" href="#top" class="show"></a><script type="text/javascript" src="/js/totop.js?v=0.0.0" async></script><script type="text/javascript" src="//cdn.bootcss.com/fancybox/2.1.5/jquery.fancybox.pack.js" async></script><script type="text/javascript" src="/js/fancybox.js?v=0.0.0" async></script><link rel="stylesheet" type="text/css" href="/css/jquery.fancybox.css?v=0.0.0"><script>(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
ga('create','UA-107160167-1','auto');ga('send','pageview');
</script><script type="text/x-mathjax-config">MathJax.Hub.Config({
  tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
  });
</script><script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML" async></script><script type="text/javascript" src="/js/codeblock-resizer.js?v=0.0.0"></script><script type="text/javascript" src="/js/smartresize.js?v=0.0.0"></script></div></body></html>