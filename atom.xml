<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>吴蔚</title>
  
  <subtitle>生命不息，折腾不止！</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://www.wuweiblog.com/"/>
  <updated>2018-12-08T10:25:50.898Z</updated>
  <id>http://www.wuweiblog.com/</id>
  
  <author>
    <name>John Doe Thanks the author of the theme</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>工作狂日记</title>
    <link href="http://www.wuweiblog.com/2018/12/08/%E5%B7%A5%E4%BD%9C%E7%8B%82%E6%97%A5%E8%AE%B0/"/>
    <id>http://www.wuweiblog.com/2018/12/08/工作狂日记/</id>
    <published>2018-12-08T10:22:45.000Z</published>
    <updated>2018-12-08T10:25:50.898Z</updated>
    
    <content type="html"><![CDATA[<p>&nbsp;&nbsp;&nbsp;&nbsp;有一天回家的时候突然情绪有些失控，不知道是不是因为连续工作时间太长，感觉情绪已经快到崩溃的边缘，感觉对什么事情都提不起兴趣。总是感觉在自我肯定和否定中挣扎。其实对自己进来的工作还是比较满意的，就是对自己的生活不太满意，感觉已经失去了自己的生活。每天想着能够练i练琴，或者写点东西，但是回家就变成了玩手机，刷知乎，然后躺在沙发上一觉睡到第二天凌晨。这几周以来在沙发上睡觉的时间远比在床上睡的多，穿着衣服睡觉的时间远比脱掉衣服睡觉的时间多，很多时候都是躺在沙发上玩着手机然后就睡着了。<br>&nbsp;&nbsp;&nbsp;&nbsp;我曾经以为自己是一个自律的人，我觉得我能够很好的管控住自己的欲望，但是其实在自律方面我好像也只是一个普通人，也会被一些快餐的电视剧，一些无聊的视频吸引 ，然后因为这些东西花费大量的时间。所以我很好奇，这些东西的吸引力究竟在什么地方，为什么我不能回家之后坐在书桌前做些能够让自己获取更大满足的事情，而是把时间花在这些东西上。这大概就是看了很多书也 过不好自己一生的典型吧，关于心理学的书看了不少，虽然也有些收获，但是具体到实践上就相差十万八千里了，有的问题并不是能够意识到就能够控制的，这大概就是能力的边界吧。我很喜欢能力的边界这个词，这个词能够让我对自己很清晰的认识而不会盲目追求完美，而实际上我是一个很想要追求完美的人，总是想着能够把事情做到最好，但是最好到底有多好其实没有一个很好的解释，把事情能够做完美的可能性几乎为0，所以总是需要探索自我能力的边界，在自我能力的边界范围内尽力把事情做好，把自己管控好。另外总是有一种恐慌的感觉，也许是看到太多的牛人，总是感觉相差太远，而自己又是一个不太愿意承认不如别人的人，所以一直很有压力，永远感觉自己不够好的滋味实际上并不好受，总是会在自我肯定和自我否定的情绪中挣扎。<br>&nbsp;&nbsp;&nbsp;&nbsp;前几天坐在公交车上发呆，感觉做什么事情都提不起兴趣，感觉自己感到满足或者不满足的阈值都很高，好像没有能够让自己感觉到特别开心的事情，当然也没有让自己感觉到特别不开心的事情，好像好的坏的都这样了，处于一个很消极的状态。其实每隔一段时间就会有这样的感觉，不过这一次来的比较强烈，尤其是在连续加班回家的路上。驻地开发实际上不是一个太愉快的体验，早上没有早餐吃，中午也不能好好休息，在繁华的市中心总是有总疏离的感觉，好像在时刻提醒着自己原来我不属于这里。其实以一个旁观者的态度来体验繁华的都市挺有意思，看着人群匆匆忙忙从眼前走过，仿佛时间被加速了，而我却在时间之外。整个工作的进度还算顺利，该做的事情也都能够保证完成，感觉除我以外大家都挺羡慕来这边的，其实说起我不太愿意来这边驻地好像大家有些不可思议，可能我更愿意简单点的生活，我不想每天思考吃什么好，该去哪个店吃，我好像对衣食住行都没有什么太大的要求，可能对我来说衣食住行只是保证生存的手段而不是最后的目标，相比之下我更在意自我的提升，我会因为练了一首新曲子而开心，我会因为练字有了成效而兴奋，我会因为写了自己满意的代码而满足。我更在意自己本身而不是因为衣食住行给我带来的满足，所以我喜欢每天完全不用思考的吃食堂，按部就班的搭最早的那一班公交到公司。实际上我的精力很有限，不太愿意在这些事情是上花太多心思，而且这些似乎也不是我擅长的事情。<br>&nbsp;&nbsp;&nbsp;&nbsp;其实我并不是一个典型的工作狂，可能最多算得上是非典型的工作狂吧，当然也可能是因为能力不够无法在工作时间内把工作做到让自己满意才被迫加班，但是我觉得工作的目的并不仅仅是挣一份工钱而已，我可以不在意自己到底工作了多长时间，我也不在意需要付出多大的努力才能把事情做好，我更想知道的是工作是否能够对于自己有提升，如果有，我愿意付出更大的努力去掌握和了解每一项技能，花时间去解决每一个问题。我不在意需要付出多大的努力才能变得牛逼，我害怕的是慢慢的自己变成了一个傻逼。 </p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;有一天回家的时候突然情绪有些失控，不知道是不是因为连续工作时间太长，感觉情绪已经快到崩溃的边缘，感觉对什么事情都提不起兴趣。总是感觉在自我肯定和否定中挣扎。其实对自己进来的工作还是比较满意的，就是对自己的生活不太满意，感觉已经
      
    
    </summary>
    
    
      <category term="随感" scheme="http://www.wuweiblog.com/tags/%E9%9A%8F%E6%84%9F/"/>
    
  </entry>
  
  <entry>
    <title>我曾经历过沧海桑田</title>
    <link href="http://www.wuweiblog.com/2018/11/26/%E6%88%91%E6%9B%BE%E7%BB%8F%E5%8E%86%E8%BF%87%E6%B2%A7%E6%B5%B7%E6%A1%91%E7%94%B0/"/>
    <id>http://www.wuweiblog.com/2018/11/26/我曾经历过沧海桑田/</id>
    <published>2018-11-26T15:21:12.000Z</published>
    <updated>2018-12-08T10:26:08.745Z</updated>
    
    <content type="html"><![CDATA[<p>&nbsp;&nbsp;&nbsp;&nbsp;有些时候心里有很多 想法但是没有来得及记录下来，等到正襟危坐打开电脑想要记录一下的时候已经有些来不及 了，肥宅最近就深有体会。可能是老猪在他这里呆久了，他也染上了老猪的好吃懒做的毛病，曾经肥宅也是一个比较自律的人，具体的自律体现在吃顿饭从来不超过二十，每天晚上一定要上床睡觉，每天至少能留出一点时间来想姑娘。和老猪呆在一起时间长了之后肥宅发现自己腰不酸了，床不睡了并且连姑娘都不再想了，对此肥宅表示很难过，连姑娘都不想的生活不是他想要的生活，所以他决定找老猪谈谈，当然这么正式的会谈仪式感还是很重要的，所以肥宅特意买了几只猪蹄还有两斤猪头肉和几瓶啤酒，这天老猪看到桌上丰盛的小吃吓得瑟瑟发抖，这是肥宅叫过老猪说：“老猪有没有时间，我有点事想跟你谈谈。”老猪本能的想要拒绝，但是看到肥宅坚定的眼神以及桌上看起来很好吃的猪头肉，情不自禁的点点头，就这样肥宅和猪开启了他们跨越人生与梦想的长谈。<br>&nbsp;&nbsp;&nbsp;&nbsp;肥宅跟猪说：“老猪，最近受你的影响，我发现自己的生活习惯很不好了呀，床也不睡了，姑娘也不想了，你是不是要背这个锅呀！”当然实际上肥宅知道这不是猪的锅，只是他自己似乎已经缺少了些什么东西，但是他是不会承认的。如果不是对于自己不在乎的东西有谁能轻易承认自己的匮乏与无能呢，他想，所以他要给自己找个理由来证明这个不是他自己的原因，此时猪就是最好的背锅侠了。听到这话猪很惊讶：“小老弟呀！你这个锅甩很好呀，每天下班回来躺在沙发刷手机刷到睡着的锅我老猪可不背，至于不去想姑娘的事情，怪我咯？不过说句心里话就你这样，别说姑娘了，母猪可能都看不上你，每天邋里邋遢，目光呆滞毫无亮点，不想姑娘是对的，就算想了估计也是白想。你能不能像我老猪一样每天弄的人模猪样的，出门不知道多少母猪对我抛媚眼，你看看我肥硕腱子肉，你知道我在圈内猪称行走的荷尔蒙，所以说小老弟呀搞清楚重点很重要呀。”肥宅听了老猪的话只能苦笑，行走的荷尔蒙什么的他不知道是不是真的，但是桌上的猪头肉很好吃是真真实实的。看到肥宅吃猪头肉吃的这么开心老猪不禁感到菊花一紧，然后又觉得好像不太对为啥吃猪头肉自己会要菊花一紧，不过这些都不是重点。肥宅听了老猪的话已经陷入了沉思。其实肥宅骨子里是一个自私的人，他所有的努力所有的坚持并不是为了取悦别人，仅仅是为了让自己得到提升，所以很多时候他并不太在乎别人的看法和意见，当然也不存在想要获得别人的关注和吸引其他人的目的，这其实谈不上是一件好事，当然也谈不上是一件坏事，不过可能肥宅的心智不够坚定，所以偶尔会感觉到孤独，偶尔会觉得迷茫。这时他想起了一句话’”他没有选择，他所走的是一条荆棘丛生的道路，他所走的每一步都无比艰难，但是这是一条最坚定和踏实的道路。”也许肥宅现在正在走一条这样的道路吧。想通了这个问题他也不再去纠结关于母猪和姑娘的问题了，但是他对老猪的人生一直很好奇，究竟是一个什么样的环境才能养出一只这样特立独行的猪呢？所以他问猪说：“老猪，说说你自己呗，在来我这里之前你过的是怎么样！”说罢连猪头肉也不吃了，认真的看着猪，听到这话猪觉得自己装逼的机会又来了，正准备高谈阔论一番，这时肥宅幽幽说到：“你这次要是再敢胡扯我就把你送去屠宰场阉掉，所以你说话最好走心。”听到这番话猪吓得差点小便失禁，看来装逼不成了，然后猪就陷入了沉思。猪曾经呆过很多地方，遇到过很多人，有温润如君子的普通人，有狡猾奸诈的普通人，有不善言辞的普通人，有口若悬河的普通人，有美貌光彩照人的普通人，也有长相平平的普通人。这又是一个很长的故事了…</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;有些时候心里有很多 想法但是没有来得及记录下来，等到正襟危坐打开电脑想要记录一下的时候已经有些来不及 了，肥宅最近就深有体会。可能是老猪在他这里呆久了，他也染上了老猪的好吃懒做的毛病，曾经肥宅也是一个比较自律的人，具体的自律体
      
    
    </summary>
    
      <category term="随感" scheme="http://www.wuweiblog.com/categories/%E9%9A%8F%E6%84%9F/"/>
    
    
      <category term="随感" scheme="http://www.wuweiblog.com/tags/%E9%9A%8F%E6%84%9F/"/>
    
  </entry>
  
  <entry>
    <title>kd树的构建</title>
    <link href="http://www.wuweiblog.com/2018/11/11/kd%E6%A0%91%E7%9A%84%E6%9E%84%E5%BB%BA/"/>
    <id>http://www.wuweiblog.com/2018/11/11/kd树的构建/</id>
    <published>2018-11-11T14:04:33.000Z</published>
    <updated>2018-11-11T15:27:17.977Z</updated>
    
    <content type="html"><![CDATA[<p>&nbsp;&nbsp;&nbsp;&nbsp;在这个普天同庆的光棍节的大日子默默的睡了个天昏地暗，然后爬起来写了个专利。实际上写专利的时候又突然搞懂了KD树的构建算法，果然工作使我快乐，以前的时候一直在用KD树，不过都是用的开源的库，所谓的混合索引方式也只是在开源库的基础上进行代码的改造，对于原理的理解还是不够深入，今天在写专利的时候重新梳理了一次觉得终于弄得有点明白了。<br>&nbsp;&nbsp;&nbsp;&nbsp;下面上一张图来说明一下这个过程：<br><img src="https://blogimage-1251632003.cos.ap-guangzhou.myqcloud.com/%E4%BA%8C%E7%BB%B4%E7%A9%BA%E9%97%B4KD%E6%A0%91.jpg">  </p><p>&nbsp;&nbsp;&nbsp;&nbsp;上图是二维空间KD树的划分过程，首先确定划分是从哪一个维度进行，是从x维还是从y维，一般确定维度的过程是计算数据集在各个维度的方差或标准差，从方差或标准差大的维度开始计算进行划分。确定维度后按照该维度进行排序取中位数将点集划分为两个部分，如线1所示，进行第一次划分之后对左右两个点集进行第二次划分如线2所示，然后依次进行划分直到所有点都只属于某一个划分。上图是对于二维点的一个划分，实际上对于高维的点也差不多，由于高维画起来比较麻烦就懒得画了。在解决划分和构造的问题的基础上我们就需要问，如果构造好了一棵树，怎么进行最近邻查询，这个就是我们下面需要讨论的问题。<br>&nbsp;&nbsp;&nbsp;&nbsp;我们用一幅图来展示整个搜索过程：</p><p><img src="https://blogimage-1251632003.cos.ap-guangzhou.myqcloud.com/%E4%BA%8C%E7%BB%B4%E7%A9%BA%E9%97%B4KD%E6%A0%91%E6%90%9C%E7%B4%A2.jpg"><br>以上就是我们的最近邻的搜索过程,红色的五角星为目标点，首先计算目标点到根节点的距离，构成一个超球，判断各个区域是否与超球有相交，通过判断可知P8与P11所在空间与超球没有交集因此忽略这两个部分，然后计算与P3和P9的距离，与根节点的距离比较，如果小于根节点的距离则缩小这个超球，为点到P3的距离，据此可以忽略整个P6右侧所有节点，以及P4节点所在空间；然后遍历剩下的节点，找到最近的节点P1。<br>参考资料：<br><a href="https://leileiluoluo.com/posts/kdtree-algorithm-and-implementation.html" target="_blank" rel="noopener">https://leileiluoluo.com/posts/kdtree-algorithm-and-implementation.html</a><br><a href="https://www.cnblogs.com/earendil/p/8135074.html" target="_blank" rel="noopener">https://www.cnblogs.com/earendil/p/8135074.html</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;在这个普天同庆的光棍节的大日子默默的睡了个天昏地暗，然后爬起来写了个专利。实际上写专利的时候又突然搞懂了KD树的构建算法，果然工作使我快乐，以前的时候一直在用KD树，不过都是用的开源的库，所谓的混合索引方式也只是在开源库的基础
      
    
    </summary>
    
      <category term="算法" scheme="http://www.wuweiblog.com/categories/%E7%AE%97%E6%B3%95/"/>
    
    
      <category term="算法" scheme="http://www.wuweiblog.com/tags/%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>选择</title>
    <link href="http://www.wuweiblog.com/2018/11/08/%E9%80%89%E6%8B%A9/"/>
    <id>http://www.wuweiblog.com/2018/11/08/选择/</id>
    <published>2018-11-07T22:43:38.000Z</published>
    <updated>2018-11-08T04:44:09.000Z</updated>
    
    <content type="html"><![CDATA[<p>&nbsp;&nbsp;&nbsp;&nbsp;前两天我公司带我的师父打电话问我有没有想要去她们那里的意愿，其实我是有些动心的毕竟目前形势看起来她们那里比我们这里似乎会好一点，当然也只是看起来，毕竟我也没有过去待过，不过大概率 的这么说应该是没有问题。这两天都没有怎么睡好其实想的问题很多，其实主要也是针对自己未来的一个去向或者对自己发展的看法。其实自己一直以来都有些焦虑，这也不是第一次机会了，不过相比于上一次，这一次的机会确实让我心动。一直有同事或者领导跟我说要确定自己的核心竞争力，其实来公司经历了这么几个领导也经历了不同的领导风格。实际上我在想，所谓的核心竞争力大概就是帮助客户把事情搞定的能力吧，不管是自己能够做好也好，能够进行人员的安排和管理也好，或者是能写代码也好，终究的目的就是把事情做好而已。那么实际上一个人确实是没有办法既负责项目的沟通和安排又能够做好软件的设计甚至是编码的工作。所以其实核心能力也分为很多部分。商务，管理，视野以及技术能力都是核心竞争力的一个部分，可是几乎没有人能够在各个方面都能够做好，或者说即使有人能够在各个方面都能够做好也会有一个比较优势（托了听了几节经济学课的福），所以我们势必会放弃发展某一些方面来加强我们其他方面的核心竞争力。<br>&nbsp;&nbsp;&nbsp;&nbsp;读书读了这么多 年，工作才一年半不到。其实感觉自己的思想还停留在将自己的技术实力定义为核心竞争力的阶段。其实我喜欢数学，喜欢算法，喜欢从技术的角度解决问题。愿意就具体的问题进行分析，但是不得不说，在很多时候其实技术并不能成为关键问题，或者说在极少的领域或在极少的时候技术水平才会成为关键因素。所以有时候看到吹水真的会很心塞，但是也不得不昧着良心去吹水，我都怀疑自己会不会以后吹水就不昧着良心了，就能够毫无心理负担的吹水了！<br>&nbsp;&nbsp;&nbsp;&nbsp;想想自己工作的这一段时间，除了一些杂事意外正儿八经的接触的项目有三个饿，一个是香港项目，这个项目实际上确实是赶鸭子上架，本来只要做一个开心的码农，安安心心的写代码然后按时干完活，按时上下班就好了的，结果硬着头皮去接了本来应该我师父去干的活，既要自己写代码又需要负责研发管理与甲方和乙方沟通，实际上工作并不算太累，毕竟自己只承担小部分的编码工作，但是当时压力确实很大，生怕自己做不好会造成巨大的损失。当然在项目执行的过程中也有很多的失误，也犯了一些错误，算是磕磕绊绊做完了一个项目；第二个江门的项目现在还处于执行阶段，本来以为是一个很简单的项目，但是实际上现在也开始变得复杂了。因为这个项目我从头开始负责的研发，同时也管理了三个研发人员（虽然是实习生，但是我认为并没有什么区别），从我的角度来说做的并不好。上了Restful API的风格，做了前后端分离，前端单页面实现。整体设计上没有太大的问题，主要的问题在于对于数据流的设计和理解不够深入以至于一直在做修修补补的工作。另外虽然大的方向的设计上没有问题，但是具体实现细节上还有很多东西值得更深入的探讨。第三个是珠海这个，这个项目实际上才启动，但是这个项目在前期做的准备要比江门项目好，在实施上我也有信心能够做的更好，这个项目应该算我成熟期的第一个项目吧，所如果不做好总是有点心慌，不知道自己是不是能够独立去进行项目的管理和架构的设计；另一方面又还是有点心血白费了的感觉。<br>&nbsp;&nbsp;&nbsp;&nbsp;在床上还是有点不舒服，不过总结了一下这一年来的工作经验之后似乎我已经确定了自己的选择，虽然我的选择可能不是最优解（即使目前看起来似乎也不是最优解）但是我想每个人总是有自己坚持的东西吧，总是要坚持一些与地位，或者与收入无关的东西。否则我们这一生不就成为了名利的奴隶了么！</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;前两天我公司带我的师父打电话问我有没有想要去她们那里的意愿，其实我是有些动心的毕竟目前形势看起来她们那里比我们这里似乎会好一点，当然也只是看起来，毕竟我也没有过去待过，不过大概率 的这么说应该是没有问题。这两天都没有怎么睡好其
      
    
    </summary>
    
      <category term="书评" scheme="http://www.wuweiblog.com/categories/%E4%B9%A6%E8%AF%84/"/>
    
    
      <category term="随感" scheme="http://www.wuweiblog.com/tags/%E9%9A%8F%E6%84%9F/"/>
    
  </entry>
  
  <entry>
    <title>不读书(七)</title>
    <link href="http://www.wuweiblog.com/2018/10/31/%E4%B8%8D%E8%AF%BB%E4%B9%A6(%E4%B8%83)/"/>
    <id>http://www.wuweiblog.com/2018/10/31/不读书(七)/</id>
    <published>2018-10-30T22:43:38.000Z</published>
    <updated>2018-11-04T02:08:13.000Z</updated>
    
    <content type="html"><![CDATA[<p>&nbsp;&nbsp;&nbsp;&nbsp;这是一本关于收纳的书，很多时候也被人将断舍离的方法用于面对生活中的其他事情，如工作、情感以及抉择等等。其实看书名就能猜出是日本人写的书，一般来说国人写书不会用这样的名字。其实是一本很有意思的书，日本作为一个空间比较狭小的国家，实际上人均居住面积是严重不足的，因此对于生活空间有很强烈的需求，从而引起了许多收纳法的流行这也是可以理解的。实际上对于我们来说可能居住空间相对较大另外物质生活也相对来说没有这么丰富，因此需求可能没有这么强烈，不过如果处在大城市需求会强烈很多。<br>&nbsp;&nbsp;&nbsp;&nbsp;其实我不是一个会执着于外物的人，所以也没有什么生活物品是无法割舍或者说因为各种原因不愿意舍弃掉的，对于我来说断与舍其实是很容易做到的，另外关于离我得好好说道说道，从这个离字其实看出了慢慢的禁欲系风格。不管收纳得多好，舍弃的多么果断实际上如果没有能控制自己的欲望则很难做到真正的自由。我认识一些朋友，不断的分手然后开始新的恋情，然后分手然后又开始。他们能够很容易割舍上一段感情，但是内心的对于情感欲望的渴求促使着他们马上又进入新的恋情以填补内心的空白，实际上就如同我们的空房子，或者空空如也的书桌，如果总是想着填满房间，摆满书桌，不管能够多么果断的断舍，片刻之后又是一片狼藉。<br>&nbsp;&nbsp;&nbsp;&nbsp;看完书后有一天在去公司的车上，我突然会想，到底我们所谓的满足感与快乐到底是个什么意思。精神自嗨和享受华服珍馐到底那一个能给我们带来更大的满足感或者更好的体验？我们享受更好的服务，买更好品质的衣服，开更好的车，住更大的房子，这些东西所给我们带来的究竟是买他们的一瞬间或者说买他们之后一小段时间内让我们内心满足，还是能够给我们长久的满足？其实我们对于物质生活所能承受的弹性范围是很大的，从受资本主义的苦到享社会主义的福（手动滑稽）其实我们都是能够接受的。高品质的物质生活确实能给我们带来满足和快乐，但是我更愿意相信对于物质生活的变化我们不管是从生理上还是精神上都能够很快适应，然后习以为常，而这些所给我们带来的满足感也会逐渐下降。所以<br>如果说我们所有的体验都是大脑多巴胺分泌刺激的结果，那么实际上做爱和嗑药才是最快乐的事情吧，那做爱到死或者嗑药到死不就是快乐到死么？但是如果可以选择的话我想大概率的人不会在正值芳年的时候选择通过这种方式享受到死吧。<br>&nbsp;&nbsp;&nbsp;&nbsp;其实享受并没有什么不好，无论是物质上的还是精神上的享受，实际上产生的效果都是相同的。但是沉迷于这种简单的胜利生理刺激所带来的快乐是不好的一件事情。性善和性恶之争自古就存在了，我个人比较偏向于性恶论，对于我们个人来说当然是希望能够吃的更好，能够穿的更好，能够睡更软的床，如果可以也希望睡更好看的姑娘（或者是汉子），然后我们不断的丰富我们的大脑，我们发现享受这些能够给我们带来的快乐，或者说享受，其实是很有限的，只是在一定的时期内起到一定的作用。所以在我看来为了获得更加持久的快乐，我们就需要学会割断一些看似能够给我们带来很大的快乐，实际上背后却是无尽空虚的东西。<br>&nbsp;&nbsp;&nbsp;&nbsp;所以说到底，其实所谓的断舍离，立足点还是这个离，如果真的能够离了自己的欲望，那么断和舍也都是自然而然的事情了。然而克制或者说控制自己的欲望其实是一件挺困难的事情，首先我们需要能够深入的剖析和了解自己，然后把自己真正喜爱的东西从那些混杂了虚荣，欲望以及他人期许的需求中抽离，然后将其舍弃。看了这本书，也有很多感悟，当然最重要的是从现在做起，管理好自己，收拾好自己的欲望，中恐怕比家里物品的收纳要来的更加的重要。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;这是一本关于收纳的书，很多时候也被人将断舍离的方法用于面对生活中的其他事情，如工作、情感以及抉择等等。其实看书名就能猜出是日本人写的书，一般来说国人写书不会用这样的名字。其实是一本很有意思的书，日本作为一个空间比较狭小的国家，
      
    
    </summary>
    
      <category term="书评" scheme="http://www.wuweiblog.com/categories/%E4%B9%A6%E8%AF%84/"/>
    
    
      <category term="断舍离，书评" scheme="http://www.wuweiblog.com/tags/%E6%96%AD%E8%88%8D%E7%A6%BB%EF%BC%8C%E4%B9%A6%E8%AF%84/"/>
    
  </entry>
  
  <entry>
    <title>Cesium搭建自己的GIS服务器</title>
    <link href="http://www.wuweiblog.com/2018/10/28/Cesium%E6%90%AD%E5%BB%BA%E8%87%AA%E5%B7%B1%E7%9A%84GIS%E6%9C%8D%E5%8A%A1%E5%99%A8/"/>
    <id>http://www.wuweiblog.com/2018/10/28/Cesium搭建自己的GIS服务器/</id>
    <published>2018-10-28T01:14:15.000Z</published>
    <updated>2018-10-28T02:07:45.590Z</updated>
    
    <content type="html"><![CDATA[<p>&nbsp;&nbsp;&nbsp;&nbsp;虽然是用GIS的路线，但是最近接触了很多其他的GIS研发公司，然后找同学也了解了一下目前行业内的研发情况；另外也有大神向我推荐了一些开源库，目前来说主要就是Cesium这个前端的GIS开源框架。所以也去了解了一下，对于我来说使用开源框架最大的问题在于学习成本，但是从另一个方面来说，由于对国产软件的支持力度日益提高，使用ESRI的平台也可能会面临各种各样的问题，因此我觉得对于各种平台的研究与支持我们是有备无患，在这样的背景之下我去了解了一下Cesium的前端GIS框架，下面主要谈谈基于这个GIS框架搭建一个GIS系统的思路以及一些尝试。  </p><ol><li>首先是数据的支持，对于一个平台来说最大问题在于其对数据的支持，如果能够对各种数据都进行很好的支持那么这个平台就是具有一定潜力的，当然如果连常见的数据格式都不能支持，那么这个平台的生命力一定不会太强，因此我们第一个考虑的就是数据支持的问题，为了了解对于各种数据格式的支持我们查看了其示例代码，同时也针对一些格式进行了测试，其示例代码的<a href="https://cesiumjs.org/Cesium/Build/Apps/Sandcastle/" target="_blank" rel="noopener">网站</a>上有对各种数据格式的支持情况，从网站上我们可以看到，实际上Cesium对于各种数据都是能够比较好的支持的，但是需要转换为它所定义的<strong>3DTiles</strong>的标准，关于<strong>3DTiles</strong>以后如果有时间再细聊下面展示一下我们自己的测试情况：<br><img src="https://blogimage-1251632003.cos.ap-guangzhou.myqcloud.com/cesium%E7%82%B9%E4%BA%91.JPG"><br>上面展示的是分类点云的情况，实际上还有三位模型的加载情况以及KML的加载情况由于截图比较麻烦就不截图进行展示了。</li><li>除了对于所支持的数据格式有要求之外，在实际应用中对于所支持的数据量的大小也是有要求的，如果只能支持少量的数据那么在实际应用中只能作为一个展示的平台，不具有太大的意义，所以能加载的数据量的大小作为一个极其重要的性能指标也需要被考虑。目前我并没有对可加载的数据量的大小进行测试，了解其官网的Demo可以粗略的了解到通过Cesium加载10亿级别的点云是没有问题的，但是这个数据量的加载需要进行测试，另外在加载的过程中一般来说会混合加载多种格式的数据，相比于加载单一格式的数据，多种格式数据同时加载对于平台的压力要远远大于加载同一个格式的数据，因此在测试过程中也需要对平台进行进一步的测试。</li><li>平台工具的使用，实际了解到Cesium是通过一种叫做3DTiles的数据格式来加载数据的，那么对于我们常见的如模型，点云以及倾斜等数据格式，都需要转换为3DTiles的格式，现在面临的最主要的问题是没有现成的工具对各种格式的转换进行很好的支持。通过3DTiles格式的说明我们可以自己编写代码进行转换，但是对于模型和倾斜数据，由于其格式较多且数据格式较为复杂，自己写的转换方式不一定能够适用于所有模型，当然目前也有一些开源的转换代码，但是对于这些转换代码没有进行测试，对其性能以及转换后的数据是否能够成功加载也存在一些疑问，需要进行进一步的测试。第3点问题是开源平台存在的主要问题，相比于成熟的商业平台，开源平台存在着生态不足，且由于开源数据格式多样造成的格式统一的困难。由于这些困难的存在导致开源平台的学习成本很高。</li><li>平台API功能的稳定性以及功能是否完善，关于这个问题我并没有深入的了解，但是做一个球并进行简单的操作是没有问题的，至于更加深入的功能可能需要更进一步的摸索。 </li></ol><p>&nbsp;&nbsp;&nbsp;&nbsp;其实了解完了以上几个问题我们对于这个平台就有一定的了解了，实际上cesium作为一个开源的三位GIS平台是具有很大的潜力的，如果我们自己需要搭建一个功能不复杂的系统可以考虑使用。那么用一个开源的前端球我们需要考虑的是服务端用什么，GIS后台服务可选的有很多开源的如GeoServer商业的如SuperMap以及ESRI等，但是考虑到成本以及既然用了开源就用到底的精神我觉得可以考虑GeoServer，至于三维数据的后台实际上只有一个比较重要的要求那就是服务器，实际上Cesium可以接受通过服务器发布数据服务然后进行展示，所以如果是纯数据则可以直接通过Nginx或者IIS发布静态数据，但是作为一个服务器可能更重要的是上传数据后进行数据的管理以及自动的转换，从这个角度来看就需要编写后台数据格式转换和后台服务的代码，这个应该是最主要的问题，不过实际上也不算太复杂，如果能够了解到各个格式转换为3DTiles的方法，自己编写代码搭建后台服务器也是可以考虑的。<br>&nbsp;&nbsp;&nbsp;&nbsp;最后一个问题就是关于数据量的问题，实际上一个服务器所能承载的数据是有限的，但是通过Nginx等负载均衡的工具也很容易就搭建出一个负载均衡的服务器，所以数据量的问题也不需要过度担心，总的来说在时间允许的条件下，如果对于功能的要求不是特别高的情况下可以考虑通过Cesium+GeoServer+Nginx的方案搭建一套自己的服务器。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;虽然是用GIS的路线，但是最近接触了很多其他的GIS研发公司，然后找同学也了解了一下目前行业内的研发情况；另外也有大神向我推荐了一些开源库，目前来说主要就是Cesium这个前端的GIS开源框架。所以也去了解了一下，对于我来说使
      
    
    </summary>
    
      <category term="学习" scheme="http://www.wuweiblog.com/categories/%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="开发" scheme="http://www.wuweiblog.com/tags/%E5%BC%80%E5%8F%91/"/>
    
  </entry>
  
  <entry>
    <title>关于PCA变换及其应用的梳理</title>
    <link href="http://www.wuweiblog.com/2018/10/14/%E5%85%B3%E4%BA%8EPCA%E5%8F%98%E6%8D%A2%E5%8F%8A%E5%85%B6%E5%BA%94%E7%94%A8%E7%9A%84%E6%A2%B3%E7%90%86/"/>
    <id>http://www.wuweiblog.com/2018/10/14/关于PCA变换及其应用的梳理/</id>
    <published>2018-10-14T06:20:14.000Z</published>
    <updated>2018-10-14T07:21:09.293Z</updated>
    
    <content type="html"><![CDATA[<p>&nbsp;&nbsp;&nbsp;&nbsp;最近写了篇关于PCA变换应用的文章，主要利用了PCA变换能够将信息集中的特点，通过PCA变换，信息集中在前几个主成分上，通过信息量的差异可以进行分类等操作。也读了一些PCA关于PCA变换应用于其他方面的文章，因此对PCA变换进行一个总结与梳理，以期能够在以后更好的对其进行应用。<br>&nbsp;&nbsp;&nbsp;&nbsp;首先介绍一下PCA变换，PCA变换又称为主成分变换其过程可以看作是对数据的重投影，我们可以简单的将PCA变换理解为一个投影变换，将数据从一个正交空间投影到另一个正交空间的过程。在这个过程中最重要的就是投影的正交基的求解，在这里首先解释一下基向量比较学术的解释是:</p><blockquote><p>给定一个向量空间$V$，若$V$中的一组线性无关向量组$B=[e_1,e_2,e_3…]$，对于$V$中任意向量都可以通过$B$线性表示，可以认为向量组$B$为向量空间$V$的一组基</p></blockquote><p>&nbsp;&nbsp;&nbsp;&nbsp;从上面的定义我们可以了解基向量的特征，当然我们最常见的基向量就是正交基，也就是说一组基不仅线性无关而且正交，关于线性无关和正交的区别在这里就不多做解释了，我们下面通过一个简单的例子说明一个二维空间的两组基<br>$$<br>B_1=\begin{bmatrix}0&amp;1\<br>1&amp;0<br>\end{bmatrix}<br>B_2=\begin{bmatrix}1&amp;1\<br>1&amp;-1<br>\end{bmatrix} （1）<br>$$<br>&nbsp;&nbsp;&nbsp;&nbsp;其中$B_1,B_2$为两组二维空间中的正交基，$B_2$可以看作是$B_1$旋转45°的结果。<br>&nbsp;&nbsp;&nbsp;&nbsp;介绍了基向量之后我们可以对PCA变换进行介绍了，从上面的描述中可得PCA变换实质是一个投影变换，因此我们需要找一个投影方向，也就是在变换空间中找到一组基向量。实际上对于任意一个向量空间都存在无数组基，因此我们需要找的一组基应该存在一些约束条件，对于PCA变换来说其约束条件在于<font face="黑体">按照投影后信息量最大的方向进行投影，投影后各个特征之间线性无关。</font>根据以上要求可以计算投影方向。具体为什么需要计算协方差及其特征向量以及PCA变换的具体计算可以参考<a href="https://www.cnblogs.com/dengdan890730/p/5495078.html" target="_blank" rel="noopener">这里</a>。<br>&nbsp;&nbsp;&nbsp;&nbsp;主要还是要说明一下PCA变换的应用意义：</p><ul><li>变量之间的去相关性；</li><li>找到信息量最大的方向；</li><li>垂直关系；</li></ul><p>实际上以上三个应用方向中前两个是很好想到的，也在很多方面得到了应用，如异常检测，数据压缩和降维以及去噪等。第三个实际上在数据空间关系上应用的比较少但是是很重要的应用，通过垂直关系可以简单的找到空间中与平面垂直的方向。进而可以找到数据所在的拟合平面，在数据的分割等应用中具有重要的意义。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;最近写了篇关于PCA变换应用的文章，主要利用了PCA变换能够将信息集中的特点，通过PCA变换，信息集中在前几个主成分上，通过信息量的差异可以进行分类等操作。也读了一些PCA关于PCA变换应用于其他方面的文章，因此对PCA变换进
      
    
    </summary>
    
      <category term="数学" scheme="http://www.wuweiblog.com/categories/%E6%95%B0%E5%AD%A6/"/>
    
    
      <category term="数学" scheme="http://www.wuweiblog.com/tags/%E6%95%B0%E5%AD%A6/"/>
    
  </entry>
  
  <entry>
    <title>有趣的人</title>
    <link href="http://www.wuweiblog.com/2018/10/05/%E6%9C%89%E8%B6%A3%E7%9A%84%E4%BA%BA/"/>
    <id>http://www.wuweiblog.com/2018/10/05/有趣的人/</id>
    <published>2018-10-05T11:02:23.000Z</published>
    <updated>2018-10-05T12:34:17.644Z</updated>
    
    <content type="html"><![CDATA[<p>&nbsp;&nbsp;&nbsp;&nbsp;有一个肥宅，家里囤积了一冰箱的肥宅快乐水，还有肥宅快乐酒。其实知道肥宅快乐水也好，肥宅快乐酒也好，似乎都不是什么好东西，不过谁在乎呢！肥宅的日子就是这样。<br>&nbsp;&nbsp;&nbsp;&nbsp;曾经有一只又黑又瘦的猪在肥宅家里住过一段时间，看起来像家猪却长出了獠牙。猪刚来家里的时候正是台风过后，肥宅已经饿了两天了，开门看见一只猪排站在门口，自然是喜不自胜一把扯起猪尾巴就把猪拽进来了，看来今天是有一顿大餐了。万万没有想到此时猪发出了杀猪般的嚎叫：“救命呀~杀猪啦~”此处并没有使用拟人的手法，这头猪真的说话了，肥宅吓了一跳连忙把猪放开了。猪得到片刻的喘息，连忙说道：“老弟，有话好好说，我就在你这里接住一段时间，时间一到我就走。”肥宅连忙拒绝：“不行，我家里不养宠物。”这么一说猪就不愿意了“老猪我可不是宠物，你见过宠物说人话的么？”肥宅一想也是挺有道理的，何况本来肥宅一人住也挺无聊，再住进一只猪也并没有什么不好的，所以猪就理所当然的搬了进来。<br>&nbsp;&nbsp;&nbsp;&nbsp;这一天猪与肥宅一起坐在沙发上吹牛逼，猪说：“你信不信，我日过的母猪比你看过的毛片还要多！”，肥宅看着硬盘里8G的种子，摇头表示除非自己是智障否则是坚决不会信的；眼看肥宅没有被自己吹过的牛逼震惊，猪表示不乐意了，拍着肥宅的肩膀说：“伙计，你说你也是一表人才，为啥自甘堕落，宁愿与种子为伍与硬盘为伴？”这番话让肥宅陷入了沉思。<br>&nbsp;&nbsp;&nbsp;&nbsp;不久之前一位自称耶稣的人也在肥宅这里住了一段时间，看到肥宅硬盘中8G的种子，耶稣表示很是同情，另外为了感谢肥宅的感谢耶稣表示要让肥宅成为一个真正的少女杀手，从而摆脱毛片恶魔的魔掌，于是跟肥宅说：“阿宅，为了感谢你的收留，我决定给你一个选择，你是愿意成为一个有趣的人还是一个有钱的人？”肥宅表示很惊讶，真的有这么好的事情么？耶稣表示是的，只是需要你做出选择。听完这番话肥宅想起了自己小时候喜欢的姑娘小花，曾经为了追求小花，肥宅攒了一年的零花钱给小花买了一个小花心仪已久的胸针正兴冲冲的想要送给小花，可是当他把胸针给小花的时候小花却对他说：“小宅，你不需要送我这么贵重的礼物，我是不会喜欢你的，我喜欢的是小强！”肥宅听后心痛不已，质问小花为什么，小花说：“因为小强比你有趣，跟他在一起比跟你一起开心多了！”听完小花的话肥宅颓然的离开了，他本就不是一个有趣的人，从小到大他只是默默的学习，默默的运动，默默的工作，默默的看着日出日落，看着雨点雪花；他不知道什么是有趣，他想也许操场上让小花笑得花枝乱颤的小强是有趣的吧，所以他一直想成为一个有趣的人，成为一个能让小花开心的人。<br>&nbsp;&nbsp;&nbsp;&nbsp;所以他对耶稣说：“我要成为一个有趣的人！”听到他的回答耶稣很惊讶，但是依然点点头然后就离开了，耶稣离开之后肥宅的生活如往常一样，也似乎并没有变得多么有趣。那个骗子一定是不想给我伙食费故意骗我的，肥宅心里想着，不过肥宅也并没有放在心上，直到猪跟他说话他才想起这件事情。所以他问猪：“老猪，你说你是不是一个有趣的人，哦不，有趣的猪？”猪白了他一眼说：“废话，我可是少女猪杀手，可谓阅猪无数了，只要我看上的猪没有不拜倒在我性感的獠牙和鬃毛上的！” “哦，看不出来你这么牛逼呀？那请问一下你是怎样做一只有趣的猪呢？” “这还不简单，跟野外的猪说安定，与圈养的猪说自由，向待宰的猪说来生，十拿九稳！”听话猪的歪理邪说肥宅只当在说笑，并没有放在心上：“喝酒，喝酒！”这晚肥仔喝醉了，醉倒在沙发上，梦里的肥宅又回到了年轻的时候，他帮小花别好胸针，小花被他逗得笑得花枝乱颤，小花笑着跟他说：“小强，你真的很有趣。”肥宅在沙发上翻了个身~</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;有一个肥宅，家里囤积了一冰箱的肥宅快乐水，还有肥宅快乐酒。其实知道肥宅快乐水也好，肥宅快乐酒也好，似乎都不是什么好东西，不过谁在乎呢！肥宅的日子就是这样。&lt;br&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;曾经有一只又黑又
      
    
    </summary>
    
      <category term="随感" scheme="http://www.wuweiblog.com/categories/%E9%9A%8F%E6%84%9F/"/>
    
    
      <category term="随感" scheme="http://www.wuweiblog.com/tags/%E9%9A%8F%E6%84%9F/"/>
    
  </entry>
  
  <entry>
    <title>一切随缘</title>
    <link href="http://www.wuweiblog.com/2018/09/08/%E4%B8%80%E5%88%87%E9%9A%8F%E7%BC%98/"/>
    <id>http://www.wuweiblog.com/2018/09/08/一切随缘/</id>
    <published>2018-09-08T13:34:34.000Z</published>
    <updated>2018-09-09T04:12:42.569Z</updated>
    
    <content type="html"><![CDATA[<p>&nbsp;&nbsp;&nbsp;&nbsp;游完泳回来看了两集破产姐妹，依旧是这么的毁三观，本来想着把实验做一下，顺便把论文写完赶快投出去交差，但是总是没有心情做实验，另外书房的灯好像坏了，只能靠台灯艰难度日了，以前的时候我最喜欢这种漆黑的环境，开一盏台灯，好像回到宿舍里通宵赶代码的日子，但是现在好像不太喜欢这样了，主要是开着台灯弹琴可能看不清谱子，但是感觉弹来弹去也没有什么长进，好像还越来越不会了，着我就很尴尬了不是，另外感觉最近总是想着看手机，想找个人聊聊天，其实也不知道聊什么。<br>&nbsp;&nbsp;&nbsp;&nbsp;最近生活习惯好了很多，晚上一般都睡得挺早的，不知道是因为搬家住的远了，每天早上起得比较早还是因为健身之后太累。昨天的时候在车上看到一个知乎话题，关于自律。今年年初算起，一个人独居有半年了，一开始其实觉得挺好，终于又可以享受一个人时间，又可以默默的躲在房间的角落发呆，或者看书；然后时间慢慢过去，有时候晚上回家又会感觉到很孤单，一种很难描述的孤单，或者我可以将其称为空虚，好像缺了点什么，然后不停的刷着各种社交工具，其实我算是一个社交很少的人，拿起手机，发现不管是微信还是QQ，似乎都没有人找，这么一段时间以来好像在社交App上主动联系我的都是邀请去参加婚礼的…WTF。<br>&nbsp;&nbsp;&nbsp;&nbsp;乱七八糟的东西写了一堆，好像心情也没有什么变化，还是不太想写代码，不太想写文章，不想做实验，不想干所有有意义的事情，只想默默的做些浪费生命的事情把时间打发了……还是想着找个人聊聊天，又拿起手机看了一下，发现一如既往没有消息，所以又失望的放下了。其实我自己都不太愿意去联系别人，有何必指望着人家会联系我呢。着大半年来总是偶尔会有这样的时候，自己一个人在家，默默的弹着琴或者什么都不做就是发呆，很想有人能够找我聊聊天，但是却没有，也不太愿意去找别人聊天，一来怕麻烦别人，二来实际上可能也不会有人喜欢尬聊吧，再说自己负面情绪这么多，如果影响了朋友们也不见得是什么好事。有很多人都觉得我这样是缺一个女朋友，实际上并不是，我好像不太需要一个女朋友这么亲密的关系，而且也很难再建立一段亲密关系。因为自从上次分手后似乎对自我边界的认识越发的清晰了，越发的不想别人介入我自己的生活边界，也不太愿意因为其他人改变生活习惯，就这么随缘吧，反正都是佛系青年，随缘也没有什么不好的。<br>&nbsp;&nbsp;&nbsp;&nbsp;上周末跟@甘甜聊了一下她婚后的带娃生活，实际上自从她结婚之后就很少聊天了，当然一方面是我工作也比较忙，另外也是觉得她太累贸然打扰似乎不太好。听着她讲述自己的生活，我刚开始是很惊讶的，因为在我的印象中我们聊天从来都是当下的苟且以及诗和远方，很少有这些生活中家长里短的事情，但是这次听她说了很多关于生活的事情，这让我仿佛看见了小时候爸妈的生活。我觉得很有意思，同时也坚定了我应该一个人生活的信念，实际上真正的soul mate是极少的，可能遇到的概率比中彩票概率还要低，另外就算是soul mate也可能被两个家庭的琐事把热情消磨得干干净净，所以还是就这样吧，一切随缘。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;游完泳回来看了两集破产姐妹，依旧是这么的毁三观，本来想着把实验做一下，顺便把论文写完赶快投出去交差，但是总是没有心情做实验，另外书房的灯好像坏了，只能靠台灯艰难度日了，以前的时候我最喜欢这种漆黑的环境，开一盏台灯，好像回到宿舍
      
    
    </summary>
    
      <category term="随感" scheme="http://www.wuweiblog.com/categories/%E9%9A%8F%E6%84%9F/"/>
    
    
      <category term="随感" scheme="http://www.wuweiblog.com/tags/%E9%9A%8F%E6%84%9F/"/>
    
  </entry>
  
  <entry>
    <title>一座孤岛</title>
    <link href="http://www.wuweiblog.com/2018/08/30/%E4%B8%80%E5%BA%A7%E5%AD%A4%E5%B2%9B/"/>
    <id>http://www.wuweiblog.com/2018/08/30/一座孤岛/</id>
    <published>2018-08-30T14:39:50.000Z</published>
    <updated>2018-08-30T15:29:18.590Z</updated>
    
    <content type="html"><![CDATA[<p>&nbsp;&nbsp;&nbsp;&nbsp;第一次看这部电影应该是本科的时候，晚上本来在研究RNN，突然有点烦，又把这部剧情舒缓的动画翻出来看看。动画讲述一位澳大利亚女孩儿与一位纽约具有自闭症的中年通过书信往来结下深厚友谊的故事，小女孩儿Mary生活的家庭并不幸福，生活中缺少父亲的陪伴和母亲的关爱，另外由于既不聪明也不可爱，所以从小就没有什么朋友。一次偶然的机会，女孩儿突发奇想，想要给一位笔友写信，于是这封信漂洋过海来到纽约，让Max看到了这封信，由此揭开了他们长达二十年的友情。<br>&nbsp;&nbsp;&nbsp;&nbsp;这是一部黏土动画，没有什么激烈的剧情冲突，只是简单的通过一封封的书信介绍各自的生活以及对生活的看法。通过一封封的书信，我们看到了两个孤独的身影，跨过万水千山，相互鼓励相互安慰。我看到有人说是爱情，实际上却不是这么认为，爱情的力量可以跨越万水千山，也可以相互鼓励和安慰，但是爱情更多的陪伴，是双目对视时眼中溢出的爱意，是房间里留下的某个人的身影，是一日不见如隔三秋的思念。而友情却不是这样，与爱相似，但是与爱无关，可以陪伴，也可以潇洒离开。在这里我不想谈关于友情还是爱情，当我重新看完这一部影片后涌现最深刻的感受是孤独。<br>&nbsp;&nbsp;&nbsp;&nbsp;我们生而孤独，如同身处一座孤岛，这便是我们一生的牢笼，不管是身体上还是心理上，爱情是找到一个愿意踏上你的岛来陪你孤独的人，而友情是在另一座岛上理解你孤独的人。实际上这两种感情中的任何一种都是可遇而不可求的，所以我们不断的遇见，然后不断的忘记，最后发现原来还是只有自己。有时候我回家，坐在书房看着我的琴发呆，或者是关上了所有的灯，坐在客厅沙发上发呆。有时候我会想，如果我这一生就这么过去了，一个人默默的在这黑暗中死去，那该是一件多么悲伤的故事。前一些日子看到日本的一个无缘死亡，会感觉到有一些恐慌；一个人没有伴侣，失去所有亲人与朋友之后独自在自己的住所迎接死亡的到来，这该有多么可怕。实际上我们又注定要迎接死亡的到来，前几天的时候以为舅奶奶去世了，我是才知道消息。在我的印象中她的身体一直都很好的，直到我舅姥爷去世，她的身体便是一日不如一日了。也许是思念，也许是孤独，总之是一日不如一日。所以我会想，如果远在天边有一个人，他能够明白我的孤独，能够分担我的忧伤，分享我的快乐，这是一件多么美好的事情。实际上社交网络与通讯工具的发展让我们距离更近了，却更加疏离了。我想你，所以我给你电话，然后我还是想你，我又给你电话，然后你觉得我很烦，我也觉得自己很烦，所以我便不想你了,失去距离让我们的沟通更加高效，但是我们的思念变得廉价，我们的理解我们的同理心变得一文不值，所以我们变得更加孤独了，我们只想找到一个温暖的怀抱来填满我们空空荡荡的房间，而不是一个美好的灵魂来填满我们空空荡荡的心。<br>&nbsp;&nbsp;&nbsp;&nbsp;电影看完了，实际上还是很羡慕剧中Mary和Max，虽然面对生活的泥潭，但是如果我知道远在天边如果有一个人能够理解我，也在盼着我的消息，想想也应该是让生活无论如何都要继续下去的动力吧。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;第一次看这部电影应该是本科的时候，晚上本来在研究RNN，突然有点烦，又把这部剧情舒缓的动画翻出来看看。动画讲述一位澳大利亚女孩儿与一位纽约具有自闭症的中年通过书信往来结下深厚友谊的故事，小女孩儿Mary生活的家庭并不幸福，生活
      
    
    </summary>
    
      <category term="影评" scheme="http://www.wuweiblog.com/categories/%E5%BD%B1%E8%AF%84/"/>
    
    
      <category term="Mary and Max，影评" scheme="http://www.wuweiblog.com/tags/Mary-and-Max%EF%BC%8C%E5%BD%B1%E8%AF%84/"/>
    
  </entry>
  
  <entry>
    <title>不读书(六)</title>
    <link href="http://www.wuweiblog.com/2018/08/29/%E4%B8%8D%E8%AF%BB%E4%B9%A6(%E5%85%AD)/"/>
    <id>http://www.wuweiblog.com/2018/08/29/不读书(六)/</id>
    <published>2018-08-29T00:09:29.000Z</published>
    <updated>2018-08-29T08:36:12.035Z</updated>
    
    <content type="html"><![CDATA[<p>&nbsp;&nbsp;&nbsp;&nbsp;要是在四五年前看这本书，我可能不会喜欢，现在看起来却是看得我直冒冷汗。其实我不太喜欢潜规则这个词，这个词显得太不正义，总有一种鬼鬼祟祟的感觉，但是不得不承认，这个词的创立简直是对几千年来社会处事准则最准确的归纳和概括。为什么叫潜规则，因为这样的规则是不能放在台面上说的，为什么不能放在台面上说，因为其中涉及了太多人性中不是那么光明伟岸的一面，所以我们本能的对其缄口不言，但是行事中却默许其存在。<br>&nbsp;&nbsp;&nbsp;&nbsp;晶姐推荐的这本书在上周终于看完了，看前半部分的时候会震撼很多，后半部分反而习以为常了，本书全面的介绍了潜规则在中国这个集权社会中的体现，分析了潜规则形成的原因并解释了这么多年来一直存在的理由。整本书以古代官场为背景，分析了古代官场中一系列的潜规则现象。实际上几千年来，我国作为一个中央集权制的国家，国家掌握着最大的资源和话语权，而作为国家代表的官员则是国家资源分配权力的实际掌控者。实际上整个社会阶层呈现一个金字塔状，最底层是数量最多的百姓，然后依次向上是逐级官僚，实际上整个官僚体系都是建立在对农名阶级的剥削基础上。但是为了避免过度剥削而引起整个体系的崩塌，于是乎建立了一整套例如仁义道德，忠君爱民，清正廉明等等不过是一套看似华丽的外衣罢了，说到底不过是赤裸裸的利益勾结和利益计算而已。<br>以下是摘抄了书中的几个观点进行分析：</p><blockquote><p>1.一个变质的政府，一个剥削性越来越强、服务性越来越弱的政府，自然也需要变质的官员，需要他们泯灭良心，心狠手辣，否则就要请你走人。这这种背景下，清官和恶棍的混合比率（即清官少，恶棍多）并不是偶然的巧合，而是定向选择的结果。恶政好比是一面筛子，淘汰清官，选择恶棍。<br>2.这就是说，在进行官场谋划，努力摆平各种利害关系的时候，无需考虑老百姓的压力，他们根本就不能构成一个压力集团，甚至连一个舆论集团也不是，不过是一盘散沙。<br>3.第一次接受了圣贤的教育，第二次则是接受胥吏衙役和人间大学的教育。第一次教育教了官员们满口仁义道德，第二次教育教了他们一肚子男盗女娼。<br>4.大家都懂得爱护羊群的重要意义。奈何抵抗不住眼前绵羊的诱惑，也抵抗不住生育狼崽子的诱惑。这也是有道理的：我不吃，别的狼照样吃；我不生，别的狼照样生。个体狼的利益与狼群的集体利益未必一致。如果我的节制不能导致别人的节制，我的自我约束对羊群来说就没有任何意义，徒然减少自己的份额而已。在老狼忍不住饕餮的时候，我可以听到一声叹息：它们要是变成刺猬，俺们不就变成清官了么？<br>5.真实的常规是：对局者双赢，老百姓买单。<br>6.老百姓是个冤大头(大头就是钱的意思。冤大头本意是花了冤枉的钱，引申为上当、不合算等. )。人家骂了他，打了他，吸了他的血，他连找人家的家长哭诉告状都找不起。唯一合算的选择，只剩下一个忍气吞声，继续让人家吸血。</p></blockquote><p>&nbsp;&nbsp;&nbsp;&nbsp;我们选取其中的部分进行详细分析，首先第一个条，劣币驱逐良币理论，官场中正直清廉的官员往往只占极小部分，而且往往郁郁不得志。为什么会出现这样的状况，实际上分析一下做正直官员与做一个非正直官员所承担的风险和收益就可以看出来，做一个正直官员往往面临着得罪同事，得罪上司等一系列风险，维护百姓的利益实际上可能还得不到好评，而一个非正直的官员既能够得到上司的好评又能够得到同事的好评，甚至能够赚得不菲的身价，而他所冒的风险仅仅是收到一些背地里的差评，而这些差评似乎对其官场生涯无关紧要，在风险和收益严重不成比例的情况下，出现劣官驱逐良官的现象也是必然的。另外第五条，对局者双赢，老百姓买单，这个看似不好理解，实际上很好理解，所谓清官与贪官的对局，从官吏组成结构来看只是一个上层制度的调整而已，是一个长期的剥削和短期过度剥削的理念对局，而下层的百姓永远免不了被剥削，因此从官吏阶层来看，对局者是处于双赢的局面的，而百姓总是付出代价的阶级。<br>&nbsp;&nbsp;&nbsp;&nbsp;以上截取了书中的部分观点，实际上我们更关心的是如何解决这个问题，从书中似乎看不到解决问题的具体方略，不过有一点是很明确的，那就是阶层的对立。实际上政府由于其具有较大的话语权，因此虽然说冠冕堂皇的将自己定义为服务者也不能免除其处于管理者的姿态。因此要解决以上问题，首要就是解决管理与服务的定位问题。而这个问题的关键在于执法权的归属问题，实际上所谓的内部监督说到底，立法执法都是统一个阶级，那就难以避免会出现潜规则的现象，因此需要权力的制衡，三权分立能够很好的实现权力的制衡，但是如果处于同一阶级，这样的制衡便失去了意义，重点在于对立的阶层需要掌握制衡的权力。以上是我的一点点看法，个人的意见，不对此负责。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;要是在四五年前看这本书，我可能不会喜欢，现在看起来却是看得我直冒冷汗。其实我不太喜欢潜规则这个词，这个词显得太不正义，总有一种鬼鬼祟祟的感觉，但是不得不承认，这个词的创立简直是对几千年来社会处事准则最准确的归纳和概括。为什么叫
      
    
    </summary>
    
      <category term="书评" scheme="http://www.wuweiblog.com/categories/%E4%B9%A6%E8%AF%84/"/>
    
    
      <category term="潜规则,书评" scheme="http://www.wuweiblog.com/tags/%E6%BD%9C%E8%A7%84%E5%88%99-%E4%B9%A6%E8%AF%84/"/>
    
  </entry>
  
  <entry>
    <title>tensorflow-二十七弹</title>
    <link href="http://www.wuweiblog.com/2018/08/22/tensorflow-%E4%BA%8C%E5%8D%81%E4%B8%83%E5%BC%B9/"/>
    <id>http://www.wuweiblog.com/2018/08/22/tensorflow-二十七弹/</id>
    <published>2018-08-22T10:19:38.000Z</published>
    <updated>2018-08-28T23:46:33.777Z</updated>
    
    <content type="html"><![CDATA[<p>&nbsp;&nbsp;&nbsp;&nbsp;tensorflow的学习进行到这个阶段，实际上已经处于一个入门阶段了，在前面的学习过程中我们着重介绍了CNN的构造以及实现过程，另外也提及了一些关于爬虫的知识以及一些关于机器学习的数学基础，现在感觉整个CNN的过程已经掌握得差不多了，剩下就是各种CNN网络得实现了，这个实际上就跟拼接积木差不多了，是一些调参得过程，其中如果进行深入的数学分析就太复杂了，所以在这里先放一放，先接触一下其他的类型的深度网络，然后再回来研究网络的构造问题，下面主要进行RNN的学习:</p><blockquote><p>RNN:循环神经网络，Recurrent Neural Network。神经网络是一种节点定向连接成环的人工神经网络。这种网络的内部状态可以展示动态时序行为。不同于前馈神经网络的是，RNN可以利用它内部的记忆来处理任意时序的输入序列，这让它可以更容易处理如不分段的手写识别、语音识别等。——百度百科</p></blockquote><p>&nbsp;&nbsp;&nbsp;&nbsp;实际上RNN更重要的作用应该是对于语义的识别，百度百科的定义我们看看就行了。本次学习以及代码参考一下文章以及博客(如有侵权，联系删除)：<br><a href="https://blog.csdn.net/liuchonge/article/details/70809288" target="_blank" rel="noopener">使用TensorFlow实现RNN模型入门篇1</a><br><a href="http://lib.csdn.net/article/aiframework/66348?knId=1756" target="_blank" rel="noopener">RNN入门详解及TensorFlow源码实现–深度学习笔记</a><br><a href="http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/" target="_blank" rel="noopener">Recurrent Neural Networks Tutorial, Part 1 – Introduction to RNNs</a><br><a href="https://r2rt.com/recurrent-neural-networks-in-tensorflow-i.html" target="_blank" rel="noopener">Recurrent Neural Networks in Tensorflow I</a><br><a href="http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-2-implementing-a-language-model-rnn-with-python-numpy-and-theano/" target="_blank" rel="noopener">Recurrent Neural Networks Tutorial, Part 2 – Implementing a RNN with Python, Numpy and Theano</a></p><h2 id="什么是RNN"><a href="#什么是RNN" class="headerlink" title="什么是RNN"></a>什么是RNN</h2><p>&nbsp;&nbsp;&nbsp;&nbsp;既然要学习RNN，那么我们就得先了解一下到底什么是RNN，实际上RNN被创造得目的在于充分利用序列数据的前后文信息。在传统的神经网络中假设没一次的输入和输入（每一次训练）是独立的，但是实际上在生活中我们面对很多问题的时候都会有一个上下文的关系，比如写文章之类的。我们语句的不同输入顺序可能有完全不同的意思，RNN就是来处理这样的问题的。另外我们从另一个角度来思考RNN，也就是我们通常说的记忆，意思就是能够从以前所有的输入数据中提取信息。理论上来说RNN能从记忆无限长时间的信息，但是在实际应用过程中会限制回溯的步长。<br>这里要祭出那张经典的图了：<br><img src="https://blogimage-1251632003.cos.ap-guangzhou.myqcloud.com/rnn.JPG"><br>&nbsp;&nbsp;&nbsp;&nbsp;这张被引用过无数次的图很形象的说明了RNN的过程，实际上左边是RNN的过程，右边是RNN展开的过程，如果我们关心五个单词的句子，整个网络就可以展开成一个五层的神经网络，每个单词就是一层，整个结构为：</p><blockquote><ul><li>$x_t$ is the input at time step $t$. For example, $x_1$ could be a one-hot vector corresponding to the second word of a sentence.  </li><li>$s_t$ is the hidden state at time step t. It’s the “memory” of the network. s_t is calculated based on the previous hidden state and the input at the current step: $s_t=f(Ux_t + Ws_{t-1})$. The function f usually is a nonlinearity such as tanh or ReLU.  $s_{-1}$, which is required to calculate the first hidden state, is typically initialized to all zeroes.  </li><li>$o_t$ is the output at step t. For example, if we wanted to predict the next word in a sentence it would be a vector of probabilities across our vocabulary. $o_t = \mathrm{softmax}(Vs_t)$.  </li></ul></blockquote><p>&nbsp;&nbsp;&nbsp;&nbsp;上面就是几个参数的说明，实际上比较简单也就没有必要再翻译了，看看就好了，下面就上面这个过程做一个简单的说明：  </p><ol><li>实际上可以认为$s_t$是一个记忆网络，能够记录以前所有的信息，而输出层$o_t$的计算仅依赖于时刻$t$的记忆，但是实际情况会复杂一些，因为$s_t$并不能记忆住前面太多步的信息（实际上也没有必要记住前所有步的信息）  </li><li>RNN实际上展开后每一层都是共享的同一参数，所不同的仅仅是输入值，通过此种方式极大的减小了参数的数目（序列输入，输入网络层数可能极大）</li><li>实际上对于部分应用来说不是所有的中间输出步骤都是有效的，我们仅仅关心最后的输出，同样对于输入我们也不需要关心每次的输入，RNN的主要特征是它的隐藏状态，这些隐藏状态可以获取序列数据的信息。</li></ol><h2 id="RNN的应用"><a href="#RNN的应用" class="headerlink" title="RNN的应用"></a>RNN的应用</h2><p>&nbsp;&nbsp;&nbsp;&nbsp;这里就随便谈谈了，实际上RNN做的最多的还是语义理解以及机器翻译工作，最常用的RNN模型问LSTM。具体的介绍参看<a href="http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/" target="_blank" rel="noopener">Recurrent Neural Networks Tutorial, Part 1 – Introduction to RNNs</a></p><h3 id="语言模型"><a href="#语言模型" class="headerlink" title="语言模型"></a>语言模型</h3><p>&nbsp;&nbsp;&nbsp;&nbsp;假设有一个m个字母的句子，我们建立如下一个语言模型去预测一个句子出现的可能性：<br>$$\begin{aligned}  P(w_1,…,w_m) = \prod_{i=1}^{m} P(w_i \mid w_1,…, w_{i-1})  \end{aligned}$$<br>&nbsp;&nbsp;&nbsp;&nbsp;通俗的来说，一个句子出现的可能性就是每个单词在它之前单词出现后可能性的后验概率的乘积。语言模型的重要之处在于可以通过语言模型形成一个打分机制，在机器翻译等工作中可以被用来选择最佳的翻译方式。语言模型的另外一个作用在于句式生成，如果我们有了足够丰富的句子，则我们可以通过构建好的语言模型生成句式。从上面的模型可以看出每一个单词生成的可能性都取决于其之前的所有单词，实际上很多模型都并不需要或者说从内存和计算时间的角度来说都无法关注到这么长远的记忆，因此我们会限制记忆的长度，并且对不同时长的记忆给不同大小的权重进行约束。</p><h2 id="RNN实现"><a href="#RNN实现" class="headerlink" title="RNN实现"></a>RNN实现</h2><p>&nbsp;&nbsp;&nbsp;&nbsp;好了以上就是RNN的一些介绍，以及其的应用，为了更加快速的入门RNN，我们通过tensorflow构建一个简单的网络对我们生成的简单数据进行训练。</p><h3 id="训练数据集的说明"><a href="#训练数据集的说明" class="headerlink" title="训练数据集的说明"></a>训练数据集的说明</h3><p>&nbsp;&nbsp;&nbsp;&nbsp;输入序列数据X：在第$t$步，$X_t$有百分之五十的可能性为1，另外百分之五十可能性为0，则$X$可能为$[1,0,0,1,1,1…]$<br>输出序列数据Y：对于任意第$t$步，$Y_t$有百分之五十的可能性为1，如果$X_{t-3}$步为1，则$Y_t$为1的可能性增加百分之50，如果$X_{t-8}$步为1，则$Y_t$为1的可能性下降百分之25%，通过这样的模式就确定了输出数据不仅和当前的输入有关，还与前几次的输入情况有关系；这样的一个网络实际上算是比较简单的网络结构了，我们根据以上生成的数据进行模型的构建。</p><h3 id="模型构建"><a href="#模型构建" class="headerlink" title="模型构建"></a>模型构建</h3><p>&nbsp;&nbsp;&nbsp;&nbsp;对于上面描述的这个简单问题，模型构建就很简单了,实际上对于一个模型来说我们首先要考虑的就是他的输入和输出问题，对于RNN模型我们输入是一个0或1的数据$X_t$以及上一个状态矢量$S_{t-1}$,输出$S_t$为可能性分布矢量，$P_t$是输出结果的预测，则有如下公式：<br>$$<br>\begin{aligned}<br>&amp;S_t=tanh(W(X_t\cdot S_{t-1})+b_s)\<br>&amp;P_t=softmax(US_t,+b_p)<br>\end{aligned}<br>$$<br>@表示向量的组合，$X_t$是一个二进制编码向量，$W$，$b_s$，$U$分别为状态矩阵，严格的来说应该证明为什么迭代就能够收敛到正确解，实际上对预测结果求导，然后导数为0分析其收敛特征，但是一般来说神经网络对于我们来说是一个黑盒过程，所以我们不太关心其背后的数学原理，假设能够收敛，则整个模型为:<br><img src="https://blogimage-1251632003.cos.ap-guangzhou.myqcloud.com/RNNsimple.JPG"><br>&nbsp;&nbsp;&nbsp;&nbsp;上图应该是比较好理解的图，$S_{-1}$为初始状态，可以都为0，然后进行循环计算，实际上训练过程有一个回溯的过程，我们在RNN的数学基础中再去讨论RNN的反向传播过程以及设置记忆长度为多少才合适的问题，现在我们只讲RNN的构造，RNN的构造主要是构造一个RNN_Cell然后复用就好了，主要代码为：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line">x = tf.placeholder(tf.int32, [batch_size, num_steps], name=<span class="string">'input_placeholder'</span>)</span><br><span class="line">y = tf.placeholder(tf.int32, [batch_size, num_steps], name=<span class="string">'labels_placeholder'</span>)</span><br><span class="line"><span class="comment">#RNN的初始化状态，全设为零。注意state是与input保持一致，接下来会有concat操作，所以这里要有batch的维度。即每个样本都要有隐层状态</span></span><br><span class="line">init_state = tf.zeros([batch_size, state_size])</span><br><span class="line"></span><br><span class="line"><span class="comment">#将输入转化为one-hot编码，两个类别。[batch_size, num_steps, num_classes]</span></span><br><span class="line">x_one_hot = tf.one_hot(x, num_classes)</span><br><span class="line"><span class="comment">#将输入unstack，即在num_steps上解绑，方便给每个循环单元输入。这里可以看出RNN每个cell都处理一个batch的输入（即batch个二进制样本输入）</span></span><br><span class="line">rnn_inputs = tf.unstack(x_one_hot, axis=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#定义rnn_cell的权重参数，</span></span><br><span class="line"><span class="keyword">with</span> tf.variable_scope(<span class="string">'rnn_cell'</span>):</span><br><span class="line">    W = tf.get_variable(<span class="string">'W'</span>, [num_classes + state_size, state_size])</span><br><span class="line">    b = tf.get_variable(<span class="string">'b'</span>, [state_size], initializer=tf.constant_initializer(<span class="number">0.0</span>))</span><br><span class="line"><span class="comment">#使之定义为reuse模式，循环使用，保持参数相同</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">rnn_cell</span><span class="params">(rnn_input, state)</span>:</span></span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(<span class="string">'rnn_cell'</span>, reuse=<span class="keyword">True</span>):</span><br><span class="line">        W = tf.get_variable(<span class="string">'W'</span>, [num_classes + state_size, state_size])</span><br><span class="line">        b = tf.get_variable(<span class="string">'b'</span>, [state_size], initializer=tf.constant_initializer(<span class="number">0.0</span>))</span><br><span class="line">    <span class="comment">#定义rnn_cell具体的操作，这里使用的是最简单的rnn，不是LSTM</span></span><br><span class="line">    <span class="keyword">return</span> tf.tanh(tf.matmul(tf.concat([rnn_input, state], <span class="number">1</span>), W) + b)</span><br><span class="line"></span><br><span class="line">state = init_state</span><br><span class="line">rnn_outputs = []</span><br><span class="line"><span class="comment">#循环num_steps次，即将一个序列输入RNN模型</span></span><br><span class="line"><span class="keyword">for</span> rnn_input <span class="keyword">in</span> rnn_inputs:</span><br><span class="line">    state = rnn_cell(rnn_input, state)</span><br><span class="line">    rnn_outputs.append(state)</span><br><span class="line">final_state = rnn_outputs[<span class="number">-1</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment">#定义softmax层</span></span><br><span class="line"><span class="keyword">with</span> tf.variable_scope(<span class="string">'softmax'</span>):</span><br><span class="line">    W = tf.get_variable(<span class="string">'W'</span>, [state_size, num_classes])</span><br><span class="line">    b = tf.get_variable(<span class="string">'b'</span>, [num_classes], initializer=tf.constant_initializer(<span class="number">0.0</span>))</span><br><span class="line"><span class="comment">#注意，这里要将num_steps个输出全部分别进行计算其输出，然后使用softmax预测</span></span><br><span class="line">logits = [tf.matmul(rnn_output, W) + b <span class="keyword">for</span> rnn_output <span class="keyword">in</span> rnn_outputs]</span><br><span class="line">predictions = [tf.nn.softmax(logit) <span class="keyword">for</span> logit <span class="keyword">in</span> logits]</span><br><span class="line"></span><br><span class="line"><span class="comment"># Turn our y placeholder into a list of labels</span></span><br><span class="line">y_as_list = tf.unstack(y, num=num_steps, axis=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#losses and train_step</span></span><br><span class="line">losses = [tf.nn.sparse_softmax_cross_entropy_with_logits(labels=label, logits=logit) <span class="keyword">for</span> \</span><br><span class="line">          logit, label <span class="keyword">in</span> zip(logits, y_as_list)]</span><br><span class="line">total_loss = tf.reduce_mean(losses)</span><br><span class="line">train_step = tf.train.AdagradOptimizer(learning_rate).minimize(total_loss)</span><br></pre></td></tr></table></figure></p><p>&nbsp;&nbsp;&nbsp;&nbsp;从以上代码可以看出，输入有$n$个单元，其中$n$为我们记忆回溯的步长，state_size为隐藏层的状态向量，长度根据需求和输入确定，我们直接看核心部分rnn_cell函数，这个函数定义了RNN的核心运算，首先是W和b的定义，然后是定义运算，整个运算过程为：tf.tanh(tf.matmul(tf.concat([rnn_input, state], 1), W) + b)，实际上就是公式中所提到的，这里有一个concat运算，这个运算时将两个矩阵进行连接，由于最开始的时候已经将编码方式转换为了one-hot编码，one-hot编码实际上意思就是采用你一个0-1的向量来对参数进行编码，组成一个参数矩阵，具体的解释可以参考<a href="https://blog.csdn.net/tengyuan93/article/details/78930285" target="_blank" rel="noopener">OneHot编码知识点</a>，通过这样的编码方式编码成可理解的向量，然后通过unstack解绑，得到每一个batch每一个step的输入，最后通过循环填充数据，然后定义输出层与中间层的W与b，进行误差的估计并采用AdagradOptimizer（随机梯度下降）的方法进行迭代。好了，整个过程就介绍到这里，下面一节就准备对RNN的数学基础进行学习。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;tensorflow的学习进行到这个阶段，实际上已经处于一个入门阶段了，在前面的学习过程中我们着重介绍了CNN的构造以及实现过程，另外也提及了一些关于爬虫的知识以及一些关于机器学习的数学基础，现在感觉整个CNN的过程已经掌握得
      
    
    </summary>
    
      <category term="学习" scheme="http://www.wuweiblog.com/categories/%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="tensorflow学习" scheme="http://www.wuweiblog.com/tags/tensorflow%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>tensorflow-二十六弹</title>
    <link href="http://www.wuweiblog.com/2018/08/13/tensorflow-%E4%BA%8C%E5%8D%81%E5%85%AD%E5%BC%B9/"/>
    <id>http://www.wuweiblog.com/2018/08/13/tensorflow-二十六弹/</id>
    <published>2018-08-13T14:54:33.000Z</published>
    <updated>2018-08-19T02:00:15.826Z</updated>
    
    <content type="html"><![CDATA[<p>&nbsp;&nbsp;&nbsp;&nbsp;写了很多关于tensorflow的部分，但是都比较零散，因为以前不管是对于python还是对于机器学习都了解得不够深刻，因此写出来的东西也就显得比较零散，代码不能够构成一个体系，所以也就谈不上什么积累，不过是多了解了一些关于tensorflow的东西而已，现在不管是对于python还是对于深度学习都有了更加深刻的理解，所以准备重新对整个过程进行组织，代码进行更加有条理的重构，以便于进行进一步的扩展。<br>&nbsp;&nbsp;目前只搭了一个CNN的框架，整个构架描述为：</p><ul><li>1.定义神经网络的基本结构单元：</li></ul><p>&nbsp;&nbsp;&nbsp;&nbsp;实际上神经所有的神经网络都会有一些基本的结构体，如权重，如偏置；因为就目前来说神经网络就是n多的拟合线性运算加上一个响应函数进行拟合，所以基本的结构单元都是相似的，因此我们定义了一个神经网络基类来初始化这些基础的结构<br>  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">  <span class="comment">#基础网络功能，包括：</span></span><br><span class="line"><span class="comment">#1.权重定义</span></span><br><span class="line"><span class="comment">#2.增益的定义</span></span><br><span class="line"><span class="comment">#3.二维卷积运算</span></span><br><span class="line"><span class="comment">#4.最大值池化</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BaseNet</span>:</span></span><br><span class="line">    <span class="comment">#初始化权重</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">weight_variable</span><span class="params">(self,shape)</span>:</span></span><br><span class="line">        <span class="comment">#从截断的正态分布中输出随机值</span></span><br><span class="line">        initial = tf.truncated_normal(shape, stddev=<span class="number">0.1</span>)</span><br><span class="line">        <span class="keyword">return</span> tf.Variable(initial)</span><br><span class="line"></span><br><span class="line">    <span class="comment">#初始化偏置</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">bias_variable</span><span class="params">(self,shape)</span>:</span></span><br><span class="line">        <span class="comment">#设置常数为0.1</span></span><br><span class="line">        initial = tf.constant(<span class="number">0.1</span>, shape=shape)</span><br><span class="line">        <span class="keyword">return</span> tf.Variable(initial)</span><br><span class="line"></span><br><span class="line">    <span class="comment">#二维卷积运算</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">conv2d</span><span class="params">(self,x, W)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> tf.nn.conv2d(x, W, strides=[<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>], padding=<span class="string">"SAME"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">#最大值池化</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">max_pool_2x2</span><span class="params">(self,x)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> tf.nn.max_pool(x, ksize=[<span class="number">1</span>,<span class="number">2</span>,<span class="number">2</span>,<span class="number">1</span>],</span><br><span class="line">                              strides=[<span class="number">1</span>,<span class="number">2</span>,<span class="number">2</span>,<span class="number">1</span>], padding=<span class="string">"SAME"</span>)</span><br></pre></td></tr></table></figure></p><p>&nbsp;&nbsp;&nbsp;&nbsp;从上面的代码中可以看到，我们定义的结构体包括：1.权重变量的定义；2.偏置变量的定义；3.卷积运算；4.池化操作；实际上卷积运算不是所有神经网络通用的操作，仅仅是卷积神经网络需要的操作，但是我们目前就是处理卷积神经网络，为了方便就这么写了。  </p><ul><li>2.特殊网络结构的定义：</li></ul><p>&nbsp;&nbsp;&nbsp;&nbsp;定义好了基本的神经网络结构以后剩下的工作就是针对某一个神经网络进行特殊的定义，目前我们只定义了卷积神经网络，实际上卷神经网络相对比较简单，主要的结构有两个部分，第一个是卷积层，通过卷积层可以进行参数共享从而减小参数个数，另外通过不同的卷积核实际上可以提取不同的特征从而对目标进行识别，另外一个是池化操作，池化操作是神经网络的一个巨大创新，通过池化这个简单的操作对近邻的特征进行概括，并且通过池化操作可以得到待识别目标的旋转不变特征。</p><ul><li><ol start="3"><li>根据数据对网络进行实例化：</li></ol></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#!/usr/bin/env python</span></span><br><span class="line"><span class="comment">#-*- coding:utf-8 -*-</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> model <span class="keyword">import</span> LeNet</span><br><span class="line"><span class="keyword">from</span> tensorflow.examples.tutorials.mnist <span class="keyword">import</span> input_data</span><br><span class="line">mnist = input_data.read_data_sets(<span class="string">"../data/MNIST_data/"</span>,one_hot=<span class="keyword">True</span>)</span><br><span class="line"></span><br><span class="line">ckptfiles = <span class="string">'./mnist_LeNet_ckpt/'</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">mnistLeNet</span><span class="params">(LeNet)</span>:</span></span><br><span class="line">    <span class="comment">#初始化网络</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        LeNet.__init__(self,<span class="number">28</span>,<span class="number">28</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment">#控制变量定义</span></span><br><span class="line">        self.learning_rate = <span class="number">0.001</span></span><br><span class="line">        <span class="comment"># 记录已经训练的次数</span></span><br><span class="line">        self.global_step = tf.Variable(<span class="number">0</span>, trainable=<span class="keyword">False</span>)</span><br><span class="line">        self.x = tf.placeholder(tf.float32, [<span class="keyword">None</span>, <span class="number">784</span>])</span><br><span class="line">        self.label = tf.placeholder(tf.float32, [<span class="keyword">None</span>, <span class="number">10</span>])</span><br><span class="line">        self.x_image = tf.reshape(self.x, [<span class="number">-1</span>,<span class="number">28</span>,<span class="number">28</span>,<span class="number">1</span>])</span><br><span class="line">        </span><br><span class="line">        <span class="comment">#网路层次定义</span></span><br><span class="line">        self.layer1(<span class="number">5</span>,<span class="number">5</span>,<span class="number">32</span>)</span><br><span class="line">        self.layer2(<span class="number">5</span>,<span class="number">5</span>,<span class="number">64</span>)</span><br><span class="line">        self.fullConnLayer(int(<span class="number">1024</span>))</span><br><span class="line">        self.outputLayer(<span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment">#计算参数定义</span></span><br><span class="line">        <span class="comment">#loss</span></span><br><span class="line">        self.loss = -tf.reduce_sum(self.label * tf.log(self.y + <span class="number">1e-10</span>))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># minimize 可传入参数 global_step， 每次训练 global_step的值会增加1</span></span><br><span class="line">        <span class="comment"># 因此，可以通过计算self.global_step这个张量的值，知道当前训练了多少步</span></span><br><span class="line">        self.train = tf.train.AdamOptimizer(self.learning_rate).minimize(</span><br><span class="line">            self.loss, global_step=self.global_step)</span><br><span class="line"></span><br><span class="line">        predict = tf.equal(tf.argmax(self.label, <span class="number">1</span>), tf.argmax(self.y, <span class="number">1</span>))</span><br><span class="line">        self.accuracy = tf.reduce_mean(tf.cast(predict, <span class="string">"float"</span>))</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">TrainMnistLeNet</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        self.net = mnistLeNet()</span><br><span class="line">        self.sess = tf.Session()</span><br><span class="line">        self.sess.run(tf.global_variables_initializer())</span><br><span class="line">        self.data = mnist</span><br><span class="line"></span><br><span class="line">    <span class="comment">#模型训练过程</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">trainMnist</span><span class="params">(self)</span>:</span></span><br><span class="line">        batch_size = <span class="number">50</span></span><br><span class="line">        train_step = <span class="number">3000</span></span><br><span class="line">        <span class="comment"># 记录训练次数, 初始化为0</span></span><br><span class="line">        step = <span class="number">0</span></span><br><span class="line">        save_interval = <span class="number">1000</span></span><br><span class="line"></span><br><span class="line">        batch_size = <span class="number">50</span></span><br><span class="line">        train_step = <span class="number">3000</span></span><br><span class="line">        <span class="comment"># 记录训练次数, 初始化为0</span></span><br><span class="line">        step = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 每隔1000步保存模型</span></span><br><span class="line">        <span class="comment">#save_interval = 1000</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># tf.train.Saver是用来保存训练结果的。</span></span><br><span class="line">        <span class="comment"># max_to_keep 用来设置最多保存多少个模型，默认是5</span></span><br><span class="line">        <span class="comment"># 如果保存的模型超过这个值，最旧的模型将被删除</span></span><br><span class="line">        saver = tf.train.Saver(max_to_keep=<span class="number">10</span>)</span><br><span class="line">        ckpt  = tf.train.get_checkpoint_state(ckptfiles)</span><br><span class="line">        merged = tf.summary.merge_all()</span><br><span class="line">        writer = tf.summary.FileWriter(ckptfiles+<span class="string">'graph'</span>,self.sess.graph)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> ckpt <span class="keyword">and</span> ckpt.model_checkpoint_path:</span><br><span class="line">            saver.restore(self.sess, ckpt.model_checkpoint_path)</span><br><span class="line">            <span class="comment"># 读取网络中的global_step的值，即当前已经训练的次数</span></span><br><span class="line">            step = self.sess.run(self.global_step)</span><br><span class="line">            print(<span class="string">'Continue from'</span>)</span><br><span class="line">            print(<span class="string">'        -&gt; Minibatch update : '</span>, step)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">while</span> step &lt; train_step:</span><br><span class="line">            x, label = self.data.train.next_batch(batch_size)</span><br><span class="line">            _, loss = self.sess.run([self.net.train, self.net.loss],</span><br><span class="line">                                    feed_dict=&#123;self.net.x: x, self.net.label: label&#125;)</span><br><span class="line">            step = self.sess.run(self.net.global_step)</span><br><span class="line">            rs=self.sess.run(merged)</span><br><span class="line">            writer.add_summary(rs, step)</span><br><span class="line">            <span class="keyword">if</span> step % <span class="number">100</span> == <span class="number">0</span>:</span><br><span class="line">                print(<span class="string">'第%5d步，当前loss：%.2f'</span> % (step, loss))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 模型保存在ckpt文件夹下</span></span><br><span class="line">        <span class="comment"># 模型文件名最后会增加global_step的值，比如1000的模型文件名为 model-1000</span></span><br><span class="line">        <span class="comment">#if step % save_interval == 0:</span></span><br><span class="line">        <span class="comment">#只保存一次模型</span></span><br><span class="line">        saver.save(self.sess, ckptfiles+<span class="string">'model'</span>, global_step=step)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">calculate_accuracy</span><span class="params">(self)</span>:</span></span><br><span class="line">        test_x = self.data.test.images</span><br><span class="line">        test_label = self.data.test.labels</span><br><span class="line">        accuracy = self.sess.run(self.net.accuracy,</span><br><span class="line">                                 feed_dict=&#123;self.net.x: test_x, self.net.label: test_label&#125;)</span><br><span class="line">        print(<span class="string">"准确率: %.2f，共测试了%d张图片 "</span> % (accuracy, len(test_label)))</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line">    app = TrainMnistLeNet()</span><br><span class="line">    app.trainMnist()</span><br><span class="line">    app.calculate_accuracy()</span><br></pre></td></tr></table></figure><p>&nbsp;&nbsp;&nbsp;&nbsp;这个步骤其实比较简单，就是根据数据对整个网络的输入和输出进行实例化操作，在我的代码中是训练的Mnist数据，所以定义输入的变量的28*28的数据，结果为10个数字，然后获取数据进行训练，由于Mnist数据的解析和处理都在tensorflow的example中被处理了，在处理过程中就省略了这个过程，实际上应该定义一个类专门对数据进行处理和优化。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;写了很多关于tensorflow的部分，但是都比较零散，因为以前不管是对于python还是对于机器学习都了解得不够深刻，因此写出来的东西也就显得比较零散，代码不能够构成一个体系，所以也就谈不上什么积累，不过是多了解了一些关于t
      
    
    </summary>
    
      <category term="学习" scheme="http://www.wuweiblog.com/categories/%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="tensorflow学习" scheme="http://www.wuweiblog.com/tags/tensorflow%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>RPC（共线条件方程）校正迭代方法分析</title>
    <link href="http://www.wuweiblog.com/2018/07/22/RPC%EF%BC%88%E5%85%B1%E7%BA%BF%E6%9D%A1%E4%BB%B6%E6%96%B9%E7%A8%8B%EF%BC%89%E6%A0%A1%E6%AD%A3%E8%BF%AD%E4%BB%A3%E6%96%B9%E6%B3%95%E5%88%86%E6%9E%90/"/>
    <id>http://www.wuweiblog.com/2018/07/22/RPC（共线条件方程）校正迭代方法分析/</id>
    <published>2018-07-22T13:47:52.000Z</published>
    <updated>2018-07-22T14:20:18.068Z</updated>
    
    <content type="html"><![CDATA[<p>这次主要分析GDAL中RPC校正的实现，以及在GDAL中PRPC校正的迭代方法，另外关于迭代计算还是有一些关于迭代的疑惑：<br><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">static</span> <span class="keyword">bool</span></span><br><span class="line">RPCInverseTransformPoint( GDALRPCTransformInfo *psTransform,</span><br><span class="line">                          <span class="keyword">double</span> dfPixel, <span class="keyword">double</span> dfLine, <span class="keyword">double</span> dfUserHeight,</span><br><span class="line">                          <span class="keyword">double</span> *pdfLong, <span class="keyword">double</span> *pdfLat )</span><br><span class="line"></span><br><span class="line">&#123;</span><br><span class="line">    <span class="comment">// Memo:</span></span><br><span class="line">    <span class="comment">// Known to work with 40 iterations with DEM on all points (int coord and</span></span><br><span class="line">    <span class="comment">// +0.5,+0.5 shift) of flock1.20160216_041050_0905.tif, especially on (0,0).</span></span><br><span class="line"></span><br><span class="line"><span class="comment">/* -------------------------------------------------------------------- */</span></span><br><span class="line"><span class="comment">/*      Compute an initial approximation based on linear                */</span></span><br><span class="line"><span class="comment">/*      interpolation from our reference point.                         */</span></span><br><span class="line"><span class="comment">/* -------------------------------------------------------------------- */</span></span><br><span class="line">    <span class="keyword">double</span> dfResultX =</span><br><span class="line">        psTransform-&gt;adfPLToLatLongGeoTransform[<span class="number">0</span>] +</span><br><span class="line">        psTransform-&gt;adfPLToLatLongGeoTransform[<span class="number">1</span>] * dfPixel +</span><br><span class="line">        psTransform-&gt;adfPLToLatLongGeoTransform[<span class="number">2</span>] * dfLine;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">double</span> dfResultY =</span><br><span class="line">        psTransform-&gt;adfPLToLatLongGeoTransform[<span class="number">3</span>] +</span><br><span class="line">        psTransform-&gt;adfPLToLatLongGeoTransform[<span class="number">4</span>] * dfPixel +</span><br><span class="line">        psTransform-&gt;adfPLToLatLongGeoTransform[<span class="number">5</span>] * dfLine;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span>( psTransform-&gt;bRPCInverseVerbose )</span><br><span class="line">    &#123;</span><br><span class="line">        CPLDebug(<span class="string">"RPC"</span>, <span class="string">"Computing inverse transform for (pixel,line)=(%f,%f)"</span>,</span><br><span class="line">                 dfPixel, dfLine);</span><br><span class="line">    &#125;</span><br><span class="line">    VSILFILE* fpLog = <span class="literal">nullptr</span>;</span><br><span class="line">    <span class="keyword">if</span>( psTransform-&gt;pszRPCInverseLog )</span><br><span class="line">    &#123;</span><br><span class="line">        fpLog =</span><br><span class="line">            VSIFOpenL( CPLResetExtension(psTransform-&gt;pszRPCInverseLog, <span class="string">"csvt"</span>),</span><br><span class="line">                       <span class="string">"wb"</span> );</span><br><span class="line">        <span class="keyword">if</span>( fpLog != <span class="literal">nullptr</span> )</span><br><span class="line">        &#123;</span><br><span class="line">            VSIFPrintfL( fpLog, <span class="string">"Integer,Real,Real,Real,String,Real,Real\n"</span> );</span><br><span class="line">            VSIFCloseL( fpLog );</span><br><span class="line">        &#125;</span><br><span class="line">        fpLog = VSIFOpenL( psTransform-&gt;pszRPCInverseLog, <span class="string">"wb"</span> );</span><br><span class="line">        <span class="keyword">if</span>( fpLog != <span class="literal">nullptr</span> )</span><br><span class="line">            VSIFPrintfL(</span><br><span class="line">                fpLog,</span><br><span class="line">                <span class="string">"iter,long,lat,height,WKT,error_pixel_x,error_pixel_y\n"</span> );</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/* -------------------------------------------------------------------- */</span></span><br><span class="line"><span class="comment">/*      Now iterate, trying to find a closer LL location that will      */</span></span><br><span class="line"><span class="comment">/*      back transform to the indicated pixel and line.                 */</span></span><br><span class="line"><span class="comment">/* -------------------------------------------------------------------- */</span></span><br><span class="line">    <span class="keyword">double</span> dfPixelDeltaX = <span class="number">0.0</span>;</span><br><span class="line">    <span class="keyword">double</span> dfPixelDeltaY = <span class="number">0.0</span>;</span><br><span class="line">    <span class="keyword">double</span> dfLastResultX = <span class="number">0.0</span>;</span><br><span class="line">    <span class="keyword">double</span> dfLastResultY = <span class="number">0.0</span>;</span><br><span class="line">    <span class="keyword">double</span> dfLastPixelDeltaX = <span class="number">0.0</span>;</span><br><span class="line">    <span class="keyword">double</span> dfLastPixelDeltaY = <span class="number">0.0</span>;</span><br><span class="line">    <span class="keyword">double</span> dfDEMH = <span class="number">0.0</span>;</span><br><span class="line">    <span class="keyword">bool</span> bLastPixelDeltaValid = <span class="literal">false</span>;</span><br><span class="line">    <span class="keyword">const</span> <span class="keyword">int</span> nMaxIterations =</span><br><span class="line">        (psTransform-&gt;nMaxIterations &gt; <span class="number">0</span>) ? psTransform-&gt;nMaxIterations :</span><br><span class="line">        (psTransform-&gt;poDS != <span class="literal">nullptr</span>) ? <span class="number">20</span> : <span class="number">10</span>;</span><br><span class="line">    <span class="keyword">int</span> nCountConsecutiveErrorBelow2 = <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">int</span> iIter = <span class="number">0</span>;  <span class="comment">// Used after for.</span></span><br><span class="line">    <span class="keyword">for</span>( ; iIter &lt; nMaxIterations; iIter++ )</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">double</span> dfBackPixel = <span class="number">0.0</span>;</span><br><span class="line">        <span class="keyword">double</span> dfBackLine = <span class="number">0.0</span>;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// Update DEMH.</span></span><br><span class="line">        dfDEMH = <span class="number">0.0</span>;</span><br><span class="line">        <span class="keyword">double</span> dfDEMPixel = <span class="number">0.0</span>;</span><br><span class="line">        <span class="keyword">double</span> dfDEMLine = <span class="number">0.0</span>;</span><br><span class="line">        <span class="keyword">if</span>( !GDALRPCGetHeightAtLongLat(psTransform, dfResultX, dfResultY,</span><br><span class="line">                                       &amp;dfDEMH, &amp;dfDEMPixel, &amp;dfDEMLine) )</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="keyword">if</span>( psTransform-&gt;poDS )</span><br><span class="line">            &#123;</span><br><span class="line">                CPLDebug(</span><br><span class="line">                    <span class="string">"RPC"</span>, <span class="string">"DEM (pixel, line) = (%g, %g)"</span>,</span><br><span class="line">                    dfDEMPixel, dfDEMLine);</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            <span class="comment">// The first time, the guess might be completely out of the</span></span><br><span class="line">            <span class="comment">// validity of the DEM, so pickup the "reference Z" as the</span></span><br><span class="line">            <span class="comment">// first guess or the closest point of the DEM by snapping to it.</span></span><br><span class="line">            <span class="keyword">if</span>( iIter == <span class="number">0</span> )</span><br><span class="line">            &#123;</span><br><span class="line">                <span class="keyword">bool</span> bUseRefZ = <span class="literal">true</span>;</span><br><span class="line">                <span class="keyword">if</span>( psTransform-&gt;poDS )</span><br><span class="line">                &#123;</span><br><span class="line">                    <span class="keyword">if</span>( dfDEMPixel &gt;= psTransform-&gt;poDS-&gt;GetRasterXSize() )</span><br><span class="line">                        dfDEMPixel = psTransform-&gt;poDS-&gt;GetRasterXSize() - <span class="number">0.5</span>;</span><br><span class="line">                    <span class="keyword">else</span> <span class="keyword">if</span>( dfDEMPixel &lt; <span class="number">0</span> )</span><br><span class="line">                        dfDEMPixel = <span class="number">0.5</span>;</span><br><span class="line">                    <span class="keyword">if</span>( dfDEMLine &gt;= psTransform-&gt;poDS-&gt;GetRasterYSize() )</span><br><span class="line">                        dfDEMLine = psTransform-&gt;poDS-&gt;GetRasterYSize() - <span class="number">0.5</span>;</span><br><span class="line">                    <span class="keyword">else</span> <span class="keyword">if</span>( dfDEMPixel &lt; <span class="number">0</span> )</span><br><span class="line">                        dfDEMPixel = <span class="number">0.5</span>;</span><br><span class="line">                    <span class="keyword">if</span>( GDALRPCGetDEMHeight( psTransform, dfDEMPixel,</span><br><span class="line">                                             dfDEMLine, &amp;dfDEMH) )</span><br><span class="line">                    &#123;</span><br><span class="line">                        bUseRefZ = <span class="literal">false</span>;</span><br><span class="line">                        CPLDebug(</span><br><span class="line">                            <span class="string">"RPC"</span>, <span class="string">"Iteration %d for (pixel, line) = (%g, %g): "</span></span><br><span class="line">                            <span class="string">"No elevation value at %.15g %.15g. "</span></span><br><span class="line">                            <span class="string">"Using elevation %g at DEM (pixel, line) = "</span></span><br><span class="line">                            <span class="string">"(%g, %g) (snapping to boundaries) instead"</span>,</span><br><span class="line">                            iIter, dfPixel, dfLine,</span><br><span class="line">                            dfResultX, dfResultY,</span><br><span class="line">                            dfDEMH, dfDEMPixel, dfDEMLine );</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line">                <span class="keyword">if</span>( bUseRefZ )</span><br><span class="line">                &#123;</span><br><span class="line">                    dfDEMH = psTransform-&gt;dfRefZ;</span><br><span class="line">                    CPLDebug(</span><br><span class="line">                        <span class="string">"RPC"</span>, <span class="string">"Iteration %d for (pixel, line) = (%g, %g): "</span></span><br><span class="line">                        <span class="string">"No elevation value at %.15g %.15g. "</span></span><br><span class="line">                        <span class="string">"Using elevation %g of reference point instead"</span>,</span><br><span class="line">                        iIter, dfPixel, dfLine,</span><br><span class="line">                        dfResultX, dfResultY,</span><br><span class="line">                        dfDEMH);</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">else</span></span><br><span class="line">            &#123;</span><br><span class="line">                CPLDebug(<span class="string">"RPC"</span>, <span class="string">"Iteration %d for (pixel, line) = (%g, %g): "</span></span><br><span class="line">                          <span class="string">"No elevation value at %.15g %.15g. Erroring out"</span>,</span><br><span class="line">                          iIter, dfPixel, dfLine, dfResultX, dfResultY);</span><br><span class="line">                <span class="keyword">if</span>( fpLog )</span><br><span class="line">                    VSIFCloseL(fpLog);</span><br><span class="line">                <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        RPCTransformPoint( psTransform, dfResultX, dfResultY,</span><br><span class="line">                           dfUserHeight + dfDEMH,</span><br><span class="line">                           &amp;dfBackPixel, &amp;dfBackLine );</span><br><span class="line"></span><br><span class="line">        dfPixelDeltaX = dfBackPixel - dfPixel;</span><br><span class="line">        dfPixelDeltaY = dfBackLine - dfLine;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span>( psTransform-&gt;bRPCInverseVerbose )</span><br><span class="line">        &#123;</span><br><span class="line">            CPLDebug(</span><br><span class="line">                <span class="string">"RPC"</span>, <span class="string">"Iter %d: dfPixelDeltaX=%.02f, dfPixelDeltaY=%.02f, "</span></span><br><span class="line">                <span class="string">"long=%f, lat=%f, height=%f"</span>,</span><br><span class="line">                iIter, dfPixelDeltaX, dfPixelDeltaY,</span><br><span class="line">                dfResultX, dfResultY, dfUserHeight + dfDEMH);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">if</span>( fpLog != <span class="literal">nullptr</span> )</span><br><span class="line">        &#123;</span><br><span class="line">            VSIFPrintfL(</span><br><span class="line">                fpLog, <span class="string">"%d,%.12f,%.12f,%f,\"POINT(%.12f %.12f)\",%f,%f\n"</span>,</span><br><span class="line">                iIter, dfResultX, dfResultY, dfUserHeight + dfDEMH,</span><br><span class="line">                dfResultX, dfResultY, dfPixelDeltaX, dfPixelDeltaY);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">const</span> <span class="keyword">double</span> dfError =</span><br><span class="line">            <span class="built_in">std</span>::max(<span class="built_in">std</span>::<span class="built_in">abs</span>(dfPixelDeltaX), <span class="built_in">std</span>::<span class="built_in">abs</span>(dfPixelDeltaY));</span><br><span class="line">        <span class="keyword">if</span>( dfError &lt; psTransform-&gt;dfPixErrThreshold )</span><br><span class="line">        &#123;</span><br><span class="line">            iIter = <span class="number">-1</span>;</span><br><span class="line">            <span class="keyword">if</span>( psTransform-&gt;bRPCInverseVerbose )</span><br><span class="line">            &#123;</span><br><span class="line">                CPLDebug( <span class="string">"RPC"</span>, <span class="string">"Converged!"</span> );</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">break</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">else</span> <span class="keyword">if</span>( psTransform-&gt;poDS != <span class="literal">nullptr</span> &amp;&amp;</span><br><span class="line">                 bLastPixelDeltaValid &amp;&amp;</span><br><span class="line">                 dfPixelDeltaX * dfLastPixelDeltaX &lt; <span class="number">0</span> &amp;&amp;</span><br><span class="line">                 dfPixelDeltaY * dfLastPixelDeltaY &lt; <span class="number">0</span> )</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="comment">// When there is a DEM, if the error changes sign, we might</span></span><br><span class="line">            <span class="comment">// oscillate forever, so take a mean position as a new guess.</span></span><br><span class="line">            <span class="keyword">if</span>( psTransform-&gt;bRPCInverseVerbose )</span><br><span class="line">            &#123;</span><br><span class="line">                CPLDebug(</span><br><span class="line">                    <span class="string">"RPC"</span>, <span class="string">"Oscillation detected. "</span></span><br><span class="line">                    <span class="string">"Taking mean of 2 previous results as new guess"</span> );</span><br><span class="line">            &#125;</span><br><span class="line">            dfResultX =</span><br><span class="line">                ( <span class="built_in">fabs</span>(dfPixelDeltaX) * dfLastResultX +</span><br><span class="line">                  <span class="built_in">fabs</span>(dfLastPixelDeltaX) * dfResultX ) /</span><br><span class="line">                (<span class="built_in">fabs</span>(dfPixelDeltaX) + <span class="built_in">fabs</span>(dfLastPixelDeltaX));</span><br><span class="line">            dfResultY =</span><br><span class="line">                ( <span class="built_in">fabs</span>(dfPixelDeltaY) * dfLastResultY +</span><br><span class="line">                  <span class="built_in">fabs</span>(dfLastPixelDeltaY) * dfResultY ) /</span><br><span class="line">                (<span class="built_in">fabs</span>(dfPixelDeltaY) + <span class="built_in">fabs</span>(dfLastPixelDeltaY));</span><br><span class="line">            bLastPixelDeltaValid = <span class="literal">false</span>;</span><br><span class="line">            nCountConsecutiveErrorBelow2 = <span class="number">0</span>;</span><br><span class="line">            <span class="keyword">continue</span>;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">double</span> dfBoostFactor = <span class="number">1.0</span>;</span><br><span class="line">        <span class="keyword">if</span>( psTransform-&gt;poDS != <span class="literal">nullptr</span> &amp;&amp;</span><br><span class="line">            nCountConsecutiveErrorBelow2 &gt;= <span class="number">5</span> &amp;&amp; dfError &lt; <span class="number">2</span> )</span><br><span class="line">        &#123;</span><br><span class="line">          <span class="comment">// When there is a DEM, if we remain below a given threshold (somewhat</span></span><br><span class="line">          <span class="comment">// arbitrarily set to 2 pixels) for some time, apply a "boost factor"</span></span><br><span class="line">          <span class="comment">// for the new guessed result, in the hope we will go out of the</span></span><br><span class="line">          <span class="comment">// somewhat current stuck situation.</span></span><br><span class="line">          dfBoostFactor = <span class="number">10</span>;</span><br><span class="line">          <span class="keyword">if</span>( psTransform-&gt;bRPCInverseVerbose )</span><br><span class="line">          &#123;</span><br><span class="line">              CPLDebug(<span class="string">"RPC"</span>, <span class="string">"Applying boost factor 10"</span>);</span><br><span class="line">          &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span>( dfError &lt; <span class="number">2</span> )</span><br><span class="line">            nCountConsecutiveErrorBelow2++;</span><br><span class="line">        <span class="keyword">else</span></span><br><span class="line">            nCountConsecutiveErrorBelow2 = <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">const</span> <span class="keyword">double</span> dfNewResultX = dfResultX</span><br><span class="line">            - ( dfPixelDeltaX * psTransform-&gt;adfPLToLatLongGeoTransform[<span class="number">1</span>] *</span><br><span class="line">                dfBoostFactor )</span><br><span class="line">            - ( dfPixelDeltaY * psTransform-&gt;adfPLToLatLongGeoTransform[<span class="number">2</span>] *</span><br><span class="line">                dfBoostFactor );</span><br><span class="line">        <span class="keyword">const</span> <span class="keyword">double</span> dfNewResultY = dfResultY</span><br><span class="line">            - ( dfPixelDeltaX * psTransform-&gt;adfPLToLatLongGeoTransform[<span class="number">4</span>] *</span><br><span class="line">                dfBoostFactor )</span><br><span class="line">            - ( dfPixelDeltaY * psTransform-&gt;adfPLToLatLongGeoTransform[<span class="number">5</span>] *</span><br><span class="line">                dfBoostFactor );</span><br><span class="line"></span><br><span class="line">        dfLastResultX = dfResultX;</span><br><span class="line">        dfLastResultY = dfResultY;</span><br><span class="line">        dfResultX = dfNewResultX;</span><br><span class="line">        dfResultY = dfNewResultY;</span><br><span class="line">        dfLastPixelDeltaX = dfPixelDeltaX;</span><br><span class="line">        dfLastPixelDeltaY = dfPixelDeltaY;</span><br><span class="line">        bLastPixelDeltaValid = <span class="literal">true</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span>( fpLog != <span class="literal">nullptr</span> )</span><br><span class="line">        VSIFCloseL( fpLog );</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span>( iIter != <span class="number">-1</span> )</span><br><span class="line">    &#123;</span><br><span class="line">        CPLDebug( <span class="string">"RPC"</span>, <span class="string">"Failed Iterations %d: Got: %.16g,%.16g  Offset=%g,%g"</span>,</span><br><span class="line">                  iIter,</span><br><span class="line">                  dfResultX, dfResultY,</span><br><span class="line">                  dfPixelDeltaX, dfPixelDeltaY );</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    *pdfLong = dfResultX;</span><br><span class="line">    *pdfLat = dfResultY;</span><br><span class="line">    <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>上述代码中有很多判断的部分，我们忽略掉这些部分直接看干货，关于PRC校正的方法在以前的文章中已经做过比较详细的分析了，在这里我们不对校正方法进行分析，直接看迭代过程：<br>首先定义了一系列的变量，包括什么迭代变量，误差限变量等，然后根据变换参数计算像素的初始坐标，计算方法为直接根据参考点进行线性插值得到的，实际上这里应该是存疑的，如果没有参考点应该怎么办，我估计这个转换参数在RPC参数中应该是保存的，所以在这里也不详细分析了，代码部分也没有具体实现，我们直接理解就是一个初始化的计算参数。迭代的计算过程为：</p><ul><li>根据坐标以及DEM计算初始位置的高程，然后就是一系列的判断，包括判断DEM的坐标系与给出影像的坐标系，判断是否能够得到高程，以及一系列其他的判断；然后最后的结果就是得到DEM的高程，整个判断结束；</li><li>RPC反算，根据坐标和RPC参数反算出对应的像素点的坐标;</li><li>计算当前给出的上一次给出的像素值和反解出的像素值的误差值；</li><li>判断误差值是否超限了，如果误差在误差限内则说明精度满足要求直接跳出循环了，如果超过误差限则进行迭代；（p.s:在这里GDAL有一个判断我觉得做得很好，在迭代的过程中给了一个猜测参数，如果整个迭代的过程出现oscillate现象，则重新给一个猜测值，这个值的给定估计也有一点经验）</li><li>新的值等于原来的值加上误差项（表现为差值）</li></ul><p>对于以上过程的前几步都是比较好理解的，但是对于这个最后一个新值的获取我有一些疑惑，如果我自己实现的话我会直接用新值带入RPC参数解算新的DEM和坐标并进行迭代，关于迭代的误差在以前的文章中也提到过，初始高程设置的不同会导致迭代次数和误差都特别大，想弄清楚两种迭代方式有什么区别，如果搞明白了我会在下一次的文章中进行详细的描述。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;这次主要分析GDAL中RPC校正的实现，以及在GDAL中PRPC校正的迭代方法，另外关于迭代计算还是有一些关于迭代的疑惑：&lt;br&gt;&lt;figure class=&quot;highlight c++&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span c
      
    
    </summary>
    
      <category term="图像处理" scheme="http://www.wuweiblog.com/categories/%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86/"/>
    
    
      <category term="图像处理数学原理" scheme="http://www.wuweiblog.com/tags/%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86%E6%95%B0%E5%AD%A6%E5%8E%9F%E7%90%86/"/>
    
  </entry>
  
  <entry>
    <title>混合高斯分布</title>
    <link href="http://www.wuweiblog.com/2018/07/15/%E6%B7%B7%E5%90%88%E9%AB%98%E6%96%AF%E5%88%86%E5%B8%83/"/>
    <id>http://www.wuweiblog.com/2018/07/15/混合高斯分布/</id>
    <published>2018-07-15T02:01:55.000Z</published>
    <updated>2018-07-21T06:12:41.798Z</updated>
    
    <content type="html"><![CDATA[<p>本来是想着做一个混合高斯分布，但是想着点云的分布应该不是混合高斯布，所以又想看看是不是应该是均匀分布，但是我仔细一想还是应该是高斯分布，因为我们现在讨论的并不是点云本身的分布特点，而是点云聚类的分布特点，实际上对于给定的类别，距离聚类中心越近属于该类别的可能性就越大，因此还是应当是高斯分布。为了更加具体的说明问题，我模拟以下两个数据集：<br><img src="http://blogimage-1251632003.cosgz.myqcloud.com/%E9%AB%98%E6%96%AF%E6%B7%B7%E5%90%88%E5%88%86%E5%B8%831.jpg"><br>图中两个点集都是均匀分布，黄色框中为的黑色点为一个随机点，实际上我们认为我们每一个点都是随机均匀分布而且激光束发出的激光理论上也应该是均匀分布，但是实际上地物的分布有其自身的特点，且相似的地物具有聚集性（地理学第一定律）因此我们可以认为地物的分布是具有高斯分布特性的，而地物的分布实际上会影响点云在空间上的密度使其呈现出与地物分布类似的密度。因此我们有理由相信高斯分布县比喻均匀分布能够更好的对地物的分布进行拟合。  </p><h2 id="高斯分布："><a href="#高斯分布：" class="headerlink" title="高斯分布："></a>高斯分布：</h2><p>$$<br>\begin{aligned}<br>&amp;p(x)=\frac{1}{\sqrt{2}\pi}exp(-\frac{x^2}{2}) \<br>&amp;p(x,y)=p(x)*p(y)=\frac{1}{\sqrt{2}\pi}exp(-\frac{x^2+y^2}{2})<br>\end{aligned}<br>$$<br>以上为一维和二维的标准高斯分布，实际上我们将高斯分布扩展到多维，则需要用引入向量表示，即我们用向量$v$表示一个随机向量则多变量的高斯分布可以表示为$p(v)=\frac{1}{\sqrt{2}\pi}exp(-\frac{v^Tv}{2})$,实际上方差的平方要变成协方差，但是由于是标准的正态分布，因此直接$v^Tv$计算就可以了,针对一般情况需要进行一个变换了，由于均值不为0方差存在差异，则我们假设一个线性变换，通过这个线性变换能够将变量变换到标准正态分布上，这个线性变换为$x=A(x-\mu)$,则标准的正态分布可以表示为：<br>$$<br>p(v) = \frac{|A|}{\sqrt{2}\pi}exp[-\frac{1}{2}(x-\mu)\Sigma^{-1}(x-\mu)]<br>$$<br>在上式中$\Sigma$为协方差矩阵,则上式表示的为均值为$\mu$协方差矩阵为$A^TA^-1$的$n$维正态分布。则我们模拟一个二维高斯正态分布的图为：<br><img src="http://blogimage-1251632003.cosgz.myqcloud.com/%E9%AB%98%E6%96%AF%E6%B7%B7%E5%90%88%E5%88%86%E5%B8%832.jpg"><br>实际上高斯分布是一种常用而且简单的分布，且一般情况下地物聚类的分布特征都符合高斯分布，因此将高斯分布用来对地物进行分类是一个合理的做法。</p><h2 id="高斯混合分布："><a href="#高斯混合分布：" class="headerlink" title="高斯混合分布："></a>高斯混合分布：</h2><p>实际上单高斯分布相对来说比较简单，但是自然地物很少存在单高斯分布的情况，由于地物的复杂性，自然地物呈现的是聚合而且混杂的趋势，因此使用单一的高斯分布对地物分布进行拟合可能存在较大误差，此时需要考虑多高斯分布的情况：<br>$$<br>p(x)=\sum_{k=1}^{K}\pi_{k}\mathcal{N}(x|\mu_{k},\Sigma_K)<br>$$<br>其中$\mathcal{N}(x|\mu_{k})$表示为均值为$\mu_k$协方差矩阵为$\Sigma_K$的高斯分布，实际上高斯混合分布可以看作是$K$个高斯分布的求和。<br>从图上看，高斯混合分布如下图所示：<br><img src="http://blogimage-1251632003.cosgz.myqcloud.com/%E9%AB%98%E6%96%AF%E6%B7%B7%E5%90%88%E5%88%86%E5%B8%833.jpg"><br>上图是两个高斯分布的模型，从上图可以看出，如果采用一个高斯函数进行拟合会存在较大的误差，拟合效果也不好，至少需要两个高斯函数进行拟合才能够达到比较好的效果。因此我们选择了一个高斯函数进行拟合时可能会存在较大的误差，由此我们也引出了下一个需要讨论的话题，理论上无限个高斯函数可以拟合任意分布，因此只要在$K$足够大的情况下可以对任意函数进行无限精确的拟合，但是这并不是我们想要的结果，这个就是我们下面需要讨论的问题，那就是高斯混合分布的求解问题。</p><h2 id="高斯混合分布求解："><a href="#高斯混合分布求解：" class="headerlink" title="高斯混合分布求解："></a>高斯混合分布求解：</h2><h3 id="初级求解方式："><a href="#初级求解方式：" class="headerlink" title="初级求解方式："></a>初级求解方式：</h3><p>在此情况下，我们假设高斯模型的数量是已知的，那么实际上需要求解的参数就是每一个高斯模型的参数$\mu$和$\Sigma$，实际上我们需要求取的是分布的参数，那么如何求取分布参数呢，一般来说就是使得所有样本都分到最大概率的那一类中，采用的方法一般是最大似然方法，公式表示为：<br>$$<br>f=\prod_{i=1}^N p(x_i)<br>$$<br>实际上由于概率都小于1在连乘过程中容易出现精度不足的现象，因此通过取对数将连乘转换为累加进行处理。则将高斯混合分布的公式代入上式并去对数则参数求解问题转换为：<br>$$<br>    max\Sigma_{i=1}^{N}log(\Sigma_{k=1}^{K}\pi_k\mathcal{N}(x_i|\mu_k,\sigma_k))<br>$$<br>对于求极值问题一般都是求导然后导数等于0进行求解，但是上式太负载，求导过于复杂因此不建议采用求导的方式求极值，一般来说是采用<strong>EM</strong>方法进行求解。<br>实际上对于上式的<strong>EM</strong>求解这里有一个训练的过程，一般来说给的训练数据都是$(x,label)$即数据与其所属类别，对于样本$x$它属于某一个类别$k$的概率为：<br>$$<br>\omega_i(k)=\frac{\pi_k\mathcal{N}(x|\mu_k,\sigma_k)}{\Sigma_{j=1}^{K}\sigma_j\mathcal{N}(x|\mu_j,\sigma_j)}<br>$$<br>在这里是假设所有类别的均值和协方差矩阵都是已知的，判断样本属于类别$k$的概率。因此我们是需要给出一个初始值的。由给定的初始值就可以计算出属于每一类的概率。然后对于每一类实际是要计算其期望和方差，因此已知每一个样本属于某一类后每一类的期望和方差的计算方法为：<br>$$<br>\begin{aligned}<br>&amp; \mu_k=\frac{1}{N}\Sigma_{i=1}^{N}\omega_i(k)x_i \<br>&amp; \sigma_k=\frac{1}{N}\Sigma_{i=1}^{N}\omega_i(k)(x_i-\mu_k)(x_i-\mu_k)^T  \<br>&amp;N_k=\Sigma_{i=1}^{N}\omega_i(k)<br>\end{aligned}<br>$$<br>分析上面的公式我们可以根据样本对每一类的均值以及方差进行重新估计，得到新的均值和方差，并以此迭代最终达到收敛。最简单的实现方式为K-Means方式，通过迭代实现聚类。</p><h3 id="类别数目未知的情况："><a href="#类别数目未知的情况：" class="headerlink" title="类别数目未知的情况："></a>类别数目未知的情况：</h3><p>实际上初级求解方式是在类别数目已知的情况下做估计，对于很多问题，特别是高斯混合分布的问题，我们通常都不知道类别数目，在此情况下求解就与简单的求解方式有着区别，因为在进行参数估计的时候还需要对类别数目有一个估计，实际上给定的类别数目越多估计肯定越准确，但是如果给定类别数目过多可能出现过拟合的现象，所以实际上我们需要对类别数目有一个约束，通常情况下有三种约束方式：$L_0,L_1,L_2$约束，解释一下，0范约束表示类别前系数$\pi_i$不为0的情况尽量多，1范约束表示类别前系数$\pi_i$的1范值尽量小，2范约束表示类别前系数$\pi_i$的2范值尽量小;<br>从误差的角度来说，则我们将混合高斯分布看做是对真值的拟合，则误差描述为：<br>$$<br>E(x)=f(x)+r<br>$$<br>其中$f(x)$为拟合误差，$r$为约束项，实际上我们需要使得误差最小将高斯分布代入到以上的计算过程中；则有<br>$$<br>\begin{aligned}<br>E(x)&amp;=p_t(x)-p(x)+r\<br>&amp;=p_t(x)-\sum_{k=1}^{K}\pi_{k}\mathcal{N}(x|\mu_{k},\Sigma_K)<br>\end{aligned}+\alpha\sum_{i}^K|\pi_i|<br>$$<br>上式加上了$L1$约束，则在求解过程中高斯分布的数目都是需要求解的，同样采用E-M方法进行求解，但是在求解过程中还需要对K值进行调整使其达到最佳值，实际上$\alpha$是对稀疏度的约束，在求解过程中能够体现出来，值越大则越稀疏，越小则稀疏度越低。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;本来是想着做一个混合高斯分布，但是想着点云的分布应该不是混合高斯布，所以又想看看是不是应该是均匀分布，但是我仔细一想还是应该是高斯分布，因为我们现在讨论的并不是点云本身的分布特点，而是点云聚类的分布特点，实际上对于给定的类别，距离聚类中心越近属于该类别的可能性就越大，因此还
      
    
    </summary>
    
      <category term="数学" scheme="http://www.wuweiblog.com/categories/%E6%95%B0%E5%AD%A6/"/>
    
    
      <category term="数学" scheme="http://www.wuweiblog.com/tags/%E6%95%B0%E5%AD%A6/"/>
    
  </entry>
  
  <entry>
    <title>python修饰符</title>
    <link href="http://www.wuweiblog.com/2018/07/08/python%E4%BF%AE%E9%A5%B0%E7%AC%A6/"/>
    <id>http://www.wuweiblog.com/2018/07/08/python修饰符/</id>
    <published>2018-07-08T15:19:47.000Z</published>
    <updated>2018-07-10T02:54:31.501Z</updated>
    
    <content type="html"><![CDATA[<p>本来分类应该分类到编程这一类下，但是想想所有关于python的学习都放在机器学习下，另外当时学习python的目的就是搞机器学习，所以也就放在了这一类下面，其实python的语法我就不再进行介绍了，面向对象的语言的语法大致上都差不多只是略有点区别而已，但是以前一直有一点不明确，那就是关于修饰符的作用，在做项目的时候看到了并且胡乱使用了一通，今天特意就这个问题进行一下总结。</p><h2 id="内置修饰符"><a href="#内置修饰符" class="headerlink" title="内置修饰符"></a>内置修饰符</h2><p>python的类内置修饰符有三个，分别为staticmethod，classmethod以及property，其作用分别为1.把类中定义的实例方法变成静态方法；2.把类中定义的实例方法变成类方法；3.将类成员修改为类属性。由于Python模块中可以定义函数，因此静态方法和类方法的用处并不多；下面对于staticmethod和classmethod两种方法进行说明，我们知道在定义类和类的成员函数后对于成员函数的调用首先要实例化一个类对象，然后通过类对象来调用成员函数，但是对于静态成员函数的调用则不需要对类进行实例化，实际上以上两种修饰符的作用都类似，但是又有一些区别。我们看几个实例：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">testclassmethod</span>:</span></span><br><span class="line"><span class="meta">@classmethod</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">printd</span><span class="params">(cls,a,b)</span>:</span></span><br><span class="line">print(a+b)</span><br><span class="line"><span class="keyword">if</span> __name__==<span class="string">'__main__'</span>:</span><br><span class="line">testclassmethod.printd(<span class="number">3</span>,<span class="number">5</span>)</span><br></pre></td></tr></table></figure></p><p>从上面这个例子我们可以看出通过classmethod修饰符的作用，通过修饰符修饰之后成员方法不需要通过实例化就可以对方法进行调用，但是通过修饰符修饰的方法中第一个参数并不是我们常见的self了，变成了cls，这个参数实际上表示类自身，通过这个参数可以调用类的属性和方法，相当于类的一个实例吧；下面我们看看staticmethod：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">teststaticmethod</span>:</span></span><br><span class="line"><span class="meta">@staticmethod</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">printd</span><span class="params">(a,b)</span>:</span></span><br><span class="line">print(a+b)</span><br><span class="line"><span class="keyword">if</span> __name__==<span class="string">'__main__'</span>:</span><br><span class="line">teststaticmethod.printd(<span class="number">3</span>,<span class="number">5</span>)</span><br><span class="line">teststaticmethod().printd(<span class="number">3</span>,<span class="number">5</span>)</span><br></pre></td></tr></table></figure></p><p>比较两部分代码我们可以看到，最主要的区别在于staticmethod进行修饰后不需要再定义一个代表类本身的参数。好了以上两种修饰符就介绍到这里，下面介绍装饰器的作用：</p><h2 id="装饰器"><a href="#装饰器" class="headerlink" title="装饰器"></a>装饰器</h2><p>在python中我们经常可以看到如下写法：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">test1</span><span class="params">(f)</span>:</span></span><br><span class="line">f()</span><br><span class="line">print(<span class="string">'test1'</span>)</span><br><span class="line"></span><br><span class="line"><span class="meta">@test1</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">test2</span><span class="params">()</span>:</span></span><br><span class="line">print(<span class="string">'test2'</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__==<span class="string">'__main__'</span>:</span><br><span class="line">test2()</span><br></pre></td></tr></table></figure></p><p>这个简单的例子说明了装饰器的作用，实际上我们的调用顺序是test1，然后在test1中调用传入函数参数f，然后调用print，我们可以将装饰器理解成函数指针，将装饰器后的函数作为参数输入到装饰器函数中进行调用。<br>P.S. 以上代码都在python3.7 下编译通过，不同版本的python可能略有区别</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;本来分类应该分类到编程这一类下，但是想想所有关于python的学习都放在机器学习下，另外当时学习python的目的就是搞机器学习，所以也就放在了这一类下面，其实python的语法我就不再进行介绍了，面向对象的语言的语法大致上都差不多只是略有点区别而已，但是以前一直有一点不明
      
    
    </summary>
    
      <category term="学习" scheme="http://www.wuweiblog.com/categories/%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="机器学习，图像处理" scheme="http://www.wuweiblog.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%8C%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86/"/>
    
  </entry>
  
  <entry>
    <title>向往的生活</title>
    <link href="http://www.wuweiblog.com/2018/07/04/%E5%90%91%E5%BE%80%E7%9A%84%E7%94%9F%E6%B4%BB/"/>
    <id>http://www.wuweiblog.com/2018/07/04/向往的生活/</id>
    <published>2018-07-04T01:04:14.000Z</published>
    <updated>2018-12-08T10:23:46.267Z</updated>
    
    <content type="html"><![CDATA[<p>&nbsp;&nbsp;&nbsp;&nbsp;最近出差很多，乱七八糟的事情也很多，很长时间没有静下心来思考和总结了，总是感觉自己在匆匆忙忙的奔来奔去，在来往的忙碌中总是感觉丢失了一些东西，虽然看似很忙碌，可是事情似乎总是做的不如自己想的那么好，总有这样或者那样的问题让人觉得不怎么舒服。我不是一个追求完美的人，但是也总是想着能够努力把事情做到最好，可是最近似乎不怎么能够满意。这两天微信上一篇《摧毁一个中间男人有多么容易》的文章在微信朋友圈甚嚣尘上，我默默的看了一篇，从文中看到了满满的焦虑，似乎只要不是暴富我们的人生都有可能被文中所说的情况击垮。所以大家都很焦虑，为工作焦虑，为生活焦虑，为生老病死焦虑，我们在焦虑中惶惶不可终日。所以我总是问自己，到底什么才是我向往的生活。<br>&nbsp;&nbsp;&nbsp;&nbsp;最近湖南电视台出品了一个节目就叫向往的生活，几位明星在山中搭了一个小屋，过上了自给自足的生活，看起来十分美好，院子里的小狗，还有牛羊以及小鸭子，一个大院子，院子里的凉亭，看起来都是那么舒服，让人心生向往。可是似乎觉得那样的生活太美好以至于显得不够真实了。可是我们自己向往的生活又是什么样子呢，这真是一个很难回答的问题，我发现我们总是在生活中背负了太多，我们需要承载父母亲人的过去，需要承载子女的未来，实际上这些都不应该是我们所需要背负的，虽然这么说看起来不孝，也很冷酷，但是这就是我心中的想法。每个人都应该去追求自己人生的价值，从而让自己不至于因为碌碌无为而把希望寄托于子女身上；每个人都应该在活着的时候让自己过得精彩，从而在死亡来临的时候我们能够从容面对。希望从来都不应该是别人给的，生活也是，我们为了实现自己生活的向往和目标而努力，而不是为别人而活。<br>&nbsp;&nbsp;&nbsp;&nbsp;当然，我们国家的传统是很有奉献精神的，父母为子女奉献一生，从而失去自己的生活，那么子女一定要有反哺的意思，所以父母从小孩儿出生开始不仅要为子女的未来准备衣食住行，还要为子女的未来出谋划策，而子女也需要承受这种恩情的重量。这样每个人都是在为别人而活，我不知道这样是否能够真的过上自己想要的生活，还是仅仅是被生活折磨的不知道自己想要什么样的生活。我觉得每个人都应该为自己而活，我想只有为自己而活才能够快乐幸福，而这种幸福才能够影响身边的人去追求自己的幸福。<br>&nbsp;&nbsp;&nbsp;&nbsp;我希望能够为自己而活，我愿意承受自己生活的重量而不是承担所有生活的重量，我希望我可以忽视掉他人的眼光，我希望我能够实现自己的理想，而这就是我向往的生活。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;最近出差很多，乱七八糟的事情也很多，很长时间没有静下心来思考和总结了，总是感觉自己在匆匆忙忙的奔来奔去，在来往的忙碌中总是感觉丢失了一些东西，虽然看似很忙碌，可是事情似乎总是做的不如自己想的那么好，总有这样或者那样的问题让人觉
      
    
    </summary>
    
      <category term="随感" scheme="http://www.wuweiblog.com/categories/%E9%9A%8F%E6%84%9F/"/>
    
    
      <category term="随感" scheme="http://www.wuweiblog.com/tags/%E9%9A%8F%E6%84%9F/"/>
    
  </entry>
  
  <entry>
    <title>空房子</title>
    <link href="http://www.wuweiblog.com/2018/06/25/%E7%A9%BA%E6%88%BF%E5%AD%90/"/>
    <id>http://www.wuweiblog.com/2018/06/25/空房子/</id>
    <published>2018-06-24T22:33:41.000Z</published>
    <updated>2018-12-08T10:28:45.270Z</updated>
    
    <content type="html"><![CDATA[<p>&nbsp;&nbsp;&nbsp;&nbsp;两个星期前把菜包送走了，那天弄到了很晚，大概晚上一点多吧，第二天配菜玩了一天之后晚上就回来了，回到了空空荡荡的房间，本来菜包在的时候有时候会觉得特别烦它，因为它总是掉毛，而且喜欢霸占我的床，并且喜欢破坏家里的东西，可是那天，没有一只毛茸茸的宝宝跳起来迎接我，一种孤独感油然而生，总觉得家里空空荡荡。<br>&nbsp;&nbsp;&nbsp;&nbsp;幸而这一段时间都在出差，所以这种孤独的感觉被冲淡了不少，但是呆在家的时候，这样的孤独仍然是不可避免，很多时候我自以为会享受孤独，实际上自己从来不曾孤独过，不管是高中还是本科，或者是硕士，在宿舍至少有舍友在一起，不论的玩还是聊天都不曾有过孤独的感觉，所以觉得一个人的时光是多么的美好，可是现在真的彻底一个人了，又开始没有理由的慌张起来，不知道为什么会慌张，可是真的会感觉到慌张，有些不知所措。<br>&nbsp;&nbsp;&nbsp;&nbsp;其实一直觉得自己是一个内心丰富的人，能够抵挡得了孤独的侵蚀，甚至能够享受独孤，可是当真当我独自面对空空荡荡的房间的时候自己又不是那么确信了，关上灯，房里安静得如同鬼魅，外面却是吵吵闹闹，这个时候就想到了那著名的一句，热闹是他们的，我什么也没有。对呀我什么也没有，除了孤独，所以在孤独的时候更愿意去思考人生，思考的角度也与别人不同，于是我在想，其实我们这一生所经历过真正孤独的时光并不多，读书的时候有着同学和家人的陪伴，到毕业了又会有伴侣的陪伴或者也会有舍友，然后结婚了，与爱人一起过完自己的这一生，我们的一生似乎都在他人的陪伴下度过，而自己一个人的时光却很少经历，所以导致我们面对孤独的时候很难从生活中获取经验，我们所了解的不过是那些书中人们所面对孤独的方式，可是那都是有着强大自控力和强大意志力的人，我们也许无法做到和他们一样的优秀。而我在面对孤独的时候难免会有些恐慌，有些不知所措。<br>&nbsp;&nbsp;&nbsp;&nbsp;其实一开始我是很开心的，因为独处的时间的多么的宝贵，然而一段时间过后，这样的开心变成了折磨，因为孤独如潮水一样不停的包围着你，侵蚀着你，让你不知道何去何从，让你倍感压抑。其实我们一生，快乐的日子并没有想象中的那么多，当然实际上悲伤的日子也没有感觉的那么多，更多的时候是平淡的日子，而着平淡中如果还透着一丝孤独，那么总是会在骨子里感觉到一丝寒意吧。<br>&nbsp;&nbsp;&nbsp;&nbsp;我送走了我的狗子，我的房子终于变得空空荡荡了，我的房子变得空空荡荡了，我很想念我的狗子。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;两个星期前把菜包送走了，那天弄到了很晚，大概晚上一点多吧，第二天配菜玩了一天之后晚上就回来了，回到了空空荡荡的房间，本来菜包在的时候有时候会觉得特别烦它，因为它总是掉毛，而且喜欢霸占我的床，并且喜欢破坏家里的东西，可是那天，没
      
    
    </summary>
    
      <category term="随感" scheme="http://www.wuweiblog.com/categories/%E9%9A%8F%E6%84%9F/"/>
    
    
      <category term="随感" scheme="http://www.wuweiblog.com/tags/%E9%9A%8F%E6%84%9F/"/>
    
  </entry>
  
  <entry>
    <title>Flask服务器linux离线配置</title>
    <link href="http://www.wuweiblog.com/2018/06/20/Flask%E6%9C%8D%E5%8A%A1%E5%99%A8linux%E7%A6%BB%E7%BA%BF%E9%85%8D%E7%BD%AE/"/>
    <id>http://www.wuweiblog.com/2018/06/20/Flask服务器linux离线配置/</id>
    <published>2018-06-19T23:41:11.000Z</published>
    <updated>2018-06-19T23:42:08.211Z</updated>
    
    <content type="html"><![CDATA[<p>Flask为Python的一个轻量级服务器框架，用来做Restful风格的后台程序是十分合适的，再结合Nginx做负载均衡，则整个后台服务器能够承受较大规模的并发和用户访问，在项目中本来是在windows下开发的，不过实际部署环境是linux，弄得有些措手不及，不过好在整个Python框架都是支持跨平台的，所以不管是Windows还是Linux也都能够适用，只是整个环境配置会比较麻烦。<br>涉及到的几个主要的组件为：</p><ul><li>1.Flask以及相关组件</li><li>2.Tornado以及相关组件</li><li>3.SQLAlchemy以及相关组件</li></ul><p>以上三个组件是比较重要的组件，由于每个组件又存在一些依赖关系，所在安装过程中显得比较麻烦，下面我依次介绍每一个组件的安装依赖，以便于在下次部署的过程中能够方便的进行部署：<br>一般来说离线安装组件都是通过pip install 来安装whl文件，因此首先需要安装setuptools以及pip组件。</p><h3 id="Flask组件的安装"><a href="#Flask组件的安装" class="headerlink" title="Flask组件的安装"></a>Flask组件的安装</h3><p>实际上如果是在线环境，则使用yum或者apt-get安装都是极其方便的，在线环境下会自动下载各个依赖，但是在离线环境下就必须先下载安装好各个依赖然后才能够进行Flask的安装，Flask的依赖包括：<br>Werkzeug&gt;=0.7；<br>Jinja2&gt;=2.4, which requires；<br>MarkupSafe；<br>Babel&gt;=0.8, which requires；<br>pytz；<br>itsdangerous&gt;=0.21；<br>实际上安装的过程比较简单，首先下载好离线包，如果没有whl文件就下载源码直接通过编译安装，如果又whl文件则通过pip根据whl文件进行安装，完成依赖库的安装后，就可以安装Flask了，安装的方法同样是下载whl文件然后通过pip install进行安装；</p><h3 id="Tornado组件的安装"><a href="#Tornado组件的安装" class="headerlink" title="Tornado组件的安装"></a>Tornado组件的安装</h3><p>Tornado的安装比较简单，首先下载源码，实际上tornado并没有找到whl文件所以只能够通过源码安装，安装的方式相对来说也比较简单，首先通过tar -zxvf命令解压文件，然后进入文件夹，这是应该又一个setup.py文件，直接通过python setup.py instll 安装tornado，安装过程中没有错误则说明安装完成；</p><h3 id="SQLAlchemy的安装"><a href="#SQLAlchemy的安装" class="headerlink" title="SQLAlchemy的安装"></a>SQLAlchemy的安装</h3><p>要通过python连接MySQL数据库，则需要一些组件，而且整个过程相对会复杂一些，首先需要安装python-devel，在安装的过程中一定要注意python的版本，下载的时候也最好下载对应操作系统，对应python版本的库，以免引起冲突；python-devel是一个rpm库，直接通过rpm -ivh命令进行安装，若在安装过程中没有提示错误，则说明安装成功，安装python运行库后需要安装mysqldb组件，首先下载python-MySQLdb压缩文件，进入压缩文件目录后可以看到有setup.py文件，直接通过python install进行安装就可以了。</p><p>完成以上所有组件的安装后直接运行服务，如果还缺某一个组件，则会提示import的头文件不存在，此时只需要将文件下载下来安装就可以了，整个配置过程是比较方便的，另外要注意的就是python连接数据库的组件，需要注意操作系统和python的版本，安装正确的版本才能够正确是使用</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;Flask为Python的一个轻量级服务器框架，用来做Restful风格的后台程序是十分合适的，再结合Nginx做负载均衡，则整个后台服务器能够承受较大规模的并发和用户访问，在项目中本来是在windows下开发的，不过实际部署环境是linux，弄得有些措手不及，不过好在整个
      
    
    </summary>
    
    
      <category term="学习" scheme="http://www.wuweiblog.com/tags/%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>月亮与六便士</title>
    <link href="http://www.wuweiblog.com/2018/06/17/%E6%9C%88%E4%BA%AE%E4%B8%8E%E5%85%AD%E4%BE%BF%E5%A3%AB/"/>
    <id>http://www.wuweiblog.com/2018/06/17/月亮与六便士/</id>
    <published>2018-06-17T05:24:33.000Z</published>
    <updated>2018-12-08T10:26:53.150Z</updated>
    
    <content type="html"><![CDATA[<p>&nbsp;&nbsp;&nbsp;&nbsp;很久没有写点东西了，前一段时间一直太忙了，晚上还要检查代码，江门的项目的部署以及香港项目的结题的事情都压在一起，慌乱得不行，导致了没有时间好好总结一下，正好趁着这个假期把该总结的东西给总结了，把想要写的文章给写了，也算是给自己一个交代。<br>&nbsp;&nbsp;&nbsp;&nbsp;《月亮与六便士》也算得上是一部著名的作品了，前一段时间还是把它看完了，不得不说相比于网络口水文，经典文学的作品看起来要艰难很多，但是也给人更多的思考。作品以法国画家高更为原型，塑造了一个证券经纪人突然抛弃妻子，奔赴一所不知名的小岛将自己的余生专注于绘画的传奇故事。<br>&nbsp;&nbsp;&nbsp;&nbsp;其实倒也说不上传奇，只是这个故事会给我们的价值观带来很大的冲击罢了，一个证券经纪人，放弃安稳的工作，抛下家人，抛弃自己的妻儿和所有财富，只身在一个陌生的地方，把自己的所有精力，精神以及生命都投入到了自己热爱的绘画工作中，直到死去。这其实想想我们大多数人有何尝不是想思特里克兰德的前半生一样，为了生活而苟且，为了生活中的菜米油盐而放下了诗和远方。也许生活不只眼前的苟且，可是我们是否又有勇气去追求我们心中的诗和远方呢？恐怕不是所有人都有放下一切重新开始的勇气吧。我们是否又能够真的看清自己内心的期盼呢，是否我们现在正在过的生活就是我们想要的生活，是否只有名利和金钱才能够满足我们日益增长的欲望呢。小说始终只是小说，说白了也就是如何过完自己的一生而已，我们是要做别人眼中的成功人士还是做自己心中的英雄？这的确是一个两难的问题，选择太难，所以大部分人其实都是在夹缝中求生存而已，所以我们可以做着自己不喜欢的工作而寄希望于有朝一日能够财务自由而去做想做的事情。我们口口声声要来一场说走就走的旅行。如果内心热爱，何必财务自由，如果内心自由何必旅行呢！<br>&nbsp;&nbsp;&nbsp;&nbsp;理想的丰满和现实的骨干永远是不可调和的矛盾，我们永远都在选择，是遵从内心还是屈服于这个世界，思特里克兰德能够遵从于自己的内心，能够不在乎这个世界的意见，最后他的成功成为了他传奇一生的注脚，可是我们普通人呢，我们放下了所有又是否能够找到一生所爱呢？对于大多数人来说月亮虽然美丽，可是却遥不可及，而眼前的六便士却能够解决我们生活的大多数问题，如何选择自然心中有数吧！</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;很久没有写点东西了，前一段时间一直太忙了，晚上还要检查代码，江门的项目的部署以及香港项目的结题的事情都压在一起，慌乱得不行，导致了没有时间好好总结一下，正好趁着这个假期把该总结的东西给总结了，把想要写的文章给写了，也算是给自己
      
    
    </summary>
    
      <category term="书评" scheme="http://www.wuweiblog.com/categories/%E4%B9%A6%E8%AF%84/"/>
    
    
      <category term="月亮与六便士" scheme="http://www.wuweiblog.com/tags/%E6%9C%88%E4%BA%AE%E4%B8%8E%E5%85%AD%E4%BE%BF%E5%A3%AB/"/>
    
  </entry>
  
</feed>
