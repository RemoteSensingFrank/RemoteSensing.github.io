<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>吴蔚</title>
  
  <subtitle>生命不息，折腾不止！</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://www.wuweiblog.com/"/>
  <updated>2019-01-06T03:38:02.966Z</updated>
  <id>http://www.wuweiblog.com/</id>
  
  <author>
    <name>John Doe Thanks the author of the theme</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>猝死</title>
    <link href="http://www.wuweiblog.com/2019/01/06/%E7%8C%9D%E6%AD%BB/"/>
    <id>http://www.wuweiblog.com/2019/01/06/猝死/</id>
    <published>2019-01-06T03:38:02.000Z</published>
    <updated>2019-01-06T03:38:02.966Z</updated>
    
    <summary type="html">
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>我有两盏灯</title>
    <link href="http://www.wuweiblog.com/2018/12/31/%E6%88%91%E6%9C%89%E4%B8%A4%E7%9B%8F%E7%81%AF/"/>
    <id>http://www.wuweiblog.com/2018/12/31/我有两盏灯/</id>
    <published>2018-12-31T15:28:53.000Z</published>
    <updated>2018-12-31T16:58:11.796Z</updated>
    
    <content type="html"><![CDATA[<p>&nbsp;&nbsp;&nbsp;&nbsp;马上就是自然年的2019年了，每年都会给自己做一个总结，或者实在农历新年或者是在自然年，当然今年也不能例外。想要总结的有很多，酒喝的刚刚好，有点微醺不太适合去写代码，最适合写点东西或者是做总结了。今天回来的时候打开了常开的那盏灯，然后思考了很久关上了，打开了了另一盏。所以才有了这个总结的题目，回家的时候刷知乎看到一个问题“哪些观念是越早建立越好的”其中有一个回答是“人总是要面临死亡的”，这个回答对我来说很重要，其实很早就面对过死亡，不过只是以一个旁观者的身份。高中的时候有个同学的父亲因为白血病去世了，那个同学是我一直以来的好朋友，朋友父亲的离世自然是很悲伤，不过那种悲伤可能更多的是同情，是以一种旁观者的姿态去感受所以并没有这么深刻。第二次是本科毕业小妹去世，小妹是我高中同学，在她去世的前几个月我们还一起吃了顿饭，真的想不到就这样离开了我们，这个世界的繁华我想怎么都还没有看够吧，就这么离开这个世界多少有些遗憾吧~小妹的离去让我感受到了生命的脆弱和遗憾；第三次是我大姑的去世，亲人的离世总是会有更多的感慨，看到小姑在葬礼上哭的撕心裂肺，看到爸爸和伯伯红肿的眼睛，还有两个表哥强撑着疲惫的身躯招呼客人，其实不管我们多么的平凡，总是会有人因为我们而欢喜悲伤的吧。<br>&nbsp;&nbsp;&nbsp;&nbsp;所经历的死亡也就这三次了。一次比一次近，所以有时候也会想其实明天和死亡的谁也不知道哪个先到来，所以死亡算是我生命历程中的一盏明灯了，随着我生命的继续它会越来越亮，直到生命的终点。作为一个有限的人，其实死亡一直在我们头顶。其实想想死亡也算是一件好事，千百千年的古人就知道生也有涯，不要以有涯而随无涯。所以我们有限的生命应该做些什么呢？或者又是什么都不做？哲学是个好东西，看起来那些对于永恒对于生命的思考没有什么意义，但是在需要的时候这些对人或者世界最本质的探索就成为了我们不可动摇的磐石，从芝诺到伊比鸠鲁，在死亡的约束下我们有了很多思考，不管怎么说，死亡总是指引着我们向未来探索，或者说不过一死而已未来又有什么可恐惧的呢！<br>&nbsp;&nbsp;&nbsp;&nbsp;另外一盏灯对我来说应该是理想了，很多人会忽略这个东西，但是我不会，死亡是现实如果只是考虑死亡那么我们很快就会得出伊比鸠鲁学派的结论，这样我们就很难理解那些为了全人类而努力奋斗直到牺牲的人了。所以一定有什么东西是能够与死亡抗衡的，它的存在不会随着个人的死亡而消失，这个就是理想了吧。其实在这里说起来显得有些空谈，但是我确实认为这个是一个很重要的东西，面对死亡名利财富，一切的一切都会消失，那么我们终其一生不管有意或者无意都在寻找一些能够与死亡抗衡的东西，对于生物来说最简单也是最直接的那就是基因，所以动物会争夺配偶，为了让基因摆脱死亡的限制传递下去。而我们人类作为自认为高级一些的生物体，基因的延续已经不是我们对抗死亡的唯一方式二零，我们还有一些更高级的方式，那就是理想。其实说起来很虚无缥缈，但是实际上很有必要，我终其一生是想要做些什么？这个就是理想，我所作的对于整个人社会或者人类有意义，或者能够被认可，然后虽然我倒在了通往理想的路上，但是毕竟开辟了一条通往理想的道路，然后更多的人会沿着这条路往前走。以前读鲁迅，知道了有些人想把名字刻在碑上试图不朽，有的人却化作了青青野草。实际上不管是刻在石碑上还是化作野草，他们的目的有一点是相同的，那就是去死亡的对抗。我们必须找一些事物来对抗死亡，然后我们才能超脱死亡。<br>&nbsp;&nbsp;&nbsp;&nbsp;跨年了，死亡的灯又亮了几分，而我理想的灯还是暗淡无光，时间已经不多了，而我必须尽快点亮那生命中的第二盏灯以免在恐惧中失去了自我。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;马上就是自然年的2019年了，每年都会给自己做一个总结，或者实在农历新年或者是在自然年，当然今年也不能例外。想要总结的有很多，酒喝的刚刚好，有点微醺不太适合去写代码，最适合写点东西或者是做总结了。今天回来的时候打开了常开的那盏
      
    
    </summary>
    
      <category term="随感" scheme="http://www.wuweiblog.com/categories/%E9%9A%8F%E6%84%9F/"/>
    
    
      <category term="随感" scheme="http://www.wuweiblog.com/tags/%E9%9A%8F%E6%84%9F/"/>
    
  </entry>
  
  <entry>
    <title>concul服务注册与发现</title>
    <link href="http://www.wuweiblog.com/2018/12/31/concul%E6%9C%8D%E5%8A%A1%E6%B3%A8%E5%86%8C%E4%B8%8E%E5%8F%91%E7%8E%B0/"/>
    <id>http://www.wuweiblog.com/2018/12/31/concul服务注册与发现/</id>
    <published>2018-12-31T08:34:08.000Z</published>
    <updated>2018-12-31T08:36:45.784Z</updated>
    
    <content type="html"><![CDATA[<h2 id="关于服务注册于发现Consul"><a href="#关于服务注册于发现Consul" class="headerlink" title="关于服务注册于发现Consul"></a>关于服务注册于发现Consul</h2><h3 id="微服务"><a href="#微服务" class="headerlink" title="微服务"></a>微服务</h3><p>&nbsp;&nbsp;&nbsp;&nbsp;对于微服务的应用来说由于客户端以及服务端应用接口特别多而且复杂，对于各个客户端之间的通信是一件很复杂得事情，通常情况下一个客户端出现了问题而停止或者更换了端口或者部署机器的地址都会导致其他与之相关联的服务都需要切换端口和IP，这会带来特别大的影响，为了降低服务部署和修改带来的影响于是产生了服务注册于发现的机制。不管什么架构设计，脱离了需求来谈架构就是空谈，但是对于一个公司有着许多不同的业务板块，每个板块之间基本独立但是又有区分，因此使用微服务的架构就是很有必要的了（避免单个服务做得过大导致一旦出现问题整个系统崩溃），提到服务注册和服务发现就不得不提一下微服务的构架，如果采用单体服务的模式，整个系统前后端统一，单体化部署使用服务注册于服务发现的框架反而增加了程序负担这样就得不偿失了：<br>微服务这个概念实际上相对来说比较新，从2012年提出到现在也才经过6年的时间，实际上从微服务被普遍认可和接收也就从2015年开始，我前几天读完了一本关于微服务架构的书也对微服务有了些了解，实际上整个微服务就是我们程序设计中的高内聚低耦合的集中体现。谈到微服务就不能不提SOA，这两种架构之间有着千丝万缕的联系，我实际上也没有真正执行过SOA架构或者微服务架构的应用（以后会执行的~)我只能谈谈我的理解了，对于一个应用来说，采用SOA架构主要过程是首先对业务整体情况进行了解，然后将整体业务拆分为独立的业务逻辑，然后对每一块业务逻辑流程进行分别开发，最后进行集成；而采用微服务的架构可能拆分得更加细粒度，举一个例子如图：  </p><center><img src="https://blogimage-1251632003.cos.ap-guangzhou.myqcloud.com/%E5%BE%AE%E6%9C%8D%E5%8A%A1vsSOA.jpeg"></center>    <p>图片来自<a href="https://blog.csdn.net/u011389515/article/details/80546084" target="_blank" rel="noopener">CSDN博客</a>侵删联系（<a href="mailto:wuwei_cug@163.com" target="_blank" rel="noopener">wuwei_cug@163.com</a>）上面一张图很清晰的说明了SOA与微服务之间的差别，SOA以业务逻辑为粒度构建应用，而微服务架构构建更加细的粒度的独立应用，然后将应用组合起来形成业务逻辑。简单的来说就是对于服务的理解有差异，当然实际上SOA还有EBS企业总线，数据总线等概念，在这里就不详细的描述了。  </p><h3 id="Consul原理"><a href="#Consul原理" class="headerlink" title="Consul原理"></a>Consul原理</h3><p>&nbsp;&nbsp;&nbsp;&nbsp;从上图中可以看到对于一件简单的应用微服务会将其拆分为很多服务然后提供接口，那么实际上这么多服务的管理自然而然就成为了一个需要考虑的问题，实际上对于这个问题SOA也出现过，而SOA采用一个很复杂的逻辑EBS总线去管理所有服务间的通信与调度以及服务的健康检查，实际上微服务并不需要这么一个复杂得逻辑，因为微服务本身的接口很清晰应用服务之间逻辑界限很明确，因此不需要一个过于复杂得调度总线，但是也需要一个服务注册于服务发现的管理，这个就是我们要着重介绍的Consul，在这里我只介绍Consul的使用以及服务的注册，更多的内容还在摸索当中：<br>&nbsp;&nbsp;&nbsp;&nbsp;Consul的架构如图：  </p><center><img src="https://blogimage-1251632003.cos.ap-guangzhou.myqcloud.com/consul%E7%BB%93%E6%9E%84.png"></center>  <p>上面这张图示从<a href="https://www.consul.io/docs/internals/architecture.html" target="_blank" rel="noopener">官网</a>下载下来的图，随之还有官网的描述，英文好的同学可以直接看原文，我会在每一个描述上添加上自己的理解。  </p><ul><li><p>Let’s break down this image and describe each piece. First of all, we can see that there are two datacenters, labeled “one” and “two”. Consul has first class support for multiple datacenters and expects this to be the common case.<br>这里提到了一个特性，多数据中心支持，数据中心实际上只是一个局域网络比如我们的内网，私有云等，一般数据中心之间的数据是不进行交互的实际上不太能明白多数据中心支持的意义在哪里，是不是为了内外网部署应用的时候能够有一个统一的管理？</p></li><li><p>Within each datacenter, we have a mixture of clients and servers. It is expected that there be between three to five servers. This strikes a balance between availability in the case of failure and performance, as consensus gets progressively slower as more machines are added. However, there is no limit to the number of clients, and they can easily scale into the thousands or tens of thousands.<br>上面谈到了关于数据中心，我们暂时不去管数据中心，我们看每一个数据中心中的部署，对于每一个数据中心其构成为分部式部署架构，主要分为client模式和server模式上面这段说明了server模式和client模式的数量，对于这两种模式其实官网也有一个解释：<br><strong>Client</strong> - A client is an agent that forwards all RPCs to a server. The client is relatively stateless. The only background activity a client performs is taking part in the LAN gossip pool. This has a minimal resource overhead and consumes only a small amount of network bandwidth.<br><strong>Server</strong> - A server is an agent with an expanded set of responsibilities including participating in the Raft quorum, maintaining cluster state, responding to RPC queries, exchanging WAN gossip with other datacenters, and forwarding queries to leaders or remote datacenters<br>官网的解释很抽象，我不太想去翻译，翻译出来没有太大的意义，我在这里谈谈我自己的理解，实际上整个服务注册说明所有注册的服务都有一个统一的维护那就是所说的LAN gossip pool，在这个数据池中存储了所有的注册服务，而客户端的作用就是有一个服务注册进来了，那么就将这个服务添加到LAN gossip pool中，另外客户端的作用就是转发查询请求给服务端。客户端的功能比较简单，相对复杂的是服务端，服务端包含客户端的功能此外还需要进行健康检查和响应查询，服务端包含leader和follower，理论上每个服务端都对注册的服务进行健康检查，然后发送给leader，通过投票机制来确定注册的服务的健康状况，因此官方推荐的服务端为3个或5个，必须为奇数个，且不宜太多增加check的花销。</p></li></ul><ul><li><p>All the nodes that are in a datacenter participate in a gossip protocol. This means there is a gossip pool that contains all the nodes for a given datacenter. This serves a few purposes: first, there is no need to configure clients with the addresses of servers; discovery is done automatically. Second, the work of detecting node failures is not placed on the servers but is distributed. This makes failure detection much more scalable than naive heartbeating schemes. Thirdly, it is used as a messaging layer to notify when important events such as leader election take place.<br>上面这一段话中唯一要注意的就是那个thrid，其他亮点在上一个客户端和服务端的解析中已经说明了，因此着重讲讲我对第三点的理解，实际上选举出leader是一个很重要的部分，consul是通过Raft协议选举出leader，具体的方法我查找了很多地方在<a href="https://stackoverflow.com/questions/27724519/how-does-a-consul-agent-know-it-is-the-leader-of-a-cluster" target="_blank" rel="noopener">stackoverflow</a>上略有提及，是通过一个叫randomized timers的方法选取的，具体的方法需要参看<a href="https://ramcloud.stanford.edu/wiki/download/attachments/11370504/raft.pdf" target="_blank" rel="noopener">论文</a>如果我有时间看论文的话在具体说说这个方法吧。现在没有时间看所以暂且放一放</p></li><li><p>The servers in each datacenter are all part of a single Raft peer set. This means that they work together to elect a single leader, a selected server which has extra duties. The leader is responsible for processing all queries and transactions. Transactions must also be replicated to all peers as part of the consensus protocol. Because of this requirement, when a non-leader server receives an RPC request, it forwards it to the cluster leader.<br>这里承接上面一段说了一些关于服务节点同步以及leader事务同步的问题，这个深入讨论就涉及代码层了，等我有时间阅读代码之后再讨论。</p></li></ul><ul><li>The server nodes also operate as part of a WAN gossip pool. This pool is different from the LAN pool as it is optimized for the higher latency of the internet and is expected to contain only other Consul server nodes. The purpose of this pool is to allow datacenters to discover each other in a low-touch manner. Bringing a new datacenter online is as easy as joining the existing WAN gossip pool. Because the servers are all operating in this pool, it also enables cross-datacenter requests. When a server receives a request for a different datacenter, it forwards it to a random server in the correct datacenter. That server may then forward to the local leader.  </li><li>This results in a very low coupling between datacenters, but because of failure detection, connection caching and multiplexing, cross-datacenter requests are relatively fast and reliable.</li><li><p>In general, data is not replicated between different Consul datacenters. When a request is made for a resource in another datacenter, the local Consul servers forward an RPC request to the remote Consul servers for that resource and return the results. If the remote datacenter is not available, then those resources will also not be available, but that won’t otherwise affect the local datacenter. There are some special situations where a limited subset of data can be replicated, such as with Consul’s built-in ACL replication capability, or external tools like consul-replicate.<br>上面提到的关于多数据中心的作用以及使用的集中体现，实际上我认为没有太大的必要做多中心，本省数据中心之间也不会互相访问，不知道多数据中心的管理意义在哪里，想来不过也就是增加了数据中心的扩展性，但是实际应用中不会多中心部署吧，与其这样还不如每个私有云部署一个来的方便，不过如果多中心之间有通信的话可能还是有些作用的，这个可能需要应用场景的支撑了，现在我还想不出有什么意义。</p></li><li><p>In some places, client agents may cache data from the servers to make it available locally for performance and reliability. Examples include Connect certificates and intentions which allow the client agent to make local decisions about inbound connection requests without a round trip to the servers. Some API endpoints also support optional result caching. This helps reliability because the local agent can continue to respond to some queries like service-discovery or Connect authorization from cache even if the connection to the servers is disrupted or the servers are temporarily unavailable.<br>上面这段提到了一种客户端缓存数据的特殊情况，主要是为了提高可靠性，通过客户端缓存某些数据以免在客户端与服务端通信不畅或服务端暂时挂起的状态下还能够保持响应的可用性。</p></li></ul><p>&nbsp;&nbsp;&nbsp;&nbsp;上面讲了一大段对于架构的介绍以及高可用性的部署，客户端服务端的分工以及各种协议的通信实际上就是为了解决服务注册和发现的问题，看似一个很小的问题却考虑了一个这么庞大的架构来处理就存在一个有没有必要的问题，对于少量服务的应用需求实际上光是理解这个consul就是一个很高的成本，采用服务注册于发现的机制完全是没有必要的，但是对于企业级的应用来说却是很有必要的，其必要性主要体现在：1.服务运维，从运维的角度来说服务的动态变化实际上极大的增加的运维测试的负担，必须要对服务有一个集中管理的过程；2.从可扩展性的角度来说，通过此种方式在分布式架构下大大提高了服务的可扩展性，甚至可以说是随意扩展；3.从开发的角度来说通过服务发现统一了接口，避免由于部署的差异导致接口的变化，当然这个实际上也可以理解为运维测试的好处。</p><h3 id="consul使用"><a href="#consul使用" class="headerlink" title="consul使用"></a>consul使用</h3><p>&nbsp;&nbsp;&nbsp;&nbsp;提了一大堆的介绍，从开发的角度说一下Consul的使用，因为死单机所以我采用的是单机Docker部署的方式，安装环境为Windows10 开发语言为python3.6.x。首先运行consul:<a href="https://livewyer.io/blog/2015/02/05/service-discovery-docker-containers-using-consul-and-registrator/" target="_blank" rel="noopener">参考网上教程</a>运行了一个单节点的consul，这个是最简单的运行，多节点就是添加节点，这个以后再说。运行完成之后在浏览器可以看到如下页面：  </p><p><center><img src="https://blogimage-1251632003.cos.ap-guangzhou.myqcloud.com/consul-ui.jpg"></center><br>&nbsp;&nbsp;&nbsp;&nbsp;从上面的界面上可以看到服务运行的状态信息，有一个consul 的节点，另外运行了一个datastorage的服务，不过这个服务failling，是一个失败的服务，因为我停掉了服务器。我们可以根据上面的参考教材通过命令去注册服务，当然我这里使用python-consul直接在服务启动的时候进行注册，具体的代码为:  </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"><span class="keyword">import</span> mongoRoute <span class="keyword">as</span> MongoRoute</span><br><span class="line"><span class="keyword">import</span> instance</span><br><span class="line"><span class="keyword">import</span> consul</span><br><span class="line"></span><br><span class="line">api = instance.api</span><br><span class="line">app = instance.app</span><br><span class="line">conf= instance.conf</span><br><span class="line"><span class="comment">#help info </span></span><br><span class="line"><span class="meta">@app.route('/filestorage/v1.0.0/help') </span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">help</span><span class="params">()</span>:</span> </span><br><span class="line">   <span class="keyword">return</span> <span class="string">'help info'</span> </span><br><span class="line"></span><br><span class="line"><span class="comment">#for health check</span></span><br><span class="line"><span class="meta">@app.route('/filestorage/v1.0.0/check', methods=['GET'])  </span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">check</span><span class="params">()</span>:</span></span><br><span class="line">   <span class="keyword">return</span> <span class="string">'success'</span></span><br><span class="line"></span><br><span class="line">api.add_resource(MongoRoute.LargeFileUploadChunkMongo,   <span class="string">'/filestorage/v1.0.0.0/mongodb/uploadlargechunk'</span>)</span><br><span class="line">api.add_resource(MongoRoute.LargeFileUploadFinishedMongo,<span class="string">'/filestorage/v1.0.0.0/mongodb/uploadlargefinished'</span>)</span><br><span class="line">api.add_resource(MongoRoute.SmallFileUploadMongo,        <span class="string">'/filestorage/v1.0.0.0/mongodb/uploadsmall'</span>)</span><br><span class="line"></span><br><span class="line">api.add_resource(MongoRoute.LargeFileDownloadMongo,      <span class="string">'/filestorage/v1.0.0.0/mongo/downloadlarge'</span>)</span><br><span class="line">api.add_resource(MongoRoute.SmallFileDownloadMongo,      <span class="string">'/filestorage/v1.0.0.0/mongo/downloadsmall'</span>)</span><br><span class="line">api.add_resource(MongoRoute.SmallFileDownloadAsFileMongo,<span class="string">'/filestorage/v1.0.0.0/mongo/downloadsmallasfile'</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#services register</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">consul_service_register</span><span class="params">()</span>:</span></span><br><span class="line">   client = consul.Consul()</span><br><span class="line">   service_id = <span class="string">"datastorage-localhost:8081"</span></span><br><span class="line">   httpcheck  = <span class="string">"http://192.168.1.25:8081/filestorage/v1.0.0/check"</span></span><br><span class="line">   check = consul.Check.http(httpcheck, <span class="string">"30s"</span>)</span><br><span class="line">   client.agent.service.register(name=<span class="string">"datastorage"</span>,service_id=service_id,address=<span class="string">'192.168.1.25'</span>,</span><br><span class="line">                  port=<span class="number">8081</span>,tags=[<span class="string">'filestorage'</span>],check=check)</span><br><span class="line"></span><br><span class="line"><span class="comment">#services unregister</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">consul_service_unregister</span><span class="params">()</span>:</span></span><br><span class="line">   client = consul.Consul()</span><br><span class="line">   service_id = <span class="string">"datastorage-localhost:8081"</span></span><br><span class="line">   client.agent.service.deregister(name=<span class="string">"datastorage"</span>,service_id=service_id)</span><br><span class="line"></span><br><span class="line"><span class="comment"># start instance </span></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>: </span><br><span class="line">   <span class="keyword">try</span>:</span><br><span class="line">      consul_service_register()</span><br><span class="line">      app.run(host=<span class="string">'0.0.0.0'</span>,port=<span class="number">8081</span>)</span><br><span class="line">   <span class="keyword">except</span> RuntimeError <span class="keyword">as</span> msg:</span><br><span class="line">      <span class="keyword">if</span> str(msg) == <span class="string">"Server going down"</span>:</span><br><span class="line">         consul_service_unregister();</span><br><span class="line">         print(msg)</span><br><span class="line">      <span class="keyword">else</span>:</span><br><span class="line">         print(<span class="string">"server stopped by accident!"</span>)</span><br></pre></td></tr></table></figure><p>以上的项目具体代码参看<a href="https://github.com/RemoteSensingFrank/micro_server" target="_blank" rel="noopener">我的Github</a>实际上就是给出注册的服务的id，服务名，然后给一个check的地址，然后consul的server会查询这个check的地址确认服务是否可用，在上面我用的是IP地址，因为服务搭建在docker中，如果用localhost是无法访问到的因此注册的时候要用绝对地址。<br>&nbsp;&nbsp;&nbsp;&nbsp;以上代码简单的介绍了一下服务注册，由于这个代码只是提供接口，并不需要接额外的服务，因此并不需要使用服务发现模块，具体使用服务发现模块也很简单就是通过服务名或服务id查询可用的host以及port然后请求就好了，如果查询不到则说明服务不可用。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;关于服务注册于发现Consul&quot;&gt;&lt;a href=&quot;#关于服务注册于发现Consul&quot; class=&quot;headerlink&quot; title=&quot;关于服务注册于发现Consul&quot;&gt;&lt;/a&gt;关于服务注册于发现Consul&lt;/h2&gt;&lt;h3 id=&quot;微服务&quot;&gt;&lt;a href=
      
    
    </summary>
    
      <category term="学习" scheme="http://www.wuweiblog.com/categories/%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="系统架构 学习" scheme="http://www.wuweiblog.com/tags/%E7%B3%BB%E7%BB%9F%E6%9E%B6%E6%9E%84-%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>原来不如此</title>
    <link href="http://www.wuweiblog.com/2018/12/24/%E5%8E%9F%E6%9D%A5%E4%B8%8D%E5%A6%82%E6%AD%A4/"/>
    <id>http://www.wuweiblog.com/2018/12/24/原来不如此/</id>
    <published>2018-12-23T23:05:21.000Z</published>
    <updated>2018-12-24T13:58:11.851Z</updated>
    
    <content type="html"><![CDATA[<p>&nbsp;&nbsp;&nbsp;&nbsp;跟师父@昊姐说好了每天花点时间做一下总结，实际上也没有做到，真是感到惭愧。这一段时间在驻地开发，时间很紧凑一开始的时候压力确实比较大，几乎每天都在加班，周末也是如此，好在在同事们的帮助下努力完成了任务，业主的要求基本上都完成了，一来展示了我们的技术水平，二来让业主满意在以后可能有项目能进一步合作，不管对公司还是对业主都挺好的。可是有时候我会想，在这个过程中我自己能够学到什么，自己的技术水平是否在这个过程中得到了提高？从天河回家的路上一直都会思考这个问题，我觉得应该是有提高的，起码对于ArcGIS的开发我的了解应该是比较深入了。但是GIS开发实际上就那些东西，真正揭开了它的面纱之后发现也不过如此了，并没有什么特别的意思，在有限的基础上实现更好的展示效果需要付出巨大的努力才能做到，想要比别人好一点点也是要付出巨大的努力，实际上也是这样，任何一次的进步一次突破都是在困境中反复纠结才能完成，如果不经历这些恐怕是很难有深刻的体会。<br>&nbsp;&nbsp;&nbsp;&nbsp;除了GIS开发以外最近也一直在思考关于系统构架与新技术的问题，主要内容包括：</p><ul><li>1.微服务构架的组成以及在我所接触到的所有产品和服务或者说我开发的产品中是否需要使用到微服务架构；</li><li>2.微服务架构服务的划分以及部署方式，由于服务如此多，如何能够进行快速部署或一键部署；</li><li>3.服务的注册与服务发现，实际上由于服务数目极多，为了维护多个服务需要一个服务注册和管理模块对每一个服务进行管理；</li><li>4.认证中心的理论和实现过程，基于系统开放性要求如果需要接入其他应用是否需要使用认证中心做统一授权；</li><li>5.数据存储模式：分布式存储与集中存储，SQL或者是NoSQL；</li></ul><p>&nbsp;&nbsp;&nbsp;&nbsp;当然，对于每一个问题的思考都伴随着各种尝试，但是由于并没有人指导和协助，因此这个过程是一个极其痛苦的过程，不过我相信任何的付出总是能够得到相应的回报，最近也有一些小小的成果出现，希望能够做得更好一些，当然也希望自己能够掌握得更多，更加完整和全面而不是对整体情况一知半解。我想每一份的努力都是好的，都应该是让自己感到快乐的事情吧。<br>&nbsp;&nbsp;&nbsp;&nbsp;另外自己总是感觉到有点焦虑，可能确实房贷给我的压力太大了，每个月的工资还了房贷与欠下的债务之后就所剩无几，经济状况不好的情况下就比较容易感觉到焦虑。可能是家里给的压力也太大了，我并不想说我父母他们有多么不好，实际上他们已经很努力的在生活了，但是随着我渐渐离开家乡看到更大的世界，看到了不一样的风景之后我们对于这个世界的体会，对于人生价值的看法有了巨大的分歧，这个不是他们的错，我想也不是我的错，昨天看完了一本书，其中有一句我觉得十分精彩：“天文纪年实际上具有很大的迷惑性，通过天文纪年的方法让我们感觉时间的流逝的相同的，实际上人类社会的发展是一个不断加速，甚至是呈现指数加速的形式，未来十年二十年的变化可能超过过去两百年变化的总和。”实际上虽然我与爸妈在生活了很多年，但是他们依然处在曾经的生活速度之中而我在不断加速，所以矛盾是难以避免的，我只希望他们能够不要想太多，毕竟我所的每一个想法焦虑都是他们所不能理解的，这个世界已经变化太快我们需要不断的调整才能够适应。<br>&nbsp;&nbsp;&nbsp;&nbsp;最近已经有很多人说我不会生活了，其实我还是很热爱生活的，我努力写东西，偶尔去游个泳，或者练琴，或者打球。但是不管是做什么事情都不太想随缘，我希望能游好自由泳，可是总是鼻子呛水，我希望能够拍出好的照片但是实际上大部分照片都不能让我满意，我希望能弹好一首曲子，但是练了半年还是弹得不怎么样。我想想可能还是自己不够喜欢，如果真正喜欢一件事情就一定会做好的，但是我到目前为止好像也没有把某一件事情做得很好，其实想想这也是一件挺可怕的事情。昨天在知乎上发现一个问题“为什么现在的年轻人一边自称佛系一边在孜孜不倦的努力？”我觉得这个问题简直问的太好了，实际上我觉得每一个认清了社会现实的人都会活得佛系，特别是内卷化日益严重的今天。内卷这个词实际上对于我们国家现状来说还算比较新潮，但是这个现象并不新潮。实际上在发达国家也曾出现过垮掉的一代，只是我们近几十年来的社会经济发展掩盖了矛盾。但是不管怎么样矛盾还是存在，随着经济发展减速阶级壁垒逐渐建立起来，凭借个人能力是很难打破阶级壁垒的。所以面对现实的无奈让我们只能选择佛系，毕竟我们做不到“举大计亦死！”终归是要苟延残喘的活着，或者又无所求那不就成了活着的真佛了么。可是我们毕竟不是印度老铁，孔老爷子终究是想兼济天下的。可是如果没有兼济天下的能力那就只能躲进自己的世界成一统，管他春秋冬夏。<br>&nbsp;&nbsp;&nbsp;&nbsp;公司很多小伙伴们都对美食有着执着的追求，真的是很羡慕他们，不知道是不是因为从小老妈做的东西已经十分美味，然后又在学校尝尽各种黑暗料理，以至于对食物失去了兴奋的感觉。其实处于这个信息或者说数据爆炸的时代既是一件美好的事情，又是一件让人绝望的事情，各种数据资料铺天盖地，轻易能够获取的东西让人失去了对未知探索的热情，快餐虽然没有营养但是能够填饱肚子，这样就够了不是么！毕竟这个世界节奏如此之快，已经让人难以招架了。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;跟师父@昊姐说好了每天花点时间做一下总结，实际上也没有做到，真是感到惭愧。这一段时间在驻地开发，时间很紧凑一开始的时候压力确实比较大，几乎每天都在加班，周末也是如此，好在在同事们的帮助下努力完成了任务，业主的要求基本上都完成了
      
    
    </summary>
    
      <category term="随感" scheme="http://www.wuweiblog.com/categories/%E9%9A%8F%E6%84%9F/"/>
    
    
      <category term="随感" scheme="http://www.wuweiblog.com/tags/%E9%9A%8F%E6%84%9F/"/>
    
  </entry>
  
  <entry>
    <title>关于激光点云数据处理</title>
    <link href="http://www.wuweiblog.com/2018/12/17/%E5%85%B3%E4%BA%8E%E6%BF%80%E5%85%89%E7%82%B9%E4%BA%91%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86/"/>
    <id>http://www.wuweiblog.com/2018/12/17/关于激光点云数据处理/</id>
    <published>2018-12-17T08:43:51.000Z</published>
    <updated>2018-12-23T08:37:27.417Z</updated>
    
    <content type="html"><![CDATA[<p>&nbsp;&nbsp;&nbsp;&nbsp;写了一套激光点云的处理代码，顺便水了一篇文章，为了水文章也刷了很多文章，对于激光点云数据的处理有些总结以便于进行进一步的研究与问题的发现；激光点云数据，如果不考虑点云回波信息则可以将其作一个空间中的几何点进行处理。在处理过程中最大的问题在于点云是离散的，不存在拓扑关系，因此通过点云提取信息会在一定的困难，所有对于激光点云信息提取的方法和手段的目的都是为了建立点与点之间的拓扑关系。而点云拓扑关系的建立都必须基于一定的假设，比如假设点云在空间中以某一种分布形式存在，或目标物本身的特征满足某一分布等。基于这个前提就可以对点云进行一系列的分析和处理，然后进一步提取点云的特征。<br>&nbsp;&nbsp;&nbsp;&nbsp;对于点云数据的处理按照我的经验主要分为两大部分，第一个大部分为点云本身的处理，增加点云的信息量，比如对点云的分割分类等操作；第二个部分为从点云中提取信息，比如通过点云识别电力线路植被危险或者是通过点云数据进行变形监测等。实际上从点云中能够提取的信息是比较有限的，特别是如果不对点云做分割分类的条件下，因为点云本身既不连续又不具备空间拓扑关系，两个点云数据集之间很难联系在一起。因此通过第一个处理步骤对点云进行分割和分类就显得十分重要，而对于点云数据的处理也多集中在这个部分。实际上不管是分割还是分类都会面临一个巨大的问题那就是如何判断点云是否属于同一簇，这个时候就需要对点云的分布有一个预先的估计，然后根据分类和分割的结果求取估计参数并根据样本对估计进行假设检验与显著性分析，然后列出显著性分析的分析函数，变化分割结果使得满足显著性分析结果取得最大值，以上整个操作就是点云处理的数学基础。从以上过程分析就可以看出，实际上对于点云分割和分类的处理，创新点一般可从：</p><ul><li>1.点云分布的估计（根据分割目标的不同可能存在不同的分布估计或者是多种分布的结合）；</li><li>2.点云参数估计的求解；</li><li>3.显著性分析或者假设检验的方法和过程；</li></ul><p>&nbsp;&nbsp;&nbsp;&nbsp;以上三点是比较大的方向，至于细节上的创新可以从这三点中进行细分；如果是要写文章的话大概就是从这三个角度去思考了。另外还有一个更加具有挑战性的工作那就是对点云的分布函数进行估计，实际上由于不同地物分布都是不同的，一般我们也可能只会在上面列的三个步骤都的过程中对分布的显著性进行计算然后调整。<br>&nbsp;&nbsp;&nbsp;&nbsp;以上都是常规的点云处理的方法了，随着机器学习方法的发展和流行，基于机器学习的点云处理方法也开始出现，其中影响力比较大的是将CNN方法用于点云处理而提出的PointNet方法了。在理解卷积神经网络的基础上理解卷积神经网络用户点云分类分割上是一件相对比较简单的事情，首先卷积神经网络的卷积核，实际上其主要作用就是提取特征，对于点云来说我们可以将其理解为提取点云的结构特征，通过卷积核将离散的点云结构化处理从而能够更好的求取特征；另外相比于影像点云没有强度，因此在处理的过程中一般是将三维空间进行分割，然后以其中的点云数目作为其强度由此构造一个三维空间的强度图，然后通过三维空间的卷积核进行深度卷积神经网络的学习。但是这个过程可能不一定是这么简单，或者说并不是这么的合理，在构建三维空间强度图的时候并没有考虑点云的实际分布，直接通过卷积核池化操作实际上就是一个暴力求解的过程。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;写了一套激光点云的处理代码，顺便水了一篇文章，为了水文章也刷了很多文章，对于激光点云数据的处理有些总结以便于进行进一步的研究与问题的发现；激光点云数据，如果不考虑点云回波信息则可以将其作一个空间中的几何点进行处理。在处理过程中
      
    
    </summary>
    
      <category term="图像处理" scheme="http://www.wuweiblog.com/categories/%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86/"/>
    
    
      <category term="学习" scheme="http://www.wuweiblog.com/tags/%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>工作狂日记</title>
    <link href="http://www.wuweiblog.com/2018/12/08/%E5%B7%A5%E4%BD%9C%E7%8B%82%E6%97%A5%E8%AE%B0/"/>
    <id>http://www.wuweiblog.com/2018/12/08/工作狂日记/</id>
    <published>2018-12-08T10:22:45.000Z</published>
    <updated>2018-12-08T10:25:50.898Z</updated>
    
    <content type="html"><![CDATA[<p>&nbsp;&nbsp;&nbsp;&nbsp;有一天回家的时候突然情绪有些失控，不知道是不是因为连续工作时间太长，感觉情绪已经快到崩溃的边缘，感觉对什么事情都提不起兴趣。总是感觉在自我肯定和否定中挣扎。其实对自己进来的工作还是比较满意的，就是对自己的生活不太满意，感觉已经失去了自己的生活。每天想着能够练i练琴，或者写点东西，但是回家就变成了玩手机，刷知乎，然后躺在沙发上一觉睡到第二天凌晨。这几周以来在沙发上睡觉的时间远比在床上睡的多，穿着衣服睡觉的时间远比脱掉衣服睡觉的时间多，很多时候都是躺在沙发上玩着手机然后就睡着了。<br>&nbsp;&nbsp;&nbsp;&nbsp;我曾经以为自己是一个自律的人，我觉得我能够很好的管控住自己的欲望，但是其实在自律方面我好像也只是一个普通人，也会被一些快餐的电视剧，一些无聊的视频吸引 ，然后因为这些东西花费大量的时间。所以我很好奇，这些东西的吸引力究竟在什么地方，为什么我不能回家之后坐在书桌前做些能够让自己获取更大满足的事情，而是把时间花在这些东西上。这大概就是看了很多书也 过不好自己一生的典型吧，关于心理学的书看了不少，虽然也有些收获，但是具体到实践上就相差十万八千里了，有的问题并不是能够意识到就能够控制的，这大概就是能力的边界吧。我很喜欢能力的边界这个词，这个词能够让我对自己很清晰的认识而不会盲目追求完美，而实际上我是一个很想要追求完美的人，总是想着能够把事情做到最好，但是最好到底有多好其实没有一个很好的解释，把事情能够做完美的可能性几乎为0，所以总是需要探索自我能力的边界，在自我能力的边界范围内尽力把事情做好，把自己管控好。另外总是有一种恐慌的感觉，也许是看到太多的牛人，总是感觉相差太远，而自己又是一个不太愿意承认不如别人的人，所以一直很有压力，永远感觉自己不够好的滋味实际上并不好受，总是会在自我肯定和自我否定的情绪中挣扎。<br>&nbsp;&nbsp;&nbsp;&nbsp;前几天坐在公交车上发呆，感觉做什么事情都提不起兴趣，感觉自己感到满足或者不满足的阈值都很高，好像没有能够让自己感觉到特别开心的事情，当然也没有让自己感觉到特别不开心的事情，好像好的坏的都这样了，处于一个很消极的状态。其实每隔一段时间就会有这样的感觉，不过这一次来的比较强烈，尤其是在连续加班回家的路上。驻地开发实际上不是一个太愉快的体验，早上没有早餐吃，中午也不能好好休息，在繁华的市中心总是有总疏离的感觉，好像在时刻提醒着自己原来我不属于这里。其实以一个旁观者的态度来体验繁华的都市挺有意思，看着人群匆匆忙忙从眼前走过，仿佛时间被加速了，而我却在时间之外。整个工作的进度还算顺利，该做的事情也都能够保证完成，感觉除我以外大家都挺羡慕来这边的，其实说起我不太愿意来这边驻地好像大家有些不可思议，可能我更愿意简单点的生活，我不想每天思考吃什么好，该去哪个店吃，我好像对衣食住行都没有什么太大的要求，可能对我来说衣食住行只是保证生存的手段而不是最后的目标，相比之下我更在意自我的提升，我会因为练了一首新曲子而开心，我会因为练字有了成效而兴奋，我会因为写了自己满意的代码而满足。我更在意自己本身而不是因为衣食住行给我带来的满足，所以我喜欢每天完全不用思考的吃食堂，按部就班的搭最早的那一班公交到公司。实际上我的精力很有限，不太愿意在这些事情是上花太多心思，而且这些似乎也不是我擅长的事情。<br>&nbsp;&nbsp;&nbsp;&nbsp;其实我并不是一个典型的工作狂，可能最多算得上是非典型的工作狂吧，当然也可能是因为能力不够无法在工作时间内把工作做到让自己满意才被迫加班，但是我觉得工作的目的并不仅仅是挣一份工钱而已，我可以不在意自己到底工作了多长时间，我也不在意需要付出多大的努力才能把事情做好，我更想知道的是工作是否能够对于自己有提升，如果有，我愿意付出更大的努力去掌握和了解每一项技能，花时间去解决每一个问题。我不在意需要付出多大的努力才能变得牛逼，我害怕的是慢慢的自己变成了一个傻逼。 </p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;有一天回家的时候突然情绪有些失控，不知道是不是因为连续工作时间太长，感觉情绪已经快到崩溃的边缘，感觉对什么事情都提不起兴趣。总是感觉在自我肯定和否定中挣扎。其实对自己进来的工作还是比较满意的，就是对自己的生活不太满意，感觉已经
      
    
    </summary>
    
    
      <category term="随感" scheme="http://www.wuweiblog.com/tags/%E9%9A%8F%E6%84%9F/"/>
    
  </entry>
  
  <entry>
    <title>我曾经历过沧海桑田</title>
    <link href="http://www.wuweiblog.com/2018/11/26/%E6%88%91%E6%9B%BE%E7%BB%8F%E5%8E%86%E8%BF%87%E6%B2%A7%E6%B5%B7%E6%A1%91%E7%94%B0/"/>
    <id>http://www.wuweiblog.com/2018/11/26/我曾经历过沧海桑田/</id>
    <published>2018-11-26T15:21:12.000Z</published>
    <updated>2018-12-08T10:26:08.745Z</updated>
    
    <content type="html"><![CDATA[<p>&nbsp;&nbsp;&nbsp;&nbsp;有些时候心里有很多 想法但是没有来得及记录下来，等到正襟危坐打开电脑想要记录一下的时候已经有些来不及 了，肥宅最近就深有体会。可能是老猪在他这里呆久了，他也染上了老猪的好吃懒做的毛病，曾经肥宅也是一个比较自律的人，具体的自律体现在吃顿饭从来不超过二十，每天晚上一定要上床睡觉，每天至少能留出一点时间来想姑娘。和老猪呆在一起时间长了之后肥宅发现自己腰不酸了，床不睡了并且连姑娘都不再想了，对此肥宅表示很难过，连姑娘都不想的生活不是他想要的生活，所以他决定找老猪谈谈，当然这么正式的会谈仪式感还是很重要的，所以肥宅特意买了几只猪蹄还有两斤猪头肉和几瓶啤酒，这天老猪看到桌上丰盛的小吃吓得瑟瑟发抖，这是肥宅叫过老猪说：“老猪有没有时间，我有点事想跟你谈谈。”老猪本能的想要拒绝，但是看到肥宅坚定的眼神以及桌上看起来很好吃的猪头肉，情不自禁的点点头，就这样肥宅和猪开启了他们跨越人生与梦想的长谈。<br>&nbsp;&nbsp;&nbsp;&nbsp;肥宅跟猪说：“老猪，最近受你的影响，我发现自己的生活习惯很不好了呀，床也不睡了，姑娘也不想了，你是不是要背这个锅呀！”当然实际上肥宅知道这不是猪的锅，只是他自己似乎已经缺少了些什么东西，但是他是不会承认的。如果不是对于自己不在乎的东西有谁能轻易承认自己的匮乏与无能呢，他想，所以他要给自己找个理由来证明这个不是他自己的原因，此时猪就是最好的背锅侠了。听到这话猪很惊讶：“小老弟呀！你这个锅甩很好呀，每天下班回来躺在沙发刷手机刷到睡着的锅我老猪可不背，至于不去想姑娘的事情，怪我咯？不过说句心里话就你这样，别说姑娘了，母猪可能都看不上你，每天邋里邋遢，目光呆滞毫无亮点，不想姑娘是对的，就算想了估计也是白想。你能不能像我老猪一样每天弄的人模猪样的，出门不知道多少母猪对我抛媚眼，你看看我肥硕腱子肉，你知道我在圈内猪称行走的荷尔蒙，所以说小老弟呀搞清楚重点很重要呀。”肥宅听了老猪的话只能苦笑，行走的荷尔蒙什么的他不知道是不是真的，但是桌上的猪头肉很好吃是真真实实的。看到肥宅吃猪头肉吃的这么开心老猪不禁感到菊花一紧，然后又觉得好像不太对为啥吃猪头肉自己会要菊花一紧，不过这些都不是重点。肥宅听了老猪的话已经陷入了沉思。其实肥宅骨子里是一个自私的人，他所有的努力所有的坚持并不是为了取悦别人，仅仅是为了让自己得到提升，所以很多时候他并不太在乎别人的看法和意见，当然也不存在想要获得别人的关注和吸引其他人的目的，这其实谈不上是一件好事，当然也谈不上是一件坏事，不过可能肥宅的心智不够坚定，所以偶尔会感觉到孤独，偶尔会觉得迷茫。这时他想起了一句话’”他没有选择，他所走的是一条荆棘丛生的道路，他所走的每一步都无比艰难，但是这是一条最坚定和踏实的道路。”也许肥宅现在正在走一条这样的道路吧。想通了这个问题他也不再去纠结关于母猪和姑娘的问题了，但是他对老猪的人生一直很好奇，究竟是一个什么样的环境才能养出一只这样特立独行的猪呢？所以他问猪说：“老猪，说说你自己呗，在来我这里之前你过的是怎么样！”说罢连猪头肉也不吃了，认真的看着猪，听到这话猪觉得自己装逼的机会又来了，正准备高谈阔论一番，这时肥宅幽幽说到：“你这次要是再敢胡扯我就把你送去屠宰场阉掉，所以你说话最好走心。”听到这番话猪吓得差点小便失禁，看来装逼不成了，然后猪就陷入了沉思。猪曾经呆过很多地方，遇到过很多人，有温润如君子的普通人，有狡猾奸诈的普通人，有不善言辞的普通人，有口若悬河的普通人，有美貌光彩照人的普通人，也有长相平平的普通人。这又是一个很长的故事了…</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;有些时候心里有很多 想法但是没有来得及记录下来，等到正襟危坐打开电脑想要记录一下的时候已经有些来不及 了，肥宅最近就深有体会。可能是老猪在他这里呆久了，他也染上了老猪的好吃懒做的毛病，曾经肥宅也是一个比较自律的人，具体的自律体
      
    
    </summary>
    
      <category term="随感" scheme="http://www.wuweiblog.com/categories/%E9%9A%8F%E6%84%9F/"/>
    
    
      <category term="随感" scheme="http://www.wuweiblog.com/tags/%E9%9A%8F%E6%84%9F/"/>
    
  </entry>
  
  <entry>
    <title>kd树的构建</title>
    <link href="http://www.wuweiblog.com/2018/11/11/kd%E6%A0%91%E7%9A%84%E6%9E%84%E5%BB%BA/"/>
    <id>http://www.wuweiblog.com/2018/11/11/kd树的构建/</id>
    <published>2018-11-11T14:04:33.000Z</published>
    <updated>2018-11-11T15:27:17.977Z</updated>
    
    <content type="html"><![CDATA[<p>&nbsp;&nbsp;&nbsp;&nbsp;在这个普天同庆的光棍节的大日子默默的睡了个天昏地暗，然后爬起来写了个专利。实际上写专利的时候又突然搞懂了KD树的构建算法，果然工作使我快乐，以前的时候一直在用KD树，不过都是用的开源的库，所谓的混合索引方式也只是在开源库的基础上进行代码的改造，对于原理的理解还是不够深入，今天在写专利的时候重新梳理了一次觉得终于弄得有点明白了。<br>&nbsp;&nbsp;&nbsp;&nbsp;下面上一张图来说明一下这个过程：<br><img src="https://blogimage-1251632003.cos.ap-guangzhou.myqcloud.com/%E4%BA%8C%E7%BB%B4%E7%A9%BA%E9%97%B4KD%E6%A0%91.jpg">  </p><p>&nbsp;&nbsp;&nbsp;&nbsp;上图是二维空间KD树的划分过程，首先确定划分是从哪一个维度进行，是从x维还是从y维，一般确定维度的过程是计算数据集在各个维度的方差或标准差，从方差或标准差大的维度开始计算进行划分。确定维度后按照该维度进行排序取中位数将点集划分为两个部分，如线1所示，进行第一次划分之后对左右两个点集进行第二次划分如线2所示，然后依次进行划分直到所有点都只属于某一个划分。上图是对于二维点的一个划分，实际上对于高维的点也差不多，由于高维画起来比较麻烦就懒得画了。在解决划分和构造的问题的基础上我们就需要问，如果构造好了一棵树，怎么进行最近邻查询，这个就是我们下面需要讨论的问题。<br>&nbsp;&nbsp;&nbsp;&nbsp;我们用一幅图来展示整个搜索过程：</p><p><img src="https://blogimage-1251632003.cos.ap-guangzhou.myqcloud.com/%E4%BA%8C%E7%BB%B4%E7%A9%BA%E9%97%B4KD%E6%A0%91%E6%90%9C%E7%B4%A2.jpg"><br>以上就是我们的最近邻的搜索过程,红色的五角星为目标点，首先计算目标点到根节点的距离，构成一个超球，判断各个区域是否与超球有相交，通过判断可知P8与P11所在空间与超球没有交集因此忽略这两个部分，然后计算与P3和P9的距离，与根节点的距离比较，如果小于根节点的距离则缩小这个超球，为点到P3的距离，据此可以忽略整个P6右侧所有节点，以及P4节点所在空间；然后遍历剩下的节点，找到最近的节点P1。<br>参考资料：<br><a href="https://leileiluoluo.com/posts/kdtree-algorithm-and-implementation.html" target="_blank" rel="noopener">https://leileiluoluo.com/posts/kdtree-algorithm-and-implementation.html</a><br><a href="https://www.cnblogs.com/earendil/p/8135074.html" target="_blank" rel="noopener">https://www.cnblogs.com/earendil/p/8135074.html</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;在这个普天同庆的光棍节的大日子默默的睡了个天昏地暗，然后爬起来写了个专利。实际上写专利的时候又突然搞懂了KD树的构建算法，果然工作使我快乐，以前的时候一直在用KD树，不过都是用的开源的库，所谓的混合索引方式也只是在开源库的基础
      
    
    </summary>
    
      <category term="算法" scheme="http://www.wuweiblog.com/categories/%E7%AE%97%E6%B3%95/"/>
    
    
      <category term="算法" scheme="http://www.wuweiblog.com/tags/%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>选择</title>
    <link href="http://www.wuweiblog.com/2018/11/08/%E9%80%89%E6%8B%A9/"/>
    <id>http://www.wuweiblog.com/2018/11/08/选择/</id>
    <published>2018-11-07T22:43:38.000Z</published>
    <updated>2018-11-08T04:44:09.000Z</updated>
    
    <content type="html"><![CDATA[<p>&nbsp;&nbsp;&nbsp;&nbsp;前两天我公司带我的师父打电话问我有没有想要去她们那里的意愿，其实我是有些动心的毕竟目前形势看起来她们那里比我们这里似乎会好一点，当然也只是看起来，毕竟我也没有过去待过，不过大概率 的这么说应该是没有问题。这两天都没有怎么睡好其实想的问题很多，其实主要也是针对自己未来的一个去向或者对自己发展的看法。其实自己一直以来都有些焦虑，这也不是第一次机会了，不过相比于上一次，这一次的机会确实让我心动。一直有同事或者领导跟我说要确定自己的核心竞争力，其实来公司经历了这么几个领导也经历了不同的领导风格。实际上我在想，所谓的核心竞争力大概就是帮助客户把事情搞定的能力吧，不管是自己能够做好也好，能够进行人员的安排和管理也好，或者是能写代码也好，终究的目的就是把事情做好而已。那么实际上一个人确实是没有办法既负责项目的沟通和安排又能够做好软件的设计甚至是编码的工作。所以其实核心能力也分为很多部分。商务，管理，视野以及技术能力都是核心竞争力的一个部分，可是几乎没有人能够在各个方面都能够做好，或者说即使有人能够在各个方面都能够做好也会有一个比较优势（托了听了几节经济学课的福），所以我们势必会放弃发展某一些方面来加强我们其他方面的核心竞争力。<br>&nbsp;&nbsp;&nbsp;&nbsp;读书读了这么多 年，工作才一年半不到。其实感觉自己的思想还停留在将自己的技术实力定义为核心竞争力的阶段。其实我喜欢数学，喜欢算法，喜欢从技术的角度解决问题。愿意就具体的问题进行分析，但是不得不说，在很多时候其实技术并不能成为关键问题，或者说在极少的领域或在极少的时候技术水平才会成为关键因素。所以有时候看到吹水真的会很心塞，但是也不得不昧着良心去吹水，我都怀疑自己会不会以后吹水就不昧着良心了，就能够毫无心理负担的吹水了！<br>&nbsp;&nbsp;&nbsp;&nbsp;想想自己工作的这一段时间，除了一些杂事意外正儿八经的接触的项目有三个饿，一个是香港项目，这个项目实际上确实是赶鸭子上架，本来只要做一个开心的码农，安安心心的写代码然后按时干完活，按时上下班就好了的，结果硬着头皮去接了本来应该我师父去干的活，既要自己写代码又需要负责研发管理与甲方和乙方沟通，实际上工作并不算太累，毕竟自己只承担小部分的编码工作，但是当时压力确实很大，生怕自己做不好会造成巨大的损失。当然在项目执行的过程中也有很多的失误，也犯了一些错误，算是磕磕绊绊做完了一个项目；第二个江门的项目现在还处于执行阶段，本来以为是一个很简单的项目，但是实际上现在也开始变得复杂了。因为这个项目我从头开始负责的研发，同时也管理了三个研发人员（虽然是实习生，但是我认为并没有什么区别），从我的角度来说做的并不好。上了Restful API的风格，做了前后端分离，前端单页面实现。整体设计上没有太大的问题，主要的问题在于对于数据流的设计和理解不够深入以至于一直在做修修补补的工作。另外虽然大的方向的设计上没有问题，但是具体实现细节上还有很多东西值得更深入的探讨。第三个是珠海这个，这个项目实际上才启动，但是这个项目在前期做的准备要比江门项目好，在实施上我也有信心能够做的更好，这个项目应该算我成熟期的第一个项目吧，所如果不做好总是有点心慌，不知道自己是不是能够独立去进行项目的管理和架构的设计；另一方面又还是有点心血白费了的感觉。<br>&nbsp;&nbsp;&nbsp;&nbsp;在床上还是有点不舒服，不过总结了一下这一年来的工作经验之后似乎我已经确定了自己的选择，虽然我的选择可能不是最优解（即使目前看起来似乎也不是最优解）但是我想每个人总是有自己坚持的东西吧，总是要坚持一些与地位，或者与收入无关的东西。否则我们这一生不就成为了名利的奴隶了么！</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;前两天我公司带我的师父打电话问我有没有想要去她们那里的意愿，其实我是有些动心的毕竟目前形势看起来她们那里比我们这里似乎会好一点，当然也只是看起来，毕竟我也没有过去待过，不过大概率 的这么说应该是没有问题。这两天都没有怎么睡好其
      
    
    </summary>
    
      <category term="书评" scheme="http://www.wuweiblog.com/categories/%E4%B9%A6%E8%AF%84/"/>
    
    
      <category term="随感" scheme="http://www.wuweiblog.com/tags/%E9%9A%8F%E6%84%9F/"/>
    
  </entry>
  
  <entry>
    <title>不读书(七)</title>
    <link href="http://www.wuweiblog.com/2018/10/31/%E4%B8%8D%E8%AF%BB%E4%B9%A6(%E4%B8%83)/"/>
    <id>http://www.wuweiblog.com/2018/10/31/不读书(七)/</id>
    <published>2018-10-30T22:43:38.000Z</published>
    <updated>2018-11-04T02:08:13.000Z</updated>
    
    <content type="html"><![CDATA[<p>&nbsp;&nbsp;&nbsp;&nbsp;这是一本关于收纳的书，很多时候也被人将断舍离的方法用于面对生活中的其他事情，如工作、情感以及抉择等等。其实看书名就能猜出是日本人写的书，一般来说国人写书不会用这样的名字。其实是一本很有意思的书，日本作为一个空间比较狭小的国家，实际上人均居住面积是严重不足的，因此对于生活空间有很强烈的需求，从而引起了许多收纳法的流行这也是可以理解的。实际上对于我们来说可能居住空间相对较大另外物质生活也相对来说没有这么丰富，因此需求可能没有这么强烈，不过如果处在大城市需求会强烈很多。<br>&nbsp;&nbsp;&nbsp;&nbsp;其实我不是一个会执着于外物的人，所以也没有什么生活物品是无法割舍或者说因为各种原因不愿意舍弃掉的，对于我来说断与舍其实是很容易做到的，另外关于离我得好好说道说道，从这个离字其实看出了慢慢的禁欲系风格。不管收纳得多好，舍弃的多么果断实际上如果没有能控制自己的欲望则很难做到真正的自由。我认识一些朋友，不断的分手然后开始新的恋情，然后分手然后又开始。他们能够很容易割舍上一段感情，但是内心的对于情感欲望的渴求促使着他们马上又进入新的恋情以填补内心的空白，实际上就如同我们的空房子，或者空空如也的书桌，如果总是想着填满房间，摆满书桌，不管能够多么果断的断舍，片刻之后又是一片狼藉。<br>&nbsp;&nbsp;&nbsp;&nbsp;看完书后有一天在去公司的车上，我突然会想，到底我们所谓的满足感与快乐到底是个什么意思。精神自嗨和享受华服珍馐到底那一个能给我们带来更大的满足感或者更好的体验？我们享受更好的服务，买更好品质的衣服，开更好的车，住更大的房子，这些东西所给我们带来的究竟是买他们的一瞬间或者说买他们之后一小段时间内让我们内心满足，还是能够给我们长久的满足？其实我们对于物质生活所能承受的弹性范围是很大的，从受资本主义的苦到享社会主义的福（手动滑稽）其实我们都是能够接受的。高品质的物质生活确实能给我们带来满足和快乐，但是我更愿意相信对于物质生活的变化我们不管是从生理上还是精神上都能够很快适应，然后习以为常，而这些所给我们带来的满足感也会逐渐下降。所以<br>如果说我们所有的体验都是大脑多巴胺分泌刺激的结果，那么实际上做爱和嗑药才是最快乐的事情吧，那做爱到死或者嗑药到死不就是快乐到死么？但是如果可以选择的话我想大概率的人不会在正值芳年的时候选择通过这种方式享受到死吧。<br>&nbsp;&nbsp;&nbsp;&nbsp;其实享受并没有什么不好，无论是物质上的还是精神上的享受，实际上产生的效果都是相同的。但是沉迷于这种简单的胜利生理刺激所带来的快乐是不好的一件事情。性善和性恶之争自古就存在了，我个人比较偏向于性恶论，对于我们个人来说当然是希望能够吃的更好，能够穿的更好，能够睡更软的床，如果可以也希望睡更好看的姑娘（或者是汉子），然后我们不断的丰富我们的大脑，我们发现享受这些能够给我们带来的快乐，或者说享受，其实是很有限的，只是在一定的时期内起到一定的作用。所以在我看来为了获得更加持久的快乐，我们就需要学会割断一些看似能够给我们带来很大的快乐，实际上背后却是无尽空虚的东西。<br>&nbsp;&nbsp;&nbsp;&nbsp;所以说到底，其实所谓的断舍离，立足点还是这个离，如果真的能够离了自己的欲望，那么断和舍也都是自然而然的事情了。然而克制或者说控制自己的欲望其实是一件挺困难的事情，首先我们需要能够深入的剖析和了解自己，然后把自己真正喜爱的东西从那些混杂了虚荣，欲望以及他人期许的需求中抽离，然后将其舍弃。看了这本书，也有很多感悟，当然最重要的是从现在做起，管理好自己，收拾好自己的欲望，中恐怕比家里物品的收纳要来的更加的重要。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;这是一本关于收纳的书，很多时候也被人将断舍离的方法用于面对生活中的其他事情，如工作、情感以及抉择等等。其实看书名就能猜出是日本人写的书，一般来说国人写书不会用这样的名字。其实是一本很有意思的书，日本作为一个空间比较狭小的国家，
      
    
    </summary>
    
      <category term="书评" scheme="http://www.wuweiblog.com/categories/%E4%B9%A6%E8%AF%84/"/>
    
    
      <category term="断舍离，书评" scheme="http://www.wuweiblog.com/tags/%E6%96%AD%E8%88%8D%E7%A6%BB%EF%BC%8C%E4%B9%A6%E8%AF%84/"/>
    
  </entry>
  
  <entry>
    <title>Cesium搭建自己的GIS服务器</title>
    <link href="http://www.wuweiblog.com/2018/10/28/Cesium%E6%90%AD%E5%BB%BA%E8%87%AA%E5%B7%B1%E7%9A%84GIS%E6%9C%8D%E5%8A%A1%E5%99%A8/"/>
    <id>http://www.wuweiblog.com/2018/10/28/Cesium搭建自己的GIS服务器/</id>
    <published>2018-10-28T01:14:15.000Z</published>
    <updated>2018-10-28T02:07:45.590Z</updated>
    
    <content type="html"><![CDATA[<p>&nbsp;&nbsp;&nbsp;&nbsp;虽然是用GIS的路线，但是最近接触了很多其他的GIS研发公司，然后找同学也了解了一下目前行业内的研发情况；另外也有大神向我推荐了一些开源库，目前来说主要就是Cesium这个前端的GIS开源框架。所以也去了解了一下，对于我来说使用开源框架最大的问题在于学习成本，但是从另一个方面来说，由于对国产软件的支持力度日益提高，使用ESRI的平台也可能会面临各种各样的问题，因此我觉得对于各种平台的研究与支持我们是有备无患，在这样的背景之下我去了解了一下Cesium的前端GIS框架，下面主要谈谈基于这个GIS框架搭建一个GIS系统的思路以及一些尝试。  </p><ol><li>首先是数据的支持，对于一个平台来说最大问题在于其对数据的支持，如果能够对各种数据都进行很好的支持那么这个平台就是具有一定潜力的，当然如果连常见的数据格式都不能支持，那么这个平台的生命力一定不会太强，因此我们第一个考虑的就是数据支持的问题，为了了解对于各种数据格式的支持我们查看了其示例代码，同时也针对一些格式进行了测试，其示例代码的<a href="https://cesiumjs.org/Cesium/Build/Apps/Sandcastle/" target="_blank" rel="noopener">网站</a>上有对各种数据格式的支持情况，从网站上我们可以看到，实际上Cesium对于各种数据都是能够比较好的支持的，但是需要转换为它所定义的<strong>3DTiles</strong>的标准，关于<strong>3DTiles</strong>以后如果有时间再细聊下面展示一下我们自己的测试情况：<br><img src="https://blogimage-1251632003.cos.ap-guangzhou.myqcloud.com/cesium%E7%82%B9%E4%BA%91.JPG"><br>上面展示的是分类点云的情况，实际上还有三位模型的加载情况以及KML的加载情况由于截图比较麻烦就不截图进行展示了。</li><li>除了对于所支持的数据格式有要求之外，在实际应用中对于所支持的数据量的大小也是有要求的，如果只能支持少量的数据那么在实际应用中只能作为一个展示的平台，不具有太大的意义，所以能加载的数据量的大小作为一个极其重要的性能指标也需要被考虑。目前我并没有对可加载的数据量的大小进行测试，了解其官网的Demo可以粗略的了解到通过Cesium加载10亿级别的点云是没有问题的，但是这个数据量的加载需要进行测试，另外在加载的过程中一般来说会混合加载多种格式的数据，相比于加载单一格式的数据，多种格式数据同时加载对于平台的压力要远远大于加载同一个格式的数据，因此在测试过程中也需要对平台进行进一步的测试。</li><li>平台工具的使用，实际了解到Cesium是通过一种叫做3DTiles的数据格式来加载数据的，那么对于我们常见的如模型，点云以及倾斜等数据格式，都需要转换为3DTiles的格式，现在面临的最主要的问题是没有现成的工具对各种格式的转换进行很好的支持。通过3DTiles格式的说明我们可以自己编写代码进行转换，但是对于模型和倾斜数据，由于其格式较多且数据格式较为复杂，自己写的转换方式不一定能够适用于所有模型，当然目前也有一些开源的转换代码，但是对于这些转换代码没有进行测试，对其性能以及转换后的数据是否能够成功加载也存在一些疑问，需要进行进一步的测试。第3点问题是开源平台存在的主要问题，相比于成熟的商业平台，开源平台存在着生态不足，且由于开源数据格式多样造成的格式统一的困难。由于这些困难的存在导致开源平台的学习成本很高。</li><li>平台API功能的稳定性以及功能是否完善，关于这个问题我并没有深入的了解，但是做一个球并进行简单的操作是没有问题的，至于更加深入的功能可能需要更进一步的摸索。 </li></ol><p>&nbsp;&nbsp;&nbsp;&nbsp;其实了解完了以上几个问题我们对于这个平台就有一定的了解了，实际上cesium作为一个开源的三位GIS平台是具有很大的潜力的，如果我们自己需要搭建一个功能不复杂的系统可以考虑使用。那么用一个开源的前端球我们需要考虑的是服务端用什么，GIS后台服务可选的有很多开源的如GeoServer商业的如SuperMap以及ESRI等，但是考虑到成本以及既然用了开源就用到底的精神我觉得可以考虑GeoServer，至于三维数据的后台实际上只有一个比较重要的要求那就是服务器，实际上Cesium可以接受通过服务器发布数据服务然后进行展示，所以如果是纯数据则可以直接通过Nginx或者IIS发布静态数据，但是作为一个服务器可能更重要的是上传数据后进行数据的管理以及自动的转换，从这个角度来看就需要编写后台数据格式转换和后台服务的代码，这个应该是最主要的问题，不过实际上也不算太复杂，如果能够了解到各个格式转换为3DTiles的方法，自己编写代码搭建后台服务器也是可以考虑的。<br>&nbsp;&nbsp;&nbsp;&nbsp;最后一个问题就是关于数据量的问题，实际上一个服务器所能承载的数据是有限的，但是通过Nginx等负载均衡的工具也很容易就搭建出一个负载均衡的服务器，所以数据量的问题也不需要过度担心，总的来说在时间允许的条件下，如果对于功能的要求不是特别高的情况下可以考虑通过Cesium+GeoServer+Nginx的方案搭建一套自己的服务器。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;虽然是用GIS的路线，但是最近接触了很多其他的GIS研发公司，然后找同学也了解了一下目前行业内的研发情况；另外也有大神向我推荐了一些开源库，目前来说主要就是Cesium这个前端的GIS开源框架。所以也去了解了一下，对于我来说使
      
    
    </summary>
    
      <category term="学习" scheme="http://www.wuweiblog.com/categories/%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="开发" scheme="http://www.wuweiblog.com/tags/%E5%BC%80%E5%8F%91/"/>
    
  </entry>
  
  <entry>
    <title>关于PCA变换及其应用的梳理</title>
    <link href="http://www.wuweiblog.com/2018/10/14/%E5%85%B3%E4%BA%8EPCA%E5%8F%98%E6%8D%A2%E5%8F%8A%E5%85%B6%E5%BA%94%E7%94%A8%E7%9A%84%E6%A2%B3%E7%90%86/"/>
    <id>http://www.wuweiblog.com/2018/10/14/关于PCA变换及其应用的梳理/</id>
    <published>2018-10-14T06:20:14.000Z</published>
    <updated>2018-10-14T07:21:09.293Z</updated>
    
    <content type="html"><![CDATA[<p>&nbsp;&nbsp;&nbsp;&nbsp;最近写了篇关于PCA变换应用的文章，主要利用了PCA变换能够将信息集中的特点，通过PCA变换，信息集中在前几个主成分上，通过信息量的差异可以进行分类等操作。也读了一些PCA关于PCA变换应用于其他方面的文章，因此对PCA变换进行一个总结与梳理，以期能够在以后更好的对其进行应用。<br>&nbsp;&nbsp;&nbsp;&nbsp;首先介绍一下PCA变换，PCA变换又称为主成分变换其过程可以看作是对数据的重投影，我们可以简单的将PCA变换理解为一个投影变换，将数据从一个正交空间投影到另一个正交空间的过程。在这个过程中最重要的就是投影的正交基的求解，在这里首先解释一下基向量比较学术的解释是:</p><blockquote><p>给定一个向量空间$V$，若$V$中的一组线性无关向量组$B=[e_1,e_2,e_3…]$，对于$V$中任意向量都可以通过$B$线性表示，可以认为向量组$B$为向量空间$V$的一组基</p></blockquote><p>&nbsp;&nbsp;&nbsp;&nbsp;从上面的定义我们可以了解基向量的特征，当然我们最常见的基向量就是正交基，也就是说一组基不仅线性无关而且正交，关于线性无关和正交的区别在这里就不多做解释了，我们下面通过一个简单的例子说明一个二维空间的两组基<br>$$<br>B_1=\begin{bmatrix}0&amp;1\<br>1&amp;0<br>\end{bmatrix}<br>B_2=\begin{bmatrix}1&amp;1\<br>1&amp;-1<br>\end{bmatrix} （1）<br>$$<br>&nbsp;&nbsp;&nbsp;&nbsp;其中$B_1,B_2$为两组二维空间中的正交基，$B_2$可以看作是$B_1$旋转45°的结果。<br>&nbsp;&nbsp;&nbsp;&nbsp;介绍了基向量之后我们可以对PCA变换进行介绍了，从上面的描述中可得PCA变换实质是一个投影变换，因此我们需要找一个投影方向，也就是在变换空间中找到一组基向量。实际上对于任意一个向量空间都存在无数组基，因此我们需要找的一组基应该存在一些约束条件，对于PCA变换来说其约束条件在于<font face="黑体">按照投影后信息量最大的方向进行投影，投影后各个特征之间线性无关。</font>根据以上要求可以计算投影方向。具体为什么需要计算协方差及其特征向量以及PCA变换的具体计算可以参考<a href="https://www.cnblogs.com/dengdan890730/p/5495078.html" target="_blank" rel="noopener">这里</a>。<br>&nbsp;&nbsp;&nbsp;&nbsp;主要还是要说明一下PCA变换的应用意义：</p><ul><li>变量之间的去相关性；</li><li>找到信息量最大的方向；</li><li>垂直关系；</li></ul><p>实际上以上三个应用方向中前两个是很好想到的，也在很多方面得到了应用，如异常检测，数据压缩和降维以及去噪等。第三个实际上在数据空间关系上应用的比较少但是是很重要的应用，通过垂直关系可以简单的找到空间中与平面垂直的方向。进而可以找到数据所在的拟合平面，在数据的分割等应用中具有重要的意义。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;最近写了篇关于PCA变换应用的文章，主要利用了PCA变换能够将信息集中的特点，通过PCA变换，信息集中在前几个主成分上，通过信息量的差异可以进行分类等操作。也读了一些PCA关于PCA变换应用于其他方面的文章，因此对PCA变换进
      
    
    </summary>
    
      <category term="数学" scheme="http://www.wuweiblog.com/categories/%E6%95%B0%E5%AD%A6/"/>
    
    
      <category term="数学" scheme="http://www.wuweiblog.com/tags/%E6%95%B0%E5%AD%A6/"/>
    
  </entry>
  
  <entry>
    <title>有趣的人</title>
    <link href="http://www.wuweiblog.com/2018/10/05/%E6%9C%89%E8%B6%A3%E7%9A%84%E4%BA%BA/"/>
    <id>http://www.wuweiblog.com/2018/10/05/有趣的人/</id>
    <published>2018-10-05T11:02:23.000Z</published>
    <updated>2018-10-05T12:34:17.644Z</updated>
    
    <content type="html"><![CDATA[<p>&nbsp;&nbsp;&nbsp;&nbsp;有一个肥宅，家里囤积了一冰箱的肥宅快乐水，还有肥宅快乐酒。其实知道肥宅快乐水也好，肥宅快乐酒也好，似乎都不是什么好东西，不过谁在乎呢！肥宅的日子就是这样。<br>&nbsp;&nbsp;&nbsp;&nbsp;曾经有一只又黑又瘦的猪在肥宅家里住过一段时间，看起来像家猪却长出了獠牙。猪刚来家里的时候正是台风过后，肥宅已经饿了两天了，开门看见一只猪排站在门口，自然是喜不自胜一把扯起猪尾巴就把猪拽进来了，看来今天是有一顿大餐了。万万没有想到此时猪发出了杀猪般的嚎叫：“救命呀~杀猪啦~”此处并没有使用拟人的手法，这头猪真的说话了，肥宅吓了一跳连忙把猪放开了。猪得到片刻的喘息，连忙说道：“老弟，有话好好说，我就在你这里接住一段时间，时间一到我就走。”肥宅连忙拒绝：“不行，我家里不养宠物。”这么一说猪就不愿意了“老猪我可不是宠物，你见过宠物说人话的么？”肥宅一想也是挺有道理的，何况本来肥宅一人住也挺无聊，再住进一只猪也并没有什么不好的，所以猪就理所当然的搬了进来。<br>&nbsp;&nbsp;&nbsp;&nbsp;这一天猪与肥宅一起坐在沙发上吹牛逼，猪说：“你信不信，我日过的母猪比你看过的毛片还要多！”，肥宅看着硬盘里8G的种子，摇头表示除非自己是智障否则是坚决不会信的；眼看肥宅没有被自己吹过的牛逼震惊，猪表示不乐意了，拍着肥宅的肩膀说：“伙计，你说你也是一表人才，为啥自甘堕落，宁愿与种子为伍与硬盘为伴？”这番话让肥宅陷入了沉思。<br>&nbsp;&nbsp;&nbsp;&nbsp;不久之前一位自称耶稣的人也在肥宅这里住了一段时间，看到肥宅硬盘中8G的种子，耶稣表示很是同情，另外为了感谢肥宅的感谢耶稣表示要让肥宅成为一个真正的少女杀手，从而摆脱毛片恶魔的魔掌，于是跟肥宅说：“阿宅，为了感谢你的收留，我决定给你一个选择，你是愿意成为一个有趣的人还是一个有钱的人？”肥宅表示很惊讶，真的有这么好的事情么？耶稣表示是的，只是需要你做出选择。听完这番话肥宅想起了自己小时候喜欢的姑娘小花，曾经为了追求小花，肥宅攒了一年的零花钱给小花买了一个小花心仪已久的胸针正兴冲冲的想要送给小花，可是当他把胸针给小花的时候小花却对他说：“小宅，你不需要送我这么贵重的礼物，我是不会喜欢你的，我喜欢的是小强！”肥宅听后心痛不已，质问小花为什么，小花说：“因为小强比你有趣，跟他在一起比跟你一起开心多了！”听完小花的话肥宅颓然的离开了，他本就不是一个有趣的人，从小到大他只是默默的学习，默默的运动，默默的工作，默默的看着日出日落，看着雨点雪花；他不知道什么是有趣，他想也许操场上让小花笑得花枝乱颤的小强是有趣的吧，所以他一直想成为一个有趣的人，成为一个能让小花开心的人。<br>&nbsp;&nbsp;&nbsp;&nbsp;所以他对耶稣说：“我要成为一个有趣的人！”听到他的回答耶稣很惊讶，但是依然点点头然后就离开了，耶稣离开之后肥宅的生活如往常一样，也似乎并没有变得多么有趣。那个骗子一定是不想给我伙食费故意骗我的，肥宅心里想着，不过肥宅也并没有放在心上，直到猪跟他说话他才想起这件事情。所以他问猪：“老猪，你说你是不是一个有趣的人，哦不，有趣的猪？”猪白了他一眼说：“废话，我可是少女猪杀手，可谓阅猪无数了，只要我看上的猪没有不拜倒在我性感的獠牙和鬃毛上的！” “哦，看不出来你这么牛逼呀？那请问一下你是怎样做一只有趣的猪呢？” “这还不简单，跟野外的猪说安定，与圈养的猪说自由，向待宰的猪说来生，十拿九稳！”听话猪的歪理邪说肥宅只当在说笑，并没有放在心上：“喝酒，喝酒！”这晚肥仔喝醉了，醉倒在沙发上，梦里的肥宅又回到了年轻的时候，他帮小花别好胸针，小花被他逗得笑得花枝乱颤，小花笑着跟他说：“小强，你真的很有趣。”肥宅在沙发上翻了个身~</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;有一个肥宅，家里囤积了一冰箱的肥宅快乐水，还有肥宅快乐酒。其实知道肥宅快乐水也好，肥宅快乐酒也好，似乎都不是什么好东西，不过谁在乎呢！肥宅的日子就是这样。&lt;br&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;曾经有一只又黑又
      
    
    </summary>
    
      <category term="随感" scheme="http://www.wuweiblog.com/categories/%E9%9A%8F%E6%84%9F/"/>
    
    
      <category term="随感" scheme="http://www.wuweiblog.com/tags/%E9%9A%8F%E6%84%9F/"/>
    
  </entry>
  
  <entry>
    <title>一切随缘</title>
    <link href="http://www.wuweiblog.com/2018/09/08/%E4%B8%80%E5%88%87%E9%9A%8F%E7%BC%98/"/>
    <id>http://www.wuweiblog.com/2018/09/08/一切随缘/</id>
    <published>2018-09-08T13:34:34.000Z</published>
    <updated>2018-09-09T04:12:42.569Z</updated>
    
    <content type="html"><![CDATA[<p>&nbsp;&nbsp;&nbsp;&nbsp;游完泳回来看了两集破产姐妹，依旧是这么的毁三观，本来想着把实验做一下，顺便把论文写完赶快投出去交差，但是总是没有心情做实验，另外书房的灯好像坏了，只能靠台灯艰难度日了，以前的时候我最喜欢这种漆黑的环境，开一盏台灯，好像回到宿舍里通宵赶代码的日子，但是现在好像不太喜欢这样了，主要是开着台灯弹琴可能看不清谱子，但是感觉弹来弹去也没有什么长进，好像还越来越不会了，着我就很尴尬了不是，另外感觉最近总是想着看手机，想找个人聊聊天，其实也不知道聊什么。<br>&nbsp;&nbsp;&nbsp;&nbsp;最近生活习惯好了很多，晚上一般都睡得挺早的，不知道是因为搬家住的远了，每天早上起得比较早还是因为健身之后太累。昨天的时候在车上看到一个知乎话题，关于自律。今年年初算起，一个人独居有半年了，一开始其实觉得挺好，终于又可以享受一个人时间，又可以默默的躲在房间的角落发呆，或者看书；然后时间慢慢过去，有时候晚上回家又会感觉到很孤单，一种很难描述的孤单，或者我可以将其称为空虚，好像缺了点什么，然后不停的刷着各种社交工具，其实我算是一个社交很少的人，拿起手机，发现不管是微信还是QQ，似乎都没有人找，这么一段时间以来好像在社交App上主动联系我的都是邀请去参加婚礼的…WTF。<br>&nbsp;&nbsp;&nbsp;&nbsp;乱七八糟的东西写了一堆，好像心情也没有什么变化，还是不太想写代码，不太想写文章，不想做实验，不想干所有有意义的事情，只想默默的做些浪费生命的事情把时间打发了……还是想着找个人聊聊天，又拿起手机看了一下，发现一如既往没有消息，所以又失望的放下了。其实我自己都不太愿意去联系别人，有何必指望着人家会联系我呢。着大半年来总是偶尔会有这样的时候，自己一个人在家，默默的弹着琴或者什么都不做就是发呆，很想有人能够找我聊聊天，但是却没有，也不太愿意去找别人聊天，一来怕麻烦别人，二来实际上可能也不会有人喜欢尬聊吧，再说自己负面情绪这么多，如果影响了朋友们也不见得是什么好事。有很多人都觉得我这样是缺一个女朋友，实际上并不是，我好像不太需要一个女朋友这么亲密的关系，而且也很难再建立一段亲密关系。因为自从上次分手后似乎对自我边界的认识越发的清晰了，越发的不想别人介入我自己的生活边界，也不太愿意因为其他人改变生活习惯，就这么随缘吧，反正都是佛系青年，随缘也没有什么不好的。<br>&nbsp;&nbsp;&nbsp;&nbsp;上周末跟@甘甜聊了一下她婚后的带娃生活，实际上自从她结婚之后就很少聊天了，当然一方面是我工作也比较忙，另外也是觉得她太累贸然打扰似乎不太好。听着她讲述自己的生活，我刚开始是很惊讶的，因为在我的印象中我们聊天从来都是当下的苟且以及诗和远方，很少有这些生活中家长里短的事情，但是这次听她说了很多关于生活的事情，这让我仿佛看见了小时候爸妈的生活。我觉得很有意思，同时也坚定了我应该一个人生活的信念，实际上真正的soul mate是极少的，可能遇到的概率比中彩票概率还要低，另外就算是soul mate也可能被两个家庭的琐事把热情消磨得干干净净，所以还是就这样吧，一切随缘。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;游完泳回来看了两集破产姐妹，依旧是这么的毁三观，本来想着把实验做一下，顺便把论文写完赶快投出去交差，但是总是没有心情做实验，另外书房的灯好像坏了，只能靠台灯艰难度日了，以前的时候我最喜欢这种漆黑的环境，开一盏台灯，好像回到宿舍
      
    
    </summary>
    
      <category term="随感" scheme="http://www.wuweiblog.com/categories/%E9%9A%8F%E6%84%9F/"/>
    
    
      <category term="随感" scheme="http://www.wuweiblog.com/tags/%E9%9A%8F%E6%84%9F/"/>
    
  </entry>
  
  <entry>
    <title>一座孤岛</title>
    <link href="http://www.wuweiblog.com/2018/08/30/%E4%B8%80%E5%BA%A7%E5%AD%A4%E5%B2%9B/"/>
    <id>http://www.wuweiblog.com/2018/08/30/一座孤岛/</id>
    <published>2018-08-30T14:39:50.000Z</published>
    <updated>2018-08-30T15:29:18.590Z</updated>
    
    <content type="html"><![CDATA[<p>&nbsp;&nbsp;&nbsp;&nbsp;第一次看这部电影应该是本科的时候，晚上本来在研究RNN，突然有点烦，又把这部剧情舒缓的动画翻出来看看。动画讲述一位澳大利亚女孩儿与一位纽约具有自闭症的中年通过书信往来结下深厚友谊的故事，小女孩儿Mary生活的家庭并不幸福，生活中缺少父亲的陪伴和母亲的关爱，另外由于既不聪明也不可爱，所以从小就没有什么朋友。一次偶然的机会，女孩儿突发奇想，想要给一位笔友写信，于是这封信漂洋过海来到纽约，让Max看到了这封信，由此揭开了他们长达二十年的友情。<br>&nbsp;&nbsp;&nbsp;&nbsp;这是一部黏土动画，没有什么激烈的剧情冲突，只是简单的通过一封封的书信介绍各自的生活以及对生活的看法。通过一封封的书信，我们看到了两个孤独的身影，跨过万水千山，相互鼓励相互安慰。我看到有人说是爱情，实际上却不是这么认为，爱情的力量可以跨越万水千山，也可以相互鼓励和安慰，但是爱情更多的陪伴，是双目对视时眼中溢出的爱意，是房间里留下的某个人的身影，是一日不见如隔三秋的思念。而友情却不是这样，与爱相似，但是与爱无关，可以陪伴，也可以潇洒离开。在这里我不想谈关于友情还是爱情，当我重新看完这一部影片后涌现最深刻的感受是孤独。<br>&nbsp;&nbsp;&nbsp;&nbsp;我们生而孤独，如同身处一座孤岛，这便是我们一生的牢笼，不管是身体上还是心理上，爱情是找到一个愿意踏上你的岛来陪你孤独的人，而友情是在另一座岛上理解你孤独的人。实际上这两种感情中的任何一种都是可遇而不可求的，所以我们不断的遇见，然后不断的忘记，最后发现原来还是只有自己。有时候我回家，坐在书房看着我的琴发呆，或者是关上了所有的灯，坐在客厅沙发上发呆。有时候我会想，如果我这一生就这么过去了，一个人默默的在这黑暗中死去，那该是一件多么悲伤的故事。前一些日子看到日本的一个无缘死亡，会感觉到有一些恐慌；一个人没有伴侣，失去所有亲人与朋友之后独自在自己的住所迎接死亡的到来，这该有多么可怕。实际上我们又注定要迎接死亡的到来，前几天的时候以为舅奶奶去世了，我是才知道消息。在我的印象中她的身体一直都很好的，直到我舅姥爷去世，她的身体便是一日不如一日了。也许是思念，也许是孤独，总之是一日不如一日。所以我会想，如果远在天边有一个人，他能够明白我的孤独，能够分担我的忧伤，分享我的快乐，这是一件多么美好的事情。实际上社交网络与通讯工具的发展让我们距离更近了，却更加疏离了。我想你，所以我给你电话，然后我还是想你，我又给你电话，然后你觉得我很烦，我也觉得自己很烦，所以我便不想你了,失去距离让我们的沟通更加高效，但是我们的思念变得廉价，我们的理解我们的同理心变得一文不值，所以我们变得更加孤独了，我们只想找到一个温暖的怀抱来填满我们空空荡荡的房间，而不是一个美好的灵魂来填满我们空空荡荡的心。<br>&nbsp;&nbsp;&nbsp;&nbsp;电影看完了，实际上还是很羡慕剧中Mary和Max，虽然面对生活的泥潭，但是如果我知道远在天边如果有一个人能够理解我，也在盼着我的消息，想想也应该是让生活无论如何都要继续下去的动力吧。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;第一次看这部电影应该是本科的时候，晚上本来在研究RNN，突然有点烦，又把这部剧情舒缓的动画翻出来看看。动画讲述一位澳大利亚女孩儿与一位纽约具有自闭症的中年通过书信往来结下深厚友谊的故事，小女孩儿Mary生活的家庭并不幸福，生活
      
    
    </summary>
    
      <category term="影评" scheme="http://www.wuweiblog.com/categories/%E5%BD%B1%E8%AF%84/"/>
    
    
      <category term="Mary and Max，影评" scheme="http://www.wuweiblog.com/tags/Mary-and-Max%EF%BC%8C%E5%BD%B1%E8%AF%84/"/>
    
  </entry>
  
  <entry>
    <title>不读书(六)</title>
    <link href="http://www.wuweiblog.com/2018/08/29/%E4%B8%8D%E8%AF%BB%E4%B9%A6(%E5%85%AD)/"/>
    <id>http://www.wuweiblog.com/2018/08/29/不读书(六)/</id>
    <published>2018-08-29T00:09:29.000Z</published>
    <updated>2018-08-29T08:36:12.035Z</updated>
    
    <content type="html"><![CDATA[<p>&nbsp;&nbsp;&nbsp;&nbsp;要是在四五年前看这本书，我可能不会喜欢，现在看起来却是看得我直冒冷汗。其实我不太喜欢潜规则这个词，这个词显得太不正义，总有一种鬼鬼祟祟的感觉，但是不得不承认，这个词的创立简直是对几千年来社会处事准则最准确的归纳和概括。为什么叫潜规则，因为这样的规则是不能放在台面上说的，为什么不能放在台面上说，因为其中涉及了太多人性中不是那么光明伟岸的一面，所以我们本能的对其缄口不言，但是行事中却默许其存在。<br>&nbsp;&nbsp;&nbsp;&nbsp;晶姐推荐的这本书在上周终于看完了，看前半部分的时候会震撼很多，后半部分反而习以为常了，本书全面的介绍了潜规则在中国这个集权社会中的体现，分析了潜规则形成的原因并解释了这么多年来一直存在的理由。整本书以古代官场为背景，分析了古代官场中一系列的潜规则现象。实际上几千年来，我国作为一个中央集权制的国家，国家掌握着最大的资源和话语权，而作为国家代表的官员则是国家资源分配权力的实际掌控者。实际上整个社会阶层呈现一个金字塔状，最底层是数量最多的百姓，然后依次向上是逐级官僚，实际上整个官僚体系都是建立在对农名阶级的剥削基础上。但是为了避免过度剥削而引起整个体系的崩塌，于是乎建立了一整套例如仁义道德，忠君爱民，清正廉明等等不过是一套看似华丽的外衣罢了，说到底不过是赤裸裸的利益勾结和利益计算而已。<br>以下是摘抄了书中的几个观点进行分析：</p><blockquote><p>1.一个变质的政府，一个剥削性越来越强、服务性越来越弱的政府，自然也需要变质的官员，需要他们泯灭良心，心狠手辣，否则就要请你走人。这这种背景下，清官和恶棍的混合比率（即清官少，恶棍多）并不是偶然的巧合，而是定向选择的结果。恶政好比是一面筛子，淘汰清官，选择恶棍。<br>2.这就是说，在进行官场谋划，努力摆平各种利害关系的时候，无需考虑老百姓的压力，他们根本就不能构成一个压力集团，甚至连一个舆论集团也不是，不过是一盘散沙。<br>3.第一次接受了圣贤的教育，第二次则是接受胥吏衙役和人间大学的教育。第一次教育教了官员们满口仁义道德，第二次教育教了他们一肚子男盗女娼。<br>4.大家都懂得爱护羊群的重要意义。奈何抵抗不住眼前绵羊的诱惑，也抵抗不住生育狼崽子的诱惑。这也是有道理的：我不吃，别的狼照样吃；我不生，别的狼照样生。个体狼的利益与狼群的集体利益未必一致。如果我的节制不能导致别人的节制，我的自我约束对羊群来说就没有任何意义，徒然减少自己的份额而已。在老狼忍不住饕餮的时候，我可以听到一声叹息：它们要是变成刺猬，俺们不就变成清官了么？<br>5.真实的常规是：对局者双赢，老百姓买单。<br>6.老百姓是个冤大头(大头就是钱的意思。冤大头本意是花了冤枉的钱，引申为上当、不合算等. )。人家骂了他，打了他，吸了他的血，他连找人家的家长哭诉告状都找不起。唯一合算的选择，只剩下一个忍气吞声，继续让人家吸血。</p></blockquote><p>&nbsp;&nbsp;&nbsp;&nbsp;我们选取其中的部分进行详细分析，首先第一个条，劣币驱逐良币理论，官场中正直清廉的官员往往只占极小部分，而且往往郁郁不得志。为什么会出现这样的状况，实际上分析一下做正直官员与做一个非正直官员所承担的风险和收益就可以看出来，做一个正直官员往往面临着得罪同事，得罪上司等一系列风险，维护百姓的利益实际上可能还得不到好评，而一个非正直的官员既能够得到上司的好评又能够得到同事的好评，甚至能够赚得不菲的身价，而他所冒的风险仅仅是收到一些背地里的差评，而这些差评似乎对其官场生涯无关紧要，在风险和收益严重不成比例的情况下，出现劣官驱逐良官的现象也是必然的。另外第五条，对局者双赢，老百姓买单，这个看似不好理解，实际上很好理解，所谓清官与贪官的对局，从官吏组成结构来看只是一个上层制度的调整而已，是一个长期的剥削和短期过度剥削的理念对局，而下层的百姓永远免不了被剥削，因此从官吏阶层来看，对局者是处于双赢的局面的，而百姓总是付出代价的阶级。<br>&nbsp;&nbsp;&nbsp;&nbsp;以上截取了书中的部分观点，实际上我们更关心的是如何解决这个问题，从书中似乎看不到解决问题的具体方略，不过有一点是很明确的，那就是阶层的对立。实际上政府由于其具有较大的话语权，因此虽然说冠冕堂皇的将自己定义为服务者也不能免除其处于管理者的姿态。因此要解决以上问题，首要就是解决管理与服务的定位问题。而这个问题的关键在于执法权的归属问题，实际上所谓的内部监督说到底，立法执法都是统一个阶级，那就难以避免会出现潜规则的现象，因此需要权力的制衡，三权分立能够很好的实现权力的制衡，但是如果处于同一阶级，这样的制衡便失去了意义，重点在于对立的阶层需要掌握制衡的权力。以上是我的一点点看法，个人的意见，不对此负责。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;要是在四五年前看这本书，我可能不会喜欢，现在看起来却是看得我直冒冷汗。其实我不太喜欢潜规则这个词，这个词显得太不正义，总有一种鬼鬼祟祟的感觉，但是不得不承认，这个词的创立简直是对几千年来社会处事准则最准确的归纳和概括。为什么叫
      
    
    </summary>
    
      <category term="书评" scheme="http://www.wuweiblog.com/categories/%E4%B9%A6%E8%AF%84/"/>
    
    
      <category term="潜规则,书评" scheme="http://www.wuweiblog.com/tags/%E6%BD%9C%E8%A7%84%E5%88%99-%E4%B9%A6%E8%AF%84/"/>
    
  </entry>
  
  <entry>
    <title>tensorflow-二十八弹</title>
    <link href="http://www.wuweiblog.com/2018/08/22/tensorflow-%E4%BA%8C%E5%8D%81%E5%85%AB%E5%BC%B9/"/>
    <id>http://www.wuweiblog.com/2018/08/22/tensorflow-二十八弹/</id>
    <published>2018-08-22T10:19:38.000Z</published>
    <updated>2019-01-06T06:02:56.833Z</updated>
    
    <content type="html"><![CDATA[<p>&nbsp;&nbsp;&nbsp;&nbsp;上一次的学习过程中介绍了RNN,然后列举了它的流程以及理解，然后运行代码试了一下，有了一个直观的体会，但是代码的逻辑实际上还是比较奇怪，而且列出的公式也不直观，实际上在27弹中所提到了公式是进行简化了的公式，所以看起来会比较复杂一些，这次在深入理解之后重新梳理了一下，然后把公式串起来。  </p><center><img src="https://blogimage-1251632003.cos.ap-guangzhou.myqcloud.com/rnn.JPG"></center><br> &nbsp;&nbsp;&nbsp;&nbsp;还是上面这张图，这张图其实描述得有点含糊，为了更清晰得说明情况我再加上一张图进行说明：<br> <center><img src="https://timgsa.baidu.com/timg?image&quality=80&size=b9999_10000&sec=1546757744614&di=4926294d374c6a407cf7834d94e55872&imgtype=0&src=http%3A%2F%2Fcrawler-fs.intsig.net%2Fcamfs%2Fdownload%3Ffilename%3D10005_d3ec5886a98e02c3a4077f619ac99734_4.gif"></center>  <p> &nbsp;&nbsp;&nbsp;&nbsp;上图来自网络，<a href="wuweiBlog.com">侵删联系</a>。把两张图结合起来看就比较好了，实际上对于一个序列输入$[x_1,x_2…x_n]$来说再训练得时候其实不必要关心每次得输入对应得输出，我们更关心的是最后的输入和输出结果。所以对于每一步实际上最重要的是中间层，从两个图中我们可以得到中间层的计算方法,为了统一我们公式表示都参考第一张图，其中$s_t$为中间层：<br>$$<br>s_t=f(W<em>s_{t-1}+U</em>x_t+b)(1)<br>$$<br>&nbsp;&nbsp;&nbsp;&nbsp;公式(1)中$s_t$为当前状态的隐含层，$W$为上一个隐含层到当前隐含层的权重，$U$为输入到隐含层的权重。这样我们就得到了从输入到隐含层的计算方式，然后我们从隐含层到输出：<br>$$<br>o_t=g(V*s_t+c)(2)<br>$$<br>&nbsp;&nbsp;&nbsp;&nbsp;在公式(2)中$o_t$为输出结果，$V$为输入到输出的权重，然后我们就得到了整个卷积神经网络的经典公式。对比上次给出的公式：<br><strong>二十七弹中的公式为</strong>：<br>$$<br>\begin{aligned}<br>&amp;S_t=f(W(X<em>t@ S</em>{t-1})+b)(3)\<br>&amp;O_t=g(US_t,+c)(4)<br>\end{aligned}<br>$$<br>&nbsp;&nbsp;&nbsp;&nbsp;从比较中可以看出实际上主要的差别集中在(1)和(3)上，实际上在二十七弹中给出的公式是简化公式，我们可以把式(1)写成如下形式:<br>$$<br>s_t=f([W,U]\cdot[s_{t-1},x_t]’+b))(5)<br>$$<br>&nbsp;&nbsp;&nbsp;&nbsp;对于式(5)其实就是将$W$与$U$合并为一个矩阵再将$s_{t-1},x_t$合并，其中$@$符号就是矩阵合并的符号，由于python矩阵合并计算相对比较简单因此采用简化公式能够减小编码量，但是却增大的理解的负担；实际上序列不可能无限长，因此每次都是截取一定长度的序列进行计算，由此产生了step参数。<br>&nbsp;&nbsp;&nbsp;&nbsp;实际上经典的RNN已经了解了，但是在应用过程中RNN存在很多变化比如以下几种图的变化：<br> <center><img src="https://pic1.zhimg.com/v2-6caa75392fe47801e605d5e8f2d3a100_r.jpg"><br> 图1.多个输入对应一个输出<br> <img src="https://pic3.zhimg.com/80/v2-87ebd6a82e32e81657682ffa0ba084ee_hd.jpg"><br> 图2.一个输入对应多个输出<br>  <img src="https://pic4.zhimg.com/80/v2-77e8a977fc3d43bec8b05633dc52ff9f_hd.jpg"><br>图3.自编码解码器<br> </center><br> &nbsp;&nbsp;&nbsp;&nbsp;以上三种是RNN中除了输入与输出等长之外最常见的三种模式了，实际上前两种都比较简单，对于多个输入对应一个输出的模式，只需要在最后一个输出上对应进行变换就好了。而对于多个输出对应一个输入的只需要对最开始的输入进行变换。整个结构没有变化过程也比较简单，单独把最后或者第一次的输入或者输出抽取出来就好了；对于多个输入对应一个输出的情况一般都是在语义的识别中，比如输入一个语句判断其情感倾向等。一个输入对应多个输出情况就比较多，比如输入第一画自动绘制，输入描述自动生成文字图片等都属于这一类。第三类是最重要的一个部分，我们叫自编码器；实际上就是根据输入得到一个输入的编码对于不同的输入都能够得到统一的编码形式使得输入数据能够统一，另外编码也能够包含输入的所有信息。编码的形式有很多种，可以直接输出最后一个隐藏状态，也可以对最后一个隐藏状态进行变换或者对所有隐藏状态进行变换得到结果。得到这个结果我们就认为是对序列输入的编码结果，而过程中的参数实际上就是编码器。有编码过程就一定有一个解码过程，解码过程就是由c得到各个输出的过程。解码实际上也是一个RNN网络，这个网络的结构就是图2中提到的多个输入对应一个输出的结构。然后训练出一个网络。这样就得到了两个网络，这一组网络我们就称为编码-解码器。编码器的优点在于不对输入和输出的长度进行限制，因此被广泛的应用。<a href="https://zhuanlan.zhihu.com/p/28054589" target="_blank" rel="noopener">参考内容</a><br> &nbsp;&nbsp;&nbsp;&nbsp;通过这次学习解决了上次遗留的一些问题，同时对RNN的结构进行了归纳，同时对编码器进行了理解，实际上通过这个结构对自编码器就能够很好的理解了。通过学习编码形式得到输出和其本身对接近的编码器和解码器从而实现对数据的抽象，对于小样本的学习来说通过自编码的形式能够对数据进行更高层次的抽象，在此基础上通过少量的样本可以达到很好的学习效果。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;上一次的学习过程中介绍了RNN,然后列举了它的流程以及理解，然后运行代码试了一下，有了一个直观的体会，但是代码的逻辑实际上还是比较奇怪，而且列出的公式也不直观，实际上在27弹中所提到了公式是进行简化了的公式，所以看起来会比较复
      
    
    </summary>
    
      <category term="学习" scheme="http://www.wuweiblog.com/categories/%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="tensorflow学习" scheme="http://www.wuweiblog.com/tags/tensorflow%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>tensorflow-二十七弹</title>
    <link href="http://www.wuweiblog.com/2018/08/22/tensorflow-%E4%BA%8C%E5%8D%81%E4%B8%83%E5%BC%B9/"/>
    <id>http://www.wuweiblog.com/2018/08/22/tensorflow-二十七弹/</id>
    <published>2018-08-22T10:19:38.000Z</published>
    <updated>2019-01-06T03:50:53.876Z</updated>
    
    <content type="html"><![CDATA[<p>&nbsp;&nbsp;&nbsp;&nbsp;tensorflow的学习进行到这个阶段，实际上已经处于一个入门阶段了，在前面的学习过程中我们着重介绍了CNN的构造以及实现过程，另外也提及了一些关于爬虫的知识以及一些关于机器学习的数学基础，现在感觉整个CNN的过程已经掌握得差不多了，剩下就是各种CNN网络得实现了，这个实际上就跟拼接积木差不多了，是一些调参得过程，其中如果进行深入的数学分析就太复杂了，所以在这里先放一放，先接触一下其他的类型的深度网络，然后再回来研究网络的构造问题，下面主要进行RNN的学习:</p><blockquote><p>RNN:循环神经网络，Recurrent Neural Network。神经网络是一种节点定向连接成环的人工神经网络。这种网络的内部状态可以展示动态时序行为。不同于前馈神经网络的是，RNN可以利用它内部的记忆来处理任意时序的输入序列，这让它可以更容易处理如不分段的手写识别、语音识别等。——百度百科</p></blockquote><p>&nbsp;&nbsp;&nbsp;&nbsp;实际上RNN更重要的作用应该是对于语义的识别，百度百科的定义我们看看就行了。本次学习以及代码参考一下文章以及博客(如有侵权，联系删除)：<br><a href="https://blog.csdn.net/liuchonge/article/details/70809288" target="_blank" rel="noopener">使用TensorFlow实现RNN模型入门篇1</a><br><a href="http://lib.csdn.net/article/aiframework/66348?knId=1756" target="_blank" rel="noopener">RNN入门详解及TensorFlow源码实现–深度学习笔记</a><br><a href="http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/" target="_blank" rel="noopener">Recurrent Neural Networks Tutorial, Part 1 – Introduction to RNNs</a><br><a href="https://r2rt.com/recurrent-neural-networks-in-tensorflow-i.html" target="_blank" rel="noopener">Recurrent Neural Networks in Tensorflow I</a><br><a href="http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-2-implementing-a-language-model-rnn-with-python-numpy-and-theano/" target="_blank" rel="noopener">Recurrent Neural Networks Tutorial, Part 2 – Implementing a RNN with Python, Numpy and Theano</a></p><h2 id="什么是RNN"><a href="#什么是RNN" class="headerlink" title="什么是RNN"></a>什么是RNN</h2><p>&nbsp;&nbsp;&nbsp;&nbsp;既然要学习RNN，那么我们就得先了解一下到底什么是RNN，实际上RNN被创造得目的在于充分利用序列数据的前后文信息。在传统的神经网络中假设没一次的输入和输入（每一次训练）是独立的，但是实际上在生活中我们面对很多问题的时候都会有一个上下文的关系，比如写文章之类的。我们语句的不同输入顺序可能有完全不同的意思，RNN就是来处理这样的问题的。另外我们从另一个角度来思考RNN，也就是我们通常说的记忆，意思就是能够从以前所有的输入数据中提取信息。理论上来说RNN能从记忆无限长时间的信息，但是在实际应用过程中会限制回溯的步长。<br>这里要祭出那张经典的图了：<br><img src="https://blogimage-1251632003.cos.ap-guangzhou.myqcloud.com/rnn.JPG"><br>&nbsp;&nbsp;&nbsp;&nbsp;这张被引用过无数次的图很形象的说明了RNN的过程，实际上左边是RNN的过程，右边是RNN展开的过程，如果我们关心五个单词的句子，整个网络就可以展开成一个五层的神经网络，每个单词就是一层，整个结构为：</p><blockquote><ul><li>$x_t$ is the input at time step $t$. For example, $x_1$ could be a one-hot vector corresponding to the second word of a sentence.  </li><li>$s_t$ is the hidden state at time step t. It’s the “memory” of the network. s_t is calculated based on the previous hidden state and the input at the current step: $s_t=f(Ux_t + Ws_{t-1})$. The function f usually is a nonlinearity such as tanh or ReLU.  $s_{-1}$, which is required to calculate the first hidden state, is typically initialized to all zeroes.  </li><li>$o_t$ is the output at step t. For example, if we wanted to predict the next word in a sentence it would be a vector of probabilities across our vocabulary. $o_t = \mathrm{softmax}(Vs_t)$.  </li></ul></blockquote><p>&nbsp;&nbsp;&nbsp;&nbsp;上面就是几个参数的说明，实际上比较简单也就没有必要再翻译了，看看就好了，下面就上面这个过程做一个简单的说明：  </p><ol><li>实际上可以认为$s_t$是一个记忆网络，能够记录以前所有的信息，而输出层$o_t$的计算仅依赖于时刻$t$的记忆，但是实际情况会复杂一些，因为$s_t$并不能记忆住前面太多步的信息（实际上也没有必要记住前所有步的信息）  </li><li>RNN实际上展开后每一层都是共享的同一参数，所不同的仅仅是输入值，通过此种方式极大的减小了参数的数目（序列输入，输入网络层数可能极大）</li><li>实际上对于部分应用来说不是所有的中间输出步骤都是有效的，我们仅仅关心最后的输出，同样对于输入我们也不需要关心每次的输入，RNN的主要特征是它的隐藏状态，这些隐藏状态可以获取序列数据的信息。</li></ol><h2 id="RNN的应用"><a href="#RNN的应用" class="headerlink" title="RNN的应用"></a>RNN的应用</h2><p>&nbsp;&nbsp;&nbsp;&nbsp;这里就随便谈谈了，实际上RNN做的最多的还是语义理解以及机器翻译工作，最常用的RNN模型问LSTM。具体的介绍参看<a href="http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/" target="_blank" rel="noopener">Recurrent Neural Networks Tutorial, Part 1 – Introduction to RNNs</a></p><h3 id="语言模型"><a href="#语言模型" class="headerlink" title="语言模型"></a>语言模型</h3><p>&nbsp;&nbsp;&nbsp;&nbsp;假设有一个m个字母的句子，我们建立如下一个语言模型去预测一个句子出现的可能性：<br>$$\begin{aligned}  P(w_1,…,w_m) = \prod_{i=1}^{m} P(w_i \mid w_1,…, w_{i-1})  \end{aligned}$$<br>&nbsp;&nbsp;&nbsp;&nbsp;通俗的来说，一个句子出现的可能性就是每个单词在它之前单词出现后可能性的后验概率的乘积。语言模型的重要之处在于可以通过语言模型形成一个打分机制，在机器翻译等工作中可以被用来选择最佳的翻译方式。语言模型的另外一个作用在于句式生成，如果我们有了足够丰富的句子，则我们可以通过构建好的语言模型生成句式。从上面的模型可以看出每一个单词生成的可能性都取决于其之前的所有单词，实际上很多模型都并不需要或者说从内存和计算时间的角度来说都无法关注到这么长远的记忆，因此我们会限制记忆的长度，并且对不同时长的记忆给不同大小的权重进行约束。</p><h2 id="RNN实现"><a href="#RNN实现" class="headerlink" title="RNN实现"></a>RNN实现</h2><p>&nbsp;&nbsp;&nbsp;&nbsp;好了以上就是RNN的一些介绍，以及其的应用，为了更加快速的入门RNN，我们通过tensorflow构建一个简单的网络对我们生成的简单数据进行训练。</p><h3 id="训练数据集的说明"><a href="#训练数据集的说明" class="headerlink" title="训练数据集的说明"></a>训练数据集的说明</h3><p>&nbsp;&nbsp;&nbsp;&nbsp;输入序列数据X：在第$t$步，$X_t$有百分之五十的可能性为1，另外百分之五十可能性为0，则$X$可能为$[1,0,0,1,1,1…]$<br>输出序列数据Y：对于任意第$t$步，$Y_t$有百分之五十的可能性为1，如果$X_{t-3}$步为1，则$Y_t$为1的可能性增加百分之50，如果$X_{t-8}$步为1，则$Y_t$为1的可能性下降百分之25%，通过这样的模式就确定了输出数据不仅和当前的输入有关，还与前几次的输入情况有关系；这样的一个网络实际上算是比较简单的网络结构了，我们根据以上生成的数据进行模型的构建。</p><h3 id="模型构建"><a href="#模型构建" class="headerlink" title="模型构建"></a>模型构建</h3><p>&nbsp;&nbsp;&nbsp;&nbsp;对于上面描述的这个简单问题，模型构建就很简单了,实际上对于一个模型来说我们首先要考虑的就是他的输入和输出问题，对于RNN模型我们输入是一个0或1的数据$X_t$以及上一个状态矢量$S_{t-1}$,输出$S_t$为可能性分布矢量，$P_t$是输出结果的预测，则有如下公式：<br>$$<br>\begin{aligned}<br>&amp;S_t=tanh(W(X_t\cdot S_{t-1})+b_s)\<br>&amp;P_t=softmax(US_t,+b_p)<br>\end{aligned}<br>$$<br>@表示向量的组合，$X_t$是一个二进制编码向量，$W$，$b_s$，$U$分别为状态矩阵，严格的来说应该证明为什么迭代就能够收敛到正确解，实际上对预测结果求导，然后导数为0分析其收敛特征，但是一般来说神经网络对于我们来说是一个黑盒过程，所以我们不太关心其背后的数学原理，假设能够收敛，则整个模型为:  </p><center><img src="https://blogimage-1251632003.cos.ap-guangzhou.myqcloud.com/RNNsimple.JPG"></center>  <p>&nbsp;&nbsp;&nbsp;&nbsp;上图应该是比较好理解的图，$S_{-1}$为初始状态，可以都为0，然后进行循环计算，实际上训练过程有一个回溯的过程，我们在RNN的数学基础中再去讨论RNN的反向传播过程以及设置记忆长度为多少才合适的问题，现在我们只讲RNN的构造，RNN的构造主要是构造一个RNN_Cell然后复用就好了，主要代码为：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line">x = tf.placeholder(tf.int32, [batch_size, num_steps], name=<span class="string">'input_placeholder'</span>)</span><br><span class="line">y = tf.placeholder(tf.int32, [batch_size, num_steps], name=<span class="string">'labels_placeholder'</span>)</span><br><span class="line"><span class="comment">#RNN的初始化状态，全设为零。注意state是与input保持一致，接下来会有concat操作，所以这里要有batch的维度。即每个样本都要有隐层状态</span></span><br><span class="line">init_state = tf.zeros([batch_size, state_size])</span><br><span class="line"></span><br><span class="line"><span class="comment">#将输入转化为one-hot编码，两个类别。[batch_size, num_steps, num_classes]</span></span><br><span class="line">x_one_hot = tf.one_hot(x, num_classes)</span><br><span class="line"><span class="comment">#将输入unstack，即在num_steps上解绑，方便给每个循环单元输入。这里可以看出RNN每个cell都处理一个batch的输入（即batch个二进制样本输入）</span></span><br><span class="line">rnn_inputs = tf.unstack(x_one_hot, axis=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#定义rnn_cell的权重参数，</span></span><br><span class="line"><span class="keyword">with</span> tf.variable_scope(<span class="string">'rnn_cell'</span>):</span><br><span class="line">    W = tf.get_variable(<span class="string">'W'</span>, [num_classes + state_size, state_size])</span><br><span class="line">    b = tf.get_variable(<span class="string">'b'</span>, [state_size], initializer=tf.constant_initializer(<span class="number">0.0</span>))</span><br><span class="line"><span class="comment">#使之定义为reuse模式，循环使用，保持参数相同</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">rnn_cell</span><span class="params">(rnn_input, state)</span>:</span></span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(<span class="string">'rnn_cell'</span>, reuse=<span class="keyword">True</span>):</span><br><span class="line">        W = tf.get_variable(<span class="string">'W'</span>, [num_classes + state_size, state_size])</span><br><span class="line">        b = tf.get_variable(<span class="string">'b'</span>, [state_size], initializer=tf.constant_initializer(<span class="number">0.0</span>))</span><br><span class="line">    <span class="comment">#定义rnn_cell具体的操作，这里使用的是最简单的rnn，不是LSTM</span></span><br><span class="line">    <span class="keyword">return</span> tf.tanh(tf.matmul(tf.concat([rnn_input, state], <span class="number">1</span>), W) + b)</span><br><span class="line"></span><br><span class="line">state = init_state</span><br><span class="line">rnn_outputs = []</span><br><span class="line"><span class="comment">#循环num_steps次，即将一个序列输入RNN模型</span></span><br><span class="line"><span class="keyword">for</span> rnn_input <span class="keyword">in</span> rnn_inputs:</span><br><span class="line">    state = rnn_cell(rnn_input, state)</span><br><span class="line">    rnn_outputs.append(state)</span><br><span class="line">final_state = rnn_outputs[<span class="number">-1</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment">#定义softmax层</span></span><br><span class="line"><span class="keyword">with</span> tf.variable_scope(<span class="string">'softmax'</span>):</span><br><span class="line">    W = tf.get_variable(<span class="string">'W'</span>, [state_size, num_classes])</span><br><span class="line">    b = tf.get_variable(<span class="string">'b'</span>, [num_classes], initializer=tf.constant_initializer(<span class="number">0.0</span>))</span><br><span class="line"><span class="comment">#注意，这里要将num_steps个输出全部分别进行计算其输出，然后使用softmax预测</span></span><br><span class="line">logits = [tf.matmul(rnn_output, W) + b <span class="keyword">for</span> rnn_output <span class="keyword">in</span> rnn_outputs]</span><br><span class="line">predictions = [tf.nn.softmax(logit) <span class="keyword">for</span> logit <span class="keyword">in</span> logits]</span><br><span class="line"></span><br><span class="line"><span class="comment"># Turn our y placeholder into a list of labels</span></span><br><span class="line">y_as_list = tf.unstack(y, num=num_steps, axis=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#losses and train_step</span></span><br><span class="line">losses = [tf.nn.sparse_softmax_cross_entropy_with_logits(labels=label, logits=logit) <span class="keyword">for</span> \</span><br><span class="line">          logit, label <span class="keyword">in</span> zip(logits, y_as_list)]</span><br><span class="line">total_loss = tf.reduce_mean(losses)</span><br><span class="line">train_step = tf.train.AdagradOptimizer(learning_rate).minimize(total_loss)</span><br></pre></td></tr></table></figure></p><p>&nbsp;&nbsp;&nbsp;&nbsp;从以上代码可以看出，输入有$n$个单元，其中$n$为我们记忆回溯的步长，state_size为隐藏层的状态向量，长度根据需求和输入确定，我们直接看核心部分rnn_cell函数，这个函数定义了RNN的核心运算，首先是W和b的定义，然后是定义运算，整个运算过程为：tf.tanh(tf.matmul(tf.concat([rnn_input, state], 1), W) + b)，实际上就是公式中所提到的，这里有一个concat运算，这个运算时将两个矩阵进行连接，由于最开始的时候已经将编码方式转换为了one-hot编码，one-hot编码实际上意思就是采用你一个0-1的向量来对参数进行编码，组成一个参数矩阵，具体的解释可以参考<a href="https://blog.csdn.net/tengyuan93/article/details/78930285" target="_blank" rel="noopener">OneHot编码知识点</a>，通过这样的编码方式编码成可理解的向量，然后通过unstack解绑，得到每一个batch每一个step的输入，最后通过循环填充数据，然后定义输出层与中间层的W与b，进行误差的估计并采用AdagradOptimizer（随机梯度下降）的方法进行迭代。好了，整个过程就介绍到这里，下面一节就准备对RNN的数学基础进行学习。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;tensorflow的学习进行到这个阶段，实际上已经处于一个入门阶段了，在前面的学习过程中我们着重介绍了CNN的构造以及实现过程，另外也提及了一些关于爬虫的知识以及一些关于机器学习的数学基础，现在感觉整个CNN的过程已经掌握得
      
    
    </summary>
    
      <category term="学习" scheme="http://www.wuweiblog.com/categories/%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="tensorflow学习" scheme="http://www.wuweiblog.com/tags/tensorflow%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>tensorflow-二十六弹</title>
    <link href="http://www.wuweiblog.com/2018/08/13/tensorflow-%E4%BA%8C%E5%8D%81%E5%85%AD%E5%BC%B9/"/>
    <id>http://www.wuweiblog.com/2018/08/13/tensorflow-二十六弹/</id>
    <published>2018-08-13T14:54:33.000Z</published>
    <updated>2018-08-19T02:00:15.826Z</updated>
    
    <content type="html"><![CDATA[<p>&nbsp;&nbsp;&nbsp;&nbsp;写了很多关于tensorflow的部分，但是都比较零散，因为以前不管是对于python还是对于机器学习都了解得不够深刻，因此写出来的东西也就显得比较零散，代码不能够构成一个体系，所以也就谈不上什么积累，不过是多了解了一些关于tensorflow的东西而已，现在不管是对于python还是对于深度学习都有了更加深刻的理解，所以准备重新对整个过程进行组织，代码进行更加有条理的重构，以便于进行进一步的扩展。<br>&nbsp;&nbsp;目前只搭了一个CNN的框架，整个构架描述为：</p><ul><li>1.定义神经网络的基本结构单元：</li></ul><p>&nbsp;&nbsp;&nbsp;&nbsp;实际上神经所有的神经网络都会有一些基本的结构体，如权重，如偏置；因为就目前来说神经网络就是n多的拟合线性运算加上一个响应函数进行拟合，所以基本的结构单元都是相似的，因此我们定义了一个神经网络基类来初始化这些基础的结构<br>  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">  <span class="comment">#基础网络功能，包括：</span></span><br><span class="line"><span class="comment">#1.权重定义</span></span><br><span class="line"><span class="comment">#2.增益的定义</span></span><br><span class="line"><span class="comment">#3.二维卷积运算</span></span><br><span class="line"><span class="comment">#4.最大值池化</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BaseNet</span>:</span></span><br><span class="line">    <span class="comment">#初始化权重</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">weight_variable</span><span class="params">(self,shape)</span>:</span></span><br><span class="line">        <span class="comment">#从截断的正态分布中输出随机值</span></span><br><span class="line">        initial = tf.truncated_normal(shape, stddev=<span class="number">0.1</span>)</span><br><span class="line">        <span class="keyword">return</span> tf.Variable(initial)</span><br><span class="line"></span><br><span class="line">    <span class="comment">#初始化偏置</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">bias_variable</span><span class="params">(self,shape)</span>:</span></span><br><span class="line">        <span class="comment">#设置常数为0.1</span></span><br><span class="line">        initial = tf.constant(<span class="number">0.1</span>, shape=shape)</span><br><span class="line">        <span class="keyword">return</span> tf.Variable(initial)</span><br><span class="line"></span><br><span class="line">    <span class="comment">#二维卷积运算</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">conv2d</span><span class="params">(self,x, W)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> tf.nn.conv2d(x, W, strides=[<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>], padding=<span class="string">"SAME"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">#最大值池化</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">max_pool_2x2</span><span class="params">(self,x)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> tf.nn.max_pool(x, ksize=[<span class="number">1</span>,<span class="number">2</span>,<span class="number">2</span>,<span class="number">1</span>],</span><br><span class="line">                              strides=[<span class="number">1</span>,<span class="number">2</span>,<span class="number">2</span>,<span class="number">1</span>], padding=<span class="string">"SAME"</span>)</span><br></pre></td></tr></table></figure></p><p>&nbsp;&nbsp;&nbsp;&nbsp;从上面的代码中可以看到，我们定义的结构体包括：1.权重变量的定义；2.偏置变量的定义；3.卷积运算；4.池化操作；实际上卷积运算不是所有神经网络通用的操作，仅仅是卷积神经网络需要的操作，但是我们目前就是处理卷积神经网络，为了方便就这么写了。  </p><ul><li>2.特殊网络结构的定义：</li></ul><p>&nbsp;&nbsp;&nbsp;&nbsp;定义好了基本的神经网络结构以后剩下的工作就是针对某一个神经网络进行特殊的定义，目前我们只定义了卷积神经网络，实际上卷神经网络相对比较简单，主要的结构有两个部分，第一个是卷积层，通过卷积层可以进行参数共享从而减小参数个数，另外通过不同的卷积核实际上可以提取不同的特征从而对目标进行识别，另外一个是池化操作，池化操作是神经网络的一个巨大创新，通过池化这个简单的操作对近邻的特征进行概括，并且通过池化操作可以得到待识别目标的旋转不变特征。</p><ul><li><ol start="3"><li>根据数据对网络进行实例化：</li></ol></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#!/usr/bin/env python</span></span><br><span class="line"><span class="comment">#-*- coding:utf-8 -*-</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> model <span class="keyword">import</span> LeNet</span><br><span class="line"><span class="keyword">from</span> tensorflow.examples.tutorials.mnist <span class="keyword">import</span> input_data</span><br><span class="line">mnist = input_data.read_data_sets(<span class="string">"../data/MNIST_data/"</span>,one_hot=<span class="keyword">True</span>)</span><br><span class="line"></span><br><span class="line">ckptfiles = <span class="string">'./mnist_LeNet_ckpt/'</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">mnistLeNet</span><span class="params">(LeNet)</span>:</span></span><br><span class="line">    <span class="comment">#初始化网络</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        LeNet.__init__(self,<span class="number">28</span>,<span class="number">28</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment">#控制变量定义</span></span><br><span class="line">        self.learning_rate = <span class="number">0.001</span></span><br><span class="line">        <span class="comment"># 记录已经训练的次数</span></span><br><span class="line">        self.global_step = tf.Variable(<span class="number">0</span>, trainable=<span class="keyword">False</span>)</span><br><span class="line">        self.x = tf.placeholder(tf.float32, [<span class="keyword">None</span>, <span class="number">784</span>])</span><br><span class="line">        self.label = tf.placeholder(tf.float32, [<span class="keyword">None</span>, <span class="number">10</span>])</span><br><span class="line">        self.x_image = tf.reshape(self.x, [<span class="number">-1</span>,<span class="number">28</span>,<span class="number">28</span>,<span class="number">1</span>])</span><br><span class="line">        </span><br><span class="line">        <span class="comment">#网路层次定义</span></span><br><span class="line">        self.layer1(<span class="number">5</span>,<span class="number">5</span>,<span class="number">32</span>)</span><br><span class="line">        self.layer2(<span class="number">5</span>,<span class="number">5</span>,<span class="number">64</span>)</span><br><span class="line">        self.fullConnLayer(int(<span class="number">1024</span>))</span><br><span class="line">        self.outputLayer(<span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment">#计算参数定义</span></span><br><span class="line">        <span class="comment">#loss</span></span><br><span class="line">        self.loss = -tf.reduce_sum(self.label * tf.log(self.y + <span class="number">1e-10</span>))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># minimize 可传入参数 global_step， 每次训练 global_step的值会增加1</span></span><br><span class="line">        <span class="comment"># 因此，可以通过计算self.global_step这个张量的值，知道当前训练了多少步</span></span><br><span class="line">        self.train = tf.train.AdamOptimizer(self.learning_rate).minimize(</span><br><span class="line">            self.loss, global_step=self.global_step)</span><br><span class="line"></span><br><span class="line">        predict = tf.equal(tf.argmax(self.label, <span class="number">1</span>), tf.argmax(self.y, <span class="number">1</span>))</span><br><span class="line">        self.accuracy = tf.reduce_mean(tf.cast(predict, <span class="string">"float"</span>))</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">TrainMnistLeNet</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        self.net = mnistLeNet()</span><br><span class="line">        self.sess = tf.Session()</span><br><span class="line">        self.sess.run(tf.global_variables_initializer())</span><br><span class="line">        self.data = mnist</span><br><span class="line"></span><br><span class="line">    <span class="comment">#模型训练过程</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">trainMnist</span><span class="params">(self)</span>:</span></span><br><span class="line">        batch_size = <span class="number">50</span></span><br><span class="line">        train_step = <span class="number">3000</span></span><br><span class="line">        <span class="comment"># 记录训练次数, 初始化为0</span></span><br><span class="line">        step = <span class="number">0</span></span><br><span class="line">        save_interval = <span class="number">1000</span></span><br><span class="line"></span><br><span class="line">        batch_size = <span class="number">50</span></span><br><span class="line">        train_step = <span class="number">3000</span></span><br><span class="line">        <span class="comment"># 记录训练次数, 初始化为0</span></span><br><span class="line">        step = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 每隔1000步保存模型</span></span><br><span class="line">        <span class="comment">#save_interval = 1000</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># tf.train.Saver是用来保存训练结果的。</span></span><br><span class="line">        <span class="comment"># max_to_keep 用来设置最多保存多少个模型，默认是5</span></span><br><span class="line">        <span class="comment"># 如果保存的模型超过这个值，最旧的模型将被删除</span></span><br><span class="line">        saver = tf.train.Saver(max_to_keep=<span class="number">10</span>)</span><br><span class="line">        ckpt  = tf.train.get_checkpoint_state(ckptfiles)</span><br><span class="line">        merged = tf.summary.merge_all()</span><br><span class="line">        writer = tf.summary.FileWriter(ckptfiles+<span class="string">'graph'</span>,self.sess.graph)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> ckpt <span class="keyword">and</span> ckpt.model_checkpoint_path:</span><br><span class="line">            saver.restore(self.sess, ckpt.model_checkpoint_path)</span><br><span class="line">            <span class="comment"># 读取网络中的global_step的值，即当前已经训练的次数</span></span><br><span class="line">            step = self.sess.run(self.global_step)</span><br><span class="line">            print(<span class="string">'Continue from'</span>)</span><br><span class="line">            print(<span class="string">'        -&gt; Minibatch update : '</span>, step)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">while</span> step &lt; train_step:</span><br><span class="line">            x, label = self.data.train.next_batch(batch_size)</span><br><span class="line">            _, loss = self.sess.run([self.net.train, self.net.loss],</span><br><span class="line">                                    feed_dict=&#123;self.net.x: x, self.net.label: label&#125;)</span><br><span class="line">            step = self.sess.run(self.net.global_step)</span><br><span class="line">            rs=self.sess.run(merged)</span><br><span class="line">            writer.add_summary(rs, step)</span><br><span class="line">            <span class="keyword">if</span> step % <span class="number">100</span> == <span class="number">0</span>:</span><br><span class="line">                print(<span class="string">'第%5d步，当前loss：%.2f'</span> % (step, loss))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 模型保存在ckpt文件夹下</span></span><br><span class="line">        <span class="comment"># 模型文件名最后会增加global_step的值，比如1000的模型文件名为 model-1000</span></span><br><span class="line">        <span class="comment">#if step % save_interval == 0:</span></span><br><span class="line">        <span class="comment">#只保存一次模型</span></span><br><span class="line">        saver.save(self.sess, ckptfiles+<span class="string">'model'</span>, global_step=step)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">calculate_accuracy</span><span class="params">(self)</span>:</span></span><br><span class="line">        test_x = self.data.test.images</span><br><span class="line">        test_label = self.data.test.labels</span><br><span class="line">        accuracy = self.sess.run(self.net.accuracy,</span><br><span class="line">                                 feed_dict=&#123;self.net.x: test_x, self.net.label: test_label&#125;)</span><br><span class="line">        print(<span class="string">"准确率: %.2f，共测试了%d张图片 "</span> % (accuracy, len(test_label)))</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line">    app = TrainMnistLeNet()</span><br><span class="line">    app.trainMnist()</span><br><span class="line">    app.calculate_accuracy()</span><br></pre></td></tr></table></figure><p>&nbsp;&nbsp;&nbsp;&nbsp;这个步骤其实比较简单，就是根据数据对整个网络的输入和输出进行实例化操作，在我的代码中是训练的Mnist数据，所以定义输入的变量的28*28的数据，结果为10个数字，然后获取数据进行训练，由于Mnist数据的解析和处理都在tensorflow的example中被处理了，在处理过程中就省略了这个过程，实际上应该定义一个类专门对数据进行处理和优化。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;写了很多关于tensorflow的部分，但是都比较零散，因为以前不管是对于python还是对于机器学习都了解得不够深刻，因此写出来的东西也就显得比较零散，代码不能够构成一个体系，所以也就谈不上什么积累，不过是多了解了一些关于t
      
    
    </summary>
    
      <category term="学习" scheme="http://www.wuweiblog.com/categories/%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="tensorflow学习" scheme="http://www.wuweiblog.com/tags/tensorflow%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>RPC（共线条件方程）校正迭代方法分析</title>
    <link href="http://www.wuweiblog.com/2018/07/22/RPC%EF%BC%88%E5%85%B1%E7%BA%BF%E6%9D%A1%E4%BB%B6%E6%96%B9%E7%A8%8B%EF%BC%89%E6%A0%A1%E6%AD%A3%E8%BF%AD%E4%BB%A3%E6%96%B9%E6%B3%95%E5%88%86%E6%9E%90/"/>
    <id>http://www.wuweiblog.com/2018/07/22/RPC（共线条件方程）校正迭代方法分析/</id>
    <published>2018-07-22T13:47:52.000Z</published>
    <updated>2018-07-22T14:20:18.068Z</updated>
    
    <content type="html"><![CDATA[<p>这次主要分析GDAL中RPC校正的实现，以及在GDAL中PRPC校正的迭代方法，另外关于迭代计算还是有一些关于迭代的疑惑：<br><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">static</span> <span class="keyword">bool</span></span><br><span class="line">RPCInverseTransformPoint( GDALRPCTransformInfo *psTransform,</span><br><span class="line">                          <span class="keyword">double</span> dfPixel, <span class="keyword">double</span> dfLine, <span class="keyword">double</span> dfUserHeight,</span><br><span class="line">                          <span class="keyword">double</span> *pdfLong, <span class="keyword">double</span> *pdfLat )</span><br><span class="line"></span><br><span class="line">&#123;</span><br><span class="line">    <span class="comment">// Memo:</span></span><br><span class="line">    <span class="comment">// Known to work with 40 iterations with DEM on all points (int coord and</span></span><br><span class="line">    <span class="comment">// +0.5,+0.5 shift) of flock1.20160216_041050_0905.tif, especially on (0,0).</span></span><br><span class="line"></span><br><span class="line"><span class="comment">/* -------------------------------------------------------------------- */</span></span><br><span class="line"><span class="comment">/*      Compute an initial approximation based on linear                */</span></span><br><span class="line"><span class="comment">/*      interpolation from our reference point.                         */</span></span><br><span class="line"><span class="comment">/* -------------------------------------------------------------------- */</span></span><br><span class="line">    <span class="keyword">double</span> dfResultX =</span><br><span class="line">        psTransform-&gt;adfPLToLatLongGeoTransform[<span class="number">0</span>] +</span><br><span class="line">        psTransform-&gt;adfPLToLatLongGeoTransform[<span class="number">1</span>] * dfPixel +</span><br><span class="line">        psTransform-&gt;adfPLToLatLongGeoTransform[<span class="number">2</span>] * dfLine;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">double</span> dfResultY =</span><br><span class="line">        psTransform-&gt;adfPLToLatLongGeoTransform[<span class="number">3</span>] +</span><br><span class="line">        psTransform-&gt;adfPLToLatLongGeoTransform[<span class="number">4</span>] * dfPixel +</span><br><span class="line">        psTransform-&gt;adfPLToLatLongGeoTransform[<span class="number">5</span>] * dfLine;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span>( psTransform-&gt;bRPCInverseVerbose )</span><br><span class="line">    &#123;</span><br><span class="line">        CPLDebug(<span class="string">"RPC"</span>, <span class="string">"Computing inverse transform for (pixel,line)=(%f,%f)"</span>,</span><br><span class="line">                 dfPixel, dfLine);</span><br><span class="line">    &#125;</span><br><span class="line">    VSILFILE* fpLog = <span class="literal">nullptr</span>;</span><br><span class="line">    <span class="keyword">if</span>( psTransform-&gt;pszRPCInverseLog )</span><br><span class="line">    &#123;</span><br><span class="line">        fpLog =</span><br><span class="line">            VSIFOpenL( CPLResetExtension(psTransform-&gt;pszRPCInverseLog, <span class="string">"csvt"</span>),</span><br><span class="line">                       <span class="string">"wb"</span> );</span><br><span class="line">        <span class="keyword">if</span>( fpLog != <span class="literal">nullptr</span> )</span><br><span class="line">        &#123;</span><br><span class="line">            VSIFPrintfL( fpLog, <span class="string">"Integer,Real,Real,Real,String,Real,Real\n"</span> );</span><br><span class="line">            VSIFCloseL( fpLog );</span><br><span class="line">        &#125;</span><br><span class="line">        fpLog = VSIFOpenL( psTransform-&gt;pszRPCInverseLog, <span class="string">"wb"</span> );</span><br><span class="line">        <span class="keyword">if</span>( fpLog != <span class="literal">nullptr</span> )</span><br><span class="line">            VSIFPrintfL(</span><br><span class="line">                fpLog,</span><br><span class="line">                <span class="string">"iter,long,lat,height,WKT,error_pixel_x,error_pixel_y\n"</span> );</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/* -------------------------------------------------------------------- */</span></span><br><span class="line"><span class="comment">/*      Now iterate, trying to find a closer LL location that will      */</span></span><br><span class="line"><span class="comment">/*      back transform to the indicated pixel and line.                 */</span></span><br><span class="line"><span class="comment">/* -------------------------------------------------------------------- */</span></span><br><span class="line">    <span class="keyword">double</span> dfPixelDeltaX = <span class="number">0.0</span>;</span><br><span class="line">    <span class="keyword">double</span> dfPixelDeltaY = <span class="number">0.0</span>;</span><br><span class="line">    <span class="keyword">double</span> dfLastResultX = <span class="number">0.0</span>;</span><br><span class="line">    <span class="keyword">double</span> dfLastResultY = <span class="number">0.0</span>;</span><br><span class="line">    <span class="keyword">double</span> dfLastPixelDeltaX = <span class="number">0.0</span>;</span><br><span class="line">    <span class="keyword">double</span> dfLastPixelDeltaY = <span class="number">0.0</span>;</span><br><span class="line">    <span class="keyword">double</span> dfDEMH = <span class="number">0.0</span>;</span><br><span class="line">    <span class="keyword">bool</span> bLastPixelDeltaValid = <span class="literal">false</span>;</span><br><span class="line">    <span class="keyword">const</span> <span class="keyword">int</span> nMaxIterations =</span><br><span class="line">        (psTransform-&gt;nMaxIterations &gt; <span class="number">0</span>) ? psTransform-&gt;nMaxIterations :</span><br><span class="line">        (psTransform-&gt;poDS != <span class="literal">nullptr</span>) ? <span class="number">20</span> : <span class="number">10</span>;</span><br><span class="line">    <span class="keyword">int</span> nCountConsecutiveErrorBelow2 = <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">int</span> iIter = <span class="number">0</span>;  <span class="comment">// Used after for.</span></span><br><span class="line">    <span class="keyword">for</span>( ; iIter &lt; nMaxIterations; iIter++ )</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">double</span> dfBackPixel = <span class="number">0.0</span>;</span><br><span class="line">        <span class="keyword">double</span> dfBackLine = <span class="number">0.0</span>;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// Update DEMH.</span></span><br><span class="line">        dfDEMH = <span class="number">0.0</span>;</span><br><span class="line">        <span class="keyword">double</span> dfDEMPixel = <span class="number">0.0</span>;</span><br><span class="line">        <span class="keyword">double</span> dfDEMLine = <span class="number">0.0</span>;</span><br><span class="line">        <span class="keyword">if</span>( !GDALRPCGetHeightAtLongLat(psTransform, dfResultX, dfResultY,</span><br><span class="line">                                       &amp;dfDEMH, &amp;dfDEMPixel, &amp;dfDEMLine) )</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="keyword">if</span>( psTransform-&gt;poDS )</span><br><span class="line">            &#123;</span><br><span class="line">                CPLDebug(</span><br><span class="line">                    <span class="string">"RPC"</span>, <span class="string">"DEM (pixel, line) = (%g, %g)"</span>,</span><br><span class="line">                    dfDEMPixel, dfDEMLine);</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            <span class="comment">// The first time, the guess might be completely out of the</span></span><br><span class="line">            <span class="comment">// validity of the DEM, so pickup the "reference Z" as the</span></span><br><span class="line">            <span class="comment">// first guess or the closest point of the DEM by snapping to it.</span></span><br><span class="line">            <span class="keyword">if</span>( iIter == <span class="number">0</span> )</span><br><span class="line">            &#123;</span><br><span class="line">                <span class="keyword">bool</span> bUseRefZ = <span class="literal">true</span>;</span><br><span class="line">                <span class="keyword">if</span>( psTransform-&gt;poDS )</span><br><span class="line">                &#123;</span><br><span class="line">                    <span class="keyword">if</span>( dfDEMPixel &gt;= psTransform-&gt;poDS-&gt;GetRasterXSize() )</span><br><span class="line">                        dfDEMPixel = psTransform-&gt;poDS-&gt;GetRasterXSize() - <span class="number">0.5</span>;</span><br><span class="line">                    <span class="keyword">else</span> <span class="keyword">if</span>( dfDEMPixel &lt; <span class="number">0</span> )</span><br><span class="line">                        dfDEMPixel = <span class="number">0.5</span>;</span><br><span class="line">                    <span class="keyword">if</span>( dfDEMLine &gt;= psTransform-&gt;poDS-&gt;GetRasterYSize() )</span><br><span class="line">                        dfDEMLine = psTransform-&gt;poDS-&gt;GetRasterYSize() - <span class="number">0.5</span>;</span><br><span class="line">                    <span class="keyword">else</span> <span class="keyword">if</span>( dfDEMPixel &lt; <span class="number">0</span> )</span><br><span class="line">                        dfDEMPixel = <span class="number">0.5</span>;</span><br><span class="line">                    <span class="keyword">if</span>( GDALRPCGetDEMHeight( psTransform, dfDEMPixel,</span><br><span class="line">                                             dfDEMLine, &amp;dfDEMH) )</span><br><span class="line">                    &#123;</span><br><span class="line">                        bUseRefZ = <span class="literal">false</span>;</span><br><span class="line">                        CPLDebug(</span><br><span class="line">                            <span class="string">"RPC"</span>, <span class="string">"Iteration %d for (pixel, line) = (%g, %g): "</span></span><br><span class="line">                            <span class="string">"No elevation value at %.15g %.15g. "</span></span><br><span class="line">                            <span class="string">"Using elevation %g at DEM (pixel, line) = "</span></span><br><span class="line">                            <span class="string">"(%g, %g) (snapping to boundaries) instead"</span>,</span><br><span class="line">                            iIter, dfPixel, dfLine,</span><br><span class="line">                            dfResultX, dfResultY,</span><br><span class="line">                            dfDEMH, dfDEMPixel, dfDEMLine );</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line">                <span class="keyword">if</span>( bUseRefZ )</span><br><span class="line">                &#123;</span><br><span class="line">                    dfDEMH = psTransform-&gt;dfRefZ;</span><br><span class="line">                    CPLDebug(</span><br><span class="line">                        <span class="string">"RPC"</span>, <span class="string">"Iteration %d for (pixel, line) = (%g, %g): "</span></span><br><span class="line">                        <span class="string">"No elevation value at %.15g %.15g. "</span></span><br><span class="line">                        <span class="string">"Using elevation %g of reference point instead"</span>,</span><br><span class="line">                        iIter, dfPixel, dfLine,</span><br><span class="line">                        dfResultX, dfResultY,</span><br><span class="line">                        dfDEMH);</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">else</span></span><br><span class="line">            &#123;</span><br><span class="line">                CPLDebug(<span class="string">"RPC"</span>, <span class="string">"Iteration %d for (pixel, line) = (%g, %g): "</span></span><br><span class="line">                          <span class="string">"No elevation value at %.15g %.15g. Erroring out"</span>,</span><br><span class="line">                          iIter, dfPixel, dfLine, dfResultX, dfResultY);</span><br><span class="line">                <span class="keyword">if</span>( fpLog )</span><br><span class="line">                    VSIFCloseL(fpLog);</span><br><span class="line">                <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        RPCTransformPoint( psTransform, dfResultX, dfResultY,</span><br><span class="line">                           dfUserHeight + dfDEMH,</span><br><span class="line">                           &amp;dfBackPixel, &amp;dfBackLine );</span><br><span class="line"></span><br><span class="line">        dfPixelDeltaX = dfBackPixel - dfPixel;</span><br><span class="line">        dfPixelDeltaY = dfBackLine - dfLine;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span>( psTransform-&gt;bRPCInverseVerbose )</span><br><span class="line">        &#123;</span><br><span class="line">            CPLDebug(</span><br><span class="line">                <span class="string">"RPC"</span>, <span class="string">"Iter %d: dfPixelDeltaX=%.02f, dfPixelDeltaY=%.02f, "</span></span><br><span class="line">                <span class="string">"long=%f, lat=%f, height=%f"</span>,</span><br><span class="line">                iIter, dfPixelDeltaX, dfPixelDeltaY,</span><br><span class="line">                dfResultX, dfResultY, dfUserHeight + dfDEMH);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">if</span>( fpLog != <span class="literal">nullptr</span> )</span><br><span class="line">        &#123;</span><br><span class="line">            VSIFPrintfL(</span><br><span class="line">                fpLog, <span class="string">"%d,%.12f,%.12f,%f,\"POINT(%.12f %.12f)\",%f,%f\n"</span>,</span><br><span class="line">                iIter, dfResultX, dfResultY, dfUserHeight + dfDEMH,</span><br><span class="line">                dfResultX, dfResultY, dfPixelDeltaX, dfPixelDeltaY);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">const</span> <span class="keyword">double</span> dfError =</span><br><span class="line">            <span class="built_in">std</span>::max(<span class="built_in">std</span>::<span class="built_in">abs</span>(dfPixelDeltaX), <span class="built_in">std</span>::<span class="built_in">abs</span>(dfPixelDeltaY));</span><br><span class="line">        <span class="keyword">if</span>( dfError &lt; psTransform-&gt;dfPixErrThreshold )</span><br><span class="line">        &#123;</span><br><span class="line">            iIter = <span class="number">-1</span>;</span><br><span class="line">            <span class="keyword">if</span>( psTransform-&gt;bRPCInverseVerbose )</span><br><span class="line">            &#123;</span><br><span class="line">                CPLDebug( <span class="string">"RPC"</span>, <span class="string">"Converged!"</span> );</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">break</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">else</span> <span class="keyword">if</span>( psTransform-&gt;poDS != <span class="literal">nullptr</span> &amp;&amp;</span><br><span class="line">                 bLastPixelDeltaValid &amp;&amp;</span><br><span class="line">                 dfPixelDeltaX * dfLastPixelDeltaX &lt; <span class="number">0</span> &amp;&amp;</span><br><span class="line">                 dfPixelDeltaY * dfLastPixelDeltaY &lt; <span class="number">0</span> )</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="comment">// When there is a DEM, if the error changes sign, we might</span></span><br><span class="line">            <span class="comment">// oscillate forever, so take a mean position as a new guess.</span></span><br><span class="line">            <span class="keyword">if</span>( psTransform-&gt;bRPCInverseVerbose )</span><br><span class="line">            &#123;</span><br><span class="line">                CPLDebug(</span><br><span class="line">                    <span class="string">"RPC"</span>, <span class="string">"Oscillation detected. "</span></span><br><span class="line">                    <span class="string">"Taking mean of 2 previous results as new guess"</span> );</span><br><span class="line">            &#125;</span><br><span class="line">            dfResultX =</span><br><span class="line">                ( <span class="built_in">fabs</span>(dfPixelDeltaX) * dfLastResultX +</span><br><span class="line">                  <span class="built_in">fabs</span>(dfLastPixelDeltaX) * dfResultX ) /</span><br><span class="line">                (<span class="built_in">fabs</span>(dfPixelDeltaX) + <span class="built_in">fabs</span>(dfLastPixelDeltaX));</span><br><span class="line">            dfResultY =</span><br><span class="line">                ( <span class="built_in">fabs</span>(dfPixelDeltaY) * dfLastResultY +</span><br><span class="line">                  <span class="built_in">fabs</span>(dfLastPixelDeltaY) * dfResultY ) /</span><br><span class="line">                (<span class="built_in">fabs</span>(dfPixelDeltaY) + <span class="built_in">fabs</span>(dfLastPixelDeltaY));</span><br><span class="line">            bLastPixelDeltaValid = <span class="literal">false</span>;</span><br><span class="line">            nCountConsecutiveErrorBelow2 = <span class="number">0</span>;</span><br><span class="line">            <span class="keyword">continue</span>;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">double</span> dfBoostFactor = <span class="number">1.0</span>;</span><br><span class="line">        <span class="keyword">if</span>( psTransform-&gt;poDS != <span class="literal">nullptr</span> &amp;&amp;</span><br><span class="line">            nCountConsecutiveErrorBelow2 &gt;= <span class="number">5</span> &amp;&amp; dfError &lt; <span class="number">2</span> )</span><br><span class="line">        &#123;</span><br><span class="line">          <span class="comment">// When there is a DEM, if we remain below a given threshold (somewhat</span></span><br><span class="line">          <span class="comment">// arbitrarily set to 2 pixels) for some time, apply a "boost factor"</span></span><br><span class="line">          <span class="comment">// for the new guessed result, in the hope we will go out of the</span></span><br><span class="line">          <span class="comment">// somewhat current stuck situation.</span></span><br><span class="line">          dfBoostFactor = <span class="number">10</span>;</span><br><span class="line">          <span class="keyword">if</span>( psTransform-&gt;bRPCInverseVerbose )</span><br><span class="line">          &#123;</span><br><span class="line">              CPLDebug(<span class="string">"RPC"</span>, <span class="string">"Applying boost factor 10"</span>);</span><br><span class="line">          &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span>( dfError &lt; <span class="number">2</span> )</span><br><span class="line">            nCountConsecutiveErrorBelow2++;</span><br><span class="line">        <span class="keyword">else</span></span><br><span class="line">            nCountConsecutiveErrorBelow2 = <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">const</span> <span class="keyword">double</span> dfNewResultX = dfResultX</span><br><span class="line">            - ( dfPixelDeltaX * psTransform-&gt;adfPLToLatLongGeoTransform[<span class="number">1</span>] *</span><br><span class="line">                dfBoostFactor )</span><br><span class="line">            - ( dfPixelDeltaY * psTransform-&gt;adfPLToLatLongGeoTransform[<span class="number">2</span>] *</span><br><span class="line">                dfBoostFactor );</span><br><span class="line">        <span class="keyword">const</span> <span class="keyword">double</span> dfNewResultY = dfResultY</span><br><span class="line">            - ( dfPixelDeltaX * psTransform-&gt;adfPLToLatLongGeoTransform[<span class="number">4</span>] *</span><br><span class="line">                dfBoostFactor )</span><br><span class="line">            - ( dfPixelDeltaY * psTransform-&gt;adfPLToLatLongGeoTransform[<span class="number">5</span>] *</span><br><span class="line">                dfBoostFactor );</span><br><span class="line"></span><br><span class="line">        dfLastResultX = dfResultX;</span><br><span class="line">        dfLastResultY = dfResultY;</span><br><span class="line">        dfResultX = dfNewResultX;</span><br><span class="line">        dfResultY = dfNewResultY;</span><br><span class="line">        dfLastPixelDeltaX = dfPixelDeltaX;</span><br><span class="line">        dfLastPixelDeltaY = dfPixelDeltaY;</span><br><span class="line">        bLastPixelDeltaValid = <span class="literal">true</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span>( fpLog != <span class="literal">nullptr</span> )</span><br><span class="line">        VSIFCloseL( fpLog );</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span>( iIter != <span class="number">-1</span> )</span><br><span class="line">    &#123;</span><br><span class="line">        CPLDebug( <span class="string">"RPC"</span>, <span class="string">"Failed Iterations %d: Got: %.16g,%.16g  Offset=%g,%g"</span>,</span><br><span class="line">                  iIter,</span><br><span class="line">                  dfResultX, dfResultY,</span><br><span class="line">                  dfPixelDeltaX, dfPixelDeltaY );</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    *pdfLong = dfResultX;</span><br><span class="line">    *pdfLat = dfResultY;</span><br><span class="line">    <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>上述代码中有很多判断的部分，我们忽略掉这些部分直接看干货，关于PRC校正的方法在以前的文章中已经做过比较详细的分析了，在这里我们不对校正方法进行分析，直接看迭代过程：<br>首先定义了一系列的变量，包括什么迭代变量，误差限变量等，然后根据变换参数计算像素的初始坐标，计算方法为直接根据参考点进行线性插值得到的，实际上这里应该是存疑的，如果没有参考点应该怎么办，我估计这个转换参数在RPC参数中应该是保存的，所以在这里也不详细分析了，代码部分也没有具体实现，我们直接理解就是一个初始化的计算参数。迭代的计算过程为：</p><ul><li>根据坐标以及DEM计算初始位置的高程，然后就是一系列的判断，包括判断DEM的坐标系与给出影像的坐标系，判断是否能够得到高程，以及一系列其他的判断；然后最后的结果就是得到DEM的高程，整个判断结束；</li><li>RPC反算，根据坐标和RPC参数反算出对应的像素点的坐标;</li><li>计算当前给出的上一次给出的像素值和反解出的像素值的误差值；</li><li>判断误差值是否超限了，如果误差在误差限内则说明精度满足要求直接跳出循环了，如果超过误差限则进行迭代；（p.s:在这里GDAL有一个判断我觉得做得很好，在迭代的过程中给了一个猜测参数，如果整个迭代的过程出现oscillate现象，则重新给一个猜测值，这个值的给定估计也有一点经验）</li><li>新的值等于原来的值加上误差项（表现为差值）</li></ul><p>对于以上过程的前几步都是比较好理解的，但是对于这个最后一个新值的获取我有一些疑惑，如果我自己实现的话我会直接用新值带入RPC参数解算新的DEM和坐标并进行迭代，关于迭代的误差在以前的文章中也提到过，初始高程设置的不同会导致迭代次数和误差都特别大，想弄清楚两种迭代方式有什么区别，如果搞明白了我会在下一次的文章中进行详细的描述。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;这次主要分析GDAL中RPC校正的实现，以及在GDAL中PRPC校正的迭代方法，另外关于迭代计算还是有一些关于迭代的疑惑：&lt;br&gt;&lt;figure class=&quot;highlight c++&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span c
      
    
    </summary>
    
      <category term="图像处理" scheme="http://www.wuweiblog.com/categories/%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86/"/>
    
    
      <category term="图像处理数学原理" scheme="http://www.wuweiblog.com/tags/%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86%E6%95%B0%E5%AD%A6%E5%8E%9F%E7%90%86/"/>
    
  </entry>
  
</feed>
