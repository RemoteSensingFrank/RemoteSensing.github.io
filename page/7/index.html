<!DOCTYPE html><html lang="zh-CN"><head><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description" content="随感，遥感，机器学习....想到什么写什么"><title>吴蔚 | 生命不息，折腾不止！</title><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/normalize/5.0.0/normalize.min.css"><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/pure/0.6.2/pure-min.css"><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/pure/0.6.2/grids-responsive-min.css"><link rel="stylesheet" type="text/css" href="/css/style.css?v=0.0.0"><link rel="stylesheet" href="//cdn.bootcss.com/font-awesome/4.7.0/css/font-awesome.min.css"><script type="text/javascript" src="//cdn.bootcss.com/jquery/3.1.1/jquery.min.js"></script><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.ico"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/apple-touch-icon.png"><link rel="alternate" type="application/atom+xml" href="/atom.xml"></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">吴蔚</h1><a id="logo" href="/.">吴蔚</a><p class="description">生命不息，折腾不止！</p></div><div id="nav-menu"><a href="/." class="current"><i class="fa fa-home"> 首页</i></a><a href="/archives/"><i class="fa fa-archive"> 归档</i></a><a href="/about/"><i class="fa fa-user"> 关于</i></a><a href="/atom.xml"><i class="fa fa-rss"> 订阅</i></a></div></div><div id="layout" class="pure-g"><div class="pure-u-1 pure-u-md-3-4"><div class="content_container"><div class="post"><h2 class="post-title"><a href="/2017/09/04/tensorflow-十四弹/">tensorflow-十四弹</a></h2><div class="post-meta">2017-09-04</div><div class="post-content"><p>在上一讲中我们已经结束了传统学习算法的学习，马上要转入深度学习，那么在这里就不得不进行一些铺垫的工作，为什么我们要提深度学习，传统的学习方法究竟在哪些地方不如深度学习，是否有可能这些缺点会被克服以及深度学习的所面对的问题．<br>总的来说，促使机器学习发展的部分原因在于传统学习算法对于高维，复杂函数的泛化性较差，在高维空间中的复杂函数的学习传统的学习方法往往面临着巨大的计算代价，而促进机器学习发展的主要有两点，１．维度灾难；２．流型学习．下面分别就这两个问题进行进一步的描述：</p></div><p class="readmore"><a href="/2017/09/04/tensorflow-十四弹/">阅读更多</a></p></div><div class="post"><h2 class="post-title"><a href="/2017/09/03/tensorflow-十三弹/">tensorflow-十三弹</a></h2><div class="post-meta">2017-09-03</div><div class="post-content"><p>上一讲中提到了一些机器学习的概念,现在我们讲讲机器学习的算法,机器学习的算法总的来说包括两大类,一类为监督学习方法,另一类为无监督学习方法,监督学习就是在具有训练样本的情况下对样本进行学习的过程,而无监督的学习方法针对没有训练样本的情况,下面就这两大类方法分别介绍:  </p>
<h2 id="1-监督学习方法"><a href="#1-监督学习方法" class="headerlink" title="1 监督学习方法"></a>1 监督学习方法</h2><h3 id="1-1逻辑回归方法"><a href="#1-1逻辑回归方法" class="headerlink" title="1.1逻辑回归方法:"></a>1.1逻辑回归方法:</h3><p>对于明确的概率分布　$p(y|x)$ 使用最大似然估计找到对于有参数分布族　$p(y|x;\theta)$　其中 $\theta$ 为参数向量，则线性回归族为：<br>$p(y|x;\theta)＝N(y;\theta^T x,I)$<br>进一步，对于分类问题来说，是求解ｘ属于某一个类别的概率，假设求解ｘ属于类别１的概率，则为：<br>$p(y=1|x;\theta)=\delta(\theta^T x)$<br>其中 $\delta$　为sigmoid函数，使用sigmoid函数的主要原因在于将线性变换的概率映射到０－１之间，实际上为什只求解ｘ属于类别１的概率，因为对于一个二类分类的结果来说，ｘ只有两种情况，在求解得到ｘ属于类别１的概率后ｘ属于类别２的概率也能够获取到了．</p></div><p class="readmore"><a href="/2017/09/03/tensorflow-十三弹/">阅读更多</a></p></div><div class="post"><h2 class="post-title"><a href="/2017/09/03/工作两个月/">工作两个月</a></h2><div class="post-meta">2017-09-03</div><div class="post-content"><p>自从上次跟亮仔他们聚了之后到现在差不多也有三个星期了，现在的工作对于我来说压力也不算大，总是觉得有些跟自己的想法不一样也不知道是为啥，可能还是觉得没有想象中那个样子吧，感觉就是已经做好了拼尽全力的准备然后突然一拳打在了空气上的感觉，难道是因为在这里确实没有什么压力的缘故么？我觉得有点烦，好像一直都在准备着大干一场然后机会总是很少，让我觉得自己好像很没有用的感觉，有时候觉得这样也挺好的，至少没有什么压力，过得还比较轻松吧，看着做我们研发部门的小伙伴们也觉得还好并没有什么压力吧，毕竟不是私企，这边做的好像也不如想象中的那么有挑战性，不说没有新东西学吧，总是感觉做的东西有些，额怎么说呢，有些在自娱自乐的感觉，不够主流。<br>我觉得让自己焦虑的主要原因就在这里了，毕竟如果做的东西并非主流，那么自己的适应面就会比较狭窄，在这样的情况下难免会有一种无所适从的感觉，此外也与自己一直以来在硕士阶段的学习有些关系吧，虽然在实验室也会做项目，可是更多的时候是以一种研究和学些的心态去面对工作的学习，并且在这个过程中我们所接触到的东西都属于比较前沿的东西，所考虑的问题更多的是如何去创新，去做算法，做些在工作中大家都会觉得不是那么接地气的东西，习惯了对每一个算法，每一个软件都刨根问底的弄个明白，而不是就单单知道怎么用，虽然成本很好，可是每次弄明白一个算法，一个功能都让我倍感快乐。可是工作中却不容许我这样了，每一个项目都有着明确的deadline，在工期的压力下没有人愿意冒着这么大的时间成本去纠结那一个个看起来简单实际上却并不简单的小问题，每个人都瞄着大目标去做，而那些需要注意的细小的点自然是能够不去深究就不要深究了，这样对大家都好不是么，可是这让我不开心，让我觉得自己很虚，或者说自己这么多年所学的东西都显得那么苍白，是呀，这么多年所做的摄影测量能比的上smart3D做得好么，如果不能那么有什么意义，费尽心思改进的算法比那些经典的方法好么，如果没有那有什么作用呢，所谓的机器学习，所谓的人工智能现在看起来不过是实验室的一个概念而已，对于一个公司来说人工智能的成本甚至比人力还要高，如果是这样那么为什么还要去发展呢！未来会怎么样，谁都不知道可是我应该怎么办，这个也是一个很值得思考的问题.<br>每天都在学习，在维护一个github仓库，在学习tensorflow在了解每一个前沿的知识，在学习最基本的数学理论希望这些能够让自己在未来受益，在公司师父让我学习项目管理的知识领导给我讲了职业规划，其实我很感激我师父，感激公司领导，他们都很好。可是我还是在焦虑，在为未来焦虑，害怕自己所学的东西会随着时间一天天过去而遗忘，而落后然后我就成为了那些别人口中对知识一无所知的人，这样的感觉很不好，至少在曾经吐槽导师的时候我完全不会有这样的焦虑，可是进入工作之后这样的焦虑就渐渐凸显出来了，不过我觉得这样也至少不是一件坏事吧，对未来的焦虑至少会让我不断的学习不断进步吧，毕竟还年轻，乘着精力和身体都还受得了的情况下好好努力吧，毕竟也和那些在清华，在UCLA的高才生们又站在了同一起跑线，可能我要靠后一点吧，不过我相信这又是一个开始……</p></div><p class="readmore"><a href="/2017/09/03/工作两个月/">阅读更多</a></p></div><div class="post"><h2 class="post-title"><a href="/2017/08/27/tensorflow-十二弹/">tensorflow-十二弹</a></h2><div class="post-meta">2017-08-27</div><div class="post-content"><p>继上一讲中的几个概率的基本概念之后这次我们记录几个提的比较多，但是可能搞得不是那么明白的几个定义:<br>１．<font color="0xff0000">vc维：</font>在介绍vc维之前首先要明白一个概念那就是散度(shatter)，如果对于一个给定的集合$S={x_1,…,x_d}$ 如果一个假设类Ｈ能够实现集合Ｓ中所有元素的任意一种标记方<br>式，则称Ｈ能够分散Ｓ．假设对于两个样本点$A,B$其将其进行二类分类，则划分方式有 ${(A+1,B-1);(A+1,B+1);(A-1,B-1);(A-1,B+1)}$四种情况，则这两个样本的散度为 $2^2$ 实际上对于具有Ｎ个样本的数据集其散度为$2^N$, 对于一个二类分类器，我们可知二维中的一条直线对多可以实现对三个样本的集合打散，如图：<br><img src="http://blogimage-1251632003.cosgz.myqcloud.com/vc3.jpeg">　<br>则上述８种情况包含了所有的组合．二VC维则是衡量这种分类器对于样本打散情况的量，我们说平面上直线分类器的VC维为三，当然对于三个点的情况，如果三个点处于一条直线上也有可能线性分类器无法实现全划分，但是这种情况并不影响．实际上二维平面上的找不到一条直线能够将四个点打散．但是实际上二维平面不止有线性分类器一种分类器，如下图：<br><img src="http://blogimage-1251632003.cosgz.myqcloud.com/vc4.jpeg">　<br>上述矩形分类器能够实现将二维空间中四个点打散，所以二维的矩形分类器的VC维是４从以上分析可以看出VC维就是能分散集合中中的最大样本数目．好了理解了VC维我们来看一个比较特殊的函数集，正弦函数集 ${asin{wx+b}}$ 我们知道正弦函数几乎可以拟合任何一种情况（傅里叶），所以对于包含任意数量的点集都能够将其打散，因此正弦函数集的VC维为无穷大．<br>2.<font color="0xff0000">非参数模型：</font>　介绍非参数模型之前需要介绍一下参数模型，什么是参数模型，就是首先假设模型的分布的模型的已知的，在此情况下通过样本估计模型的参数，这样的模型叫做参数模型，而非参数模型假设样本分布是自由的，因此不需要对模型的参数进行估计而是通过其他的手法进行估计．<br>３.<font color="0xff0000">点估计：</font> 点估计是有样本估计总体分布所含未知参数的真值，称为估计值，点估计的精确程度用置信区间表示，由于样本是从总体中获取，且假设每一个样本都是独立同分布，则可以用样本对总体进行估计．点估计的方法：最大似然法：最大似然法是一种使用得最广泛的点估计方法，最大似然估计就是利用已知样本的结果反推最有可能导致这样结果的参数值，假设有样本 $X={X_1,X_2,…,X_n}$ 其样本的分布密度为$L(X,\theta)$，现在样本是已知的，则将L当作 $\theta$的函数称为似然函数，则将似然函数表述为：$f(X_1,\theta),f(X_2,\theta)…f(X_n,\theta)$,其中$f(X,\theta)$　为总体分布的密度函数或概率函数在已知样本的情况下可以分布参数进行估计；最小二乘方法，最小而乘的参数估计是保证计算的函数尽可能好的拟合观测样本从而得到拟合函数．<br>4.<font color="0xff0000">估计的无偏性:</font>无偏估计意味这样本估计量的数学期望等于其总体分布的真值，一个估计是无偏的则说明其概率分布的期望值等于它所估计的参数，无偏性并不是说我们用任何一个特定样本得到的估计值等于d，甚或很接近0。而是说，如果我们能够从总体中抽取关于Y的无限多个样本，并且每次都计算一个估计值，那么将所有随机样本的这些估计值平均起来，我们便得到总体的均值．<br>5.<font color="0xff0000">核方法:</font>　核方法的主要思想为，在低维空间中线性不可分的点集将其映射到高维空间中则往往变为可分的，而这个将低维空间向高维空间映射的方法我们称为核方法，实际上通过核方法将高维向量中的内积转换为低维的点的核函数计算，由此大大简化了在高维空间的计算方法，实际上核函数的构造具有很多技巧以及对于核函数是否存在也具有一套判断方法，这个设计到比较复杂的过程以后有机会在进行详细说明．<br>今天介绍了概率论中的一些重要的概念，这些概念对于我们理解机器学习的算法至关重要，另外机器学习的基本方法是深度学习的基础，因此我们需要将这些概念理解清楚才能够更好的学习深度学习的理论和方法．<br>参考资料：<br><a href="http://www.cnblogs.com/wuyuegb2312/archive/2012/12/03/2799893.html" target="_blank" rel="noopener">VC维参考资料</a><br><a href="https://www.zhihu.com/question/22855599" target="_blank" rel="noopener">非参数模型资料</a><br><a href="https://baike.baidu.com/item/%E7%82%B9%E4%BC%B0%E8%AE%A1/10842926?fr=aladdin" target="_blank" rel="noopener">点估计</a><br><a href="http://blog.csdn.net/xianlingmao/article/details/7719122" target="_blank" rel="noopener">核方法</a></p></div><p class="readmore"><a href="/2017/08/27/tensorflow-十二弹/">阅读更多</a></p></div><div class="post"><h2 class="post-title"><a href="/2017/08/26/基于Google影像的大数据几何校正思考/">基于Google影像的大数据几何校正思考</a></h2><div class="post-meta">2017-08-26</div><div class="post-content"><p>前一段时间一直在闲暇的时候做着无人机影像处理方面的工作，也算是小有建树了。前两天突然有了一个思考，那就是关于如何更快速的进行校正处理，实际上我们处理的图片，或者说我们处理的影像在很多情况下只需要有一个准确的相对位置关系而并不是需要绝对精度有多么的高，因为理论上影像绝对精度不可能高于影像分辨率，因为控制点的选取本身对于影像来说从影像上选取像控点就存在误差，而亚像元精度的像控点选取通过测量控制点的方式几乎是无法做到的，鉴于此种情况，我认为影像的校正处理还是应该集中在相对位置的校正上，而绝对未知跟具体的影像质量硬件技术有关。实际上在地图应用的过程中能够与底图叠合我们认为就是比较精确的了，那么我们是不是可以利用Google Map提供的瓦片数据自动的选取控制点进行校正。<br>对于以上采用Google Map瓦片数据进行校正我认为主要有两个优势：</p></div><p class="readmore"><a href="/2017/08/26/基于Google影像的大数据几何校正思考/">阅读更多</a></p></div><div class="post"><h2 class="post-title"><a href="/2017/08/24/tensorflow-十一弹/">tensorflow-十一弹</a></h2><div class="post-meta">2017-08-24</div><div class="post-content"><p>到现在已经很久么有进行tensorflow的说明了，不过我们还是用这个主题继续将一下机器学习相关的数学基础，在上一次提到了概率论和统计分析，我们这次详细讲讲概率，提到概率就不得不提随机变量，简单的说随机变量就是在某一个范围内可能取得任意一个值的变量，由于其取值是不确定的，由此而产生了概率，由随机变量的描述可以看出，其实概率是一种描述随机变量状态的一个数学基础，随机变量可能是连续的，也可能是离散的，这个跟所选的随机变量有关。提到了随机变量就必须提出另外一个重要的概念，那就是就是概率分布，概率分布是描述一个或一系列随机变量处理其取值范围内任意一种状态的可能性，这个描述很说明概率实际上用来描述随机变量，而随机变量的取值是随机的，因此对随机变量的描述只能描述为处于某种状态的可能性。由于是描述可能性，因此概率分布有一些特点：</p></div><p class="readmore"><a href="/2017/08/24/tensorflow-十一弹/">阅读更多</a></p></div><div class="post"><h2 class="post-title"><a href="/2017/08/24/程序环境配置/">程序环境配置</a></h2><div class="post-meta">2017-08-24</div><div class="post-content"><p>UAV代码环境配置说明：<br>由于在代码中使用了很多开源的库文件，其中有一些库配置起来还比较麻烦所以我打算做一个环境配置的详细说明，WTK，想想就是一件<br>很麻烦但是又不得不去做的事情，做开源很多时候都是这样，为了一点小小的成就感，当然如果可能也为了服务大家，就需要忍受无穷无尽<br>的麻烦，不过其中的快乐也不足为外人道了，好了，废话不多说讲讲各种库文件的配置：</p></div><p class="readmore"><a href="/2017/08/24/程序环境配置/">阅读更多</a></p></div><div class="post"><h2 class="post-title"><a href="/2017/08/20/峥嵘岁月/">峥嵘岁月</a></h2><div class="post-meta">2017-08-20</div><div class="post-content"><p>周六的时候跟科姐，凯迪还有亮仔聚了聚，话说大家至少也有一年没有见面了，跟亮仔那更是有好几年没有见面了，实际上到目前为止大家认识应该有十年了，以前的时候很难想象大家说十年之后的我们会是什么样子，但是转眼也就十年了，十年之间我们好像没有什么变化，只是从当年的那个山沟沟逃到了广州这个大都市，很少面对繁华的CBD有什么特别的感触，不过这一次真的有一种面对命运的感觉尤其是当我们一行人走在天桥上看着四周高耸的大楼，还有来往的车辆和闪烁的霓虹，我开玩笑似的说了一句我们这几个人在这里有一种中国合伙人的即视感。<br>谁又说不是呢，几个毕业没有多久的年轻人，什么都没有，只是怀着满腔的热情从那个十八线的小城市来到了广州这个大都市，一边是面对这个繁华都市和追求和向往一边是面的这繁华都市的无奈，很多时候我们并没有得选择，或者说不愿意去选择，科姐说他的男朋友一定要坚持留在上海即使每天上下班都要在地铁上挤得跟沙丁鱼罐头一样，所以他们分手了，当时吐槽了很多，我们也只是默默的听着而已，想想谁都不容易，都是狗日的生活。<br>其实还是亮仔说的比较客观，其实我们当属幸运的了，在这样的大城市只是买房压力比较大而已，其实也并不需要为每日的生计发愁。我其实很赞同亮仔的说法，我们还有闲心去感叹生活的艰难，可是有很多人连抬头看看这些高耸建筑的勇气都没有。每一代人都有他们的苦难和难处，我们当然也不例外。工作后跟老同学见面聊天都有意义了很多，不像以前读书时那样只是回忆一些曾经的八卦消息，不过一起聚聚感觉心里瞬间踏实了很多，至少我不是一个人在奋斗，这大概就是我的感觉了，世界是他们的，不过终究是我们的，所有受过的苦难都会变成曾经的峥嵘岁月，每一代人都需要岁月沉淀才会变得厚重然后才会举重若轻吧。</p></div><p class="readmore"><a href="/2017/08/20/峥嵘岁月/">阅读更多</a></p></div><div class="post"><h2 class="post-title"><a href="/2017/08/16/tensorflow-第十弹/">tensorflow-第十弹</a></h2><div class="post-meta">2017-08-16</div><div class="post-content"><p>今天稍微描述一下模型深度的度量方式，第一种为基于评估构架所需执行的顺序指令的数目。即对应的流程图中的最长路径视为模型的深度。然而不同的定义方法和不同的函数可能导致模型流程图的计算方法的不同能图的深度也存在一定的差异。第二种为在深度概率模型中使用的方法，他不是将计算图的深度视为模型深度，而是将描述概念彼此如何关联的图的深度视为模型的深度。<br>实际上通过深度图来判断模型是不是深度学习模型，所以对于模型深度的计算也不一定总是能够达成共识，但是与传统的基于浅层学习的方法相比，基于深度学习的方法模型更加复杂，具有更多的组合特征。<br>实际上机器学习的数学原理主要有两个，一个是线性代数，另一个为概率论，线性代数在这里就不详细介绍，实际上就是一些矩阵，，张量的操作，在以前的学习和分析中也介绍过，下面主要介绍一下概率论在机器学习中的应用。<br>实际上在程序设计中处理的大部分实体都是确定且必然的，对于确定的输入通常会有一个确定的输出，但是对于机器学习来说处理的问题往往是一些直觉性的问题，这类问题的特点在于需要在不确定的情况下进行计算或推理，具体的来说不确定性有三种来源</p></div><p class="readmore"><a href="/2017/08/16/tensorflow-第十弹/">阅读更多</a></p></div><div class="post"><h2 class="post-title"><a href="/2017/08/15/tensorflow-第九弹/">tensorflow-第九弹</a></h2><div class="post-meta">2017-08-15</div><div class="post-content"><p>说过要做读书笔记的，说话要算话，从今天开始一天至少看一节；  </p>
<blockquote>
<p>神经网络的拓扑结构随着时间的变化也在不断的发生着变化，但是我们可以使用一种统一的范式进行描述，我们可以将神经网络简单的描述为神经一个有限元的集合 $N={u_1,u_2…,}$ 被称为节点或神经元和以及有限元的集合 $H\subseteq N \times N$ 被称为边，相互按照一定的方向连接组成的网络。神经网络的表现由一系列变量决定，这些变量我们称之为权重，由于神经网络是一个复杂的网络，整体分析比较麻烦，另外各个层的网络存在相似性，因此我们目前只分析单个有限元的的一次处理，并不涉及到权重的变化过程。对于 $x_t (t=1,…,T)$ 其中每一个 $x_t$ 表示某一层神经元，我们称之为event，实际上对于任意的一个 $x_t (t=1,…,T)$ 它的值的来源方式存在两种情况，直接由环境输入，或者有前一个event的生成，我们考虑由前一个event生成的情况 $event_k-&gt;event_t$ 则 $x_t = f(net_k)$ 而 $net_k$ 为生成网络，而 $f$ 称为响应函数，生成的网络可能有多种情况，如线性网络，或者乘性网络，也可能有更加复杂的多项式网络。响应函数也有多种选择。实际上在神经网络的计算过程中不管是采用哪种网络类型，随着传递的层次的增加，权重参数的数目会以指数的形式增加，由此造成参数过于复杂的情况，在此情况下发展了共享参数的神经网络，权重参数的共享极大的减少了参数的数目，降低的我网络的复杂度。另外对于监督学习的情况，一般来说都是假设判断结果和实际结果的误差最小，由误差最小的原则对神经网络的权重进行调整，由此得到最佳判别的神经网络，另外监督学习的网络认为输入的event与上一次的event是相互独立的。这个假设在序列决策和增强学习中存在着问题，针对此问题发展了增强学习的方法，增强学习的方法假设下一次的evnet能够回推到输入的event，而权重的调整目标在于使得经过计算后的event能够最好的恢复输入数据。  </p></div><p class="readmore"><a href="/2017/08/15/tensorflow-第九弹/">阅读更多</a></p></div><nav class="page-navigator"><a class="extend prev" rel="prev" href="/page/6/">上一页</a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/5/">5</a><a class="page-number" href="/page/6/">6</a><span class="page-number current">7</span><a class="page-number" href="/page/8/">8</a><a class="page-number" href="/page/9/">9</a><span class="space">&hellip;</span><a class="page-number" href="/page/11/">11</a><a class="extend next" rel="next" href="/page/8/">下一页</a></nav></div></div><div class="pure-u-1-4 hidden_mid_and_down"><div id="sidebar"><div class="widget"><form action="//www.google.com/search" method="get" accept-charset="utf-8" target="_blank" class="search-form"><input type="text" name="q" maxlength="20" placeholder="Search"/><input type="hidden" name="sitesearch" value="http://www.wuweiblog.com"/></form></div><div class="widget"><div class="widget-title"><i class="fa fa-folder-o"> 分类</i></div><ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/书评/">书评</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/图像处理/">图像处理</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/学习/">学习</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/小说/">小说</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/影评/">影评</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/数学/">数学</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/游戏/">游戏</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/随感/">随感</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-star-o"> 标签</i></div><div class="tagcloud"><a href="/tags/随感，毕业/" style="font-size: 15px;">随感，毕业</a> <a href="/tags/开发/" style="font-size: 15px;">开发</a> <a href="/tags/linux-学习/" style="font-size: 15px;">linux 学习</a> <a href="/tags/学习/" style="font-size: 15px;">学习</a> <a href="/tags/数学/" style="font-size: 15px;">数学</a> <a href="/tags/图像处理数学原理/" style="font-size: 15px;">图像处理数学原理</a> <a href="/tags/效率/" style="font-size: 15px;">效率</a> <a href="/tags/机器学习/" style="font-size: 15px;">机器学习</a> <a href="/tags/机器学习，图像处理/" style="font-size: 15px;">机器学习，图像处理</a> <a href="/tags/tensorflow学习/" style="font-size: 15px;">tensorflow学习</a> <a href="/tags/读书笔记/" style="font-size: 15px;">读书笔记</a> <a href="/tags/你好疯子，影评/" style="font-size: 15px;">你好疯子，影评</a> <a href="/tags/图像处理/" style="font-size: 15px;">图像处理</a> <a href="/tags/openMVG-openMVS学习/" style="font-size: 15px;">openMVG openMVS学习</a> <a href="/tags/随感/" style="font-size: 15px;">随感</a> <a href="/tags/喜剧之王，影评/" style="font-size: 15px;">喜剧之王，影评</a> <a href="/tags/The-Legend-of-1900/" style="font-size: 15px;">The Legend of 1900</a> <a href="/tags/随感-摄影测量/" style="font-size: 15px;">随感-摄影测量</a> <a href="/tags/ArcGIS环境配置/" style="font-size: 15px;">ArcGIS环境配置</a> <a href="/tags/校正方法，控制点，光束法平差/" style="font-size: 15px;">校正方法，控制点，光束法平差</a> <a href="/tags/linux学习/" style="font-size: 15px;">linux学习</a> <a href="/tags/月亮与六便士/" style="font-size: 15px;">月亮与六便士</a> <a href="/tags/电影十二公民/" style="font-size: 15px;">电影十二公民</a> <a href="/tags/小说/" style="font-size: 15px;">小说</a> <a href="/tags/V字仇杀队-浪潮，影评/" style="font-size: 15px;">V字仇杀队,浪潮，影评</a> <a href="/tags/随感－代码重构/" style="font-size: 15px;">随感－代码重构</a> <a href="/tags/R-学习/" style="font-size: 15px;">R 学习</a> <a href="/tags/爱乐之城，影评/" style="font-size: 15px;">爱乐之城，影评</a> <a href="/tags/狗子日记/" style="font-size: 15px;">狗子日记</a> <a href="/tags/一个叫欧维的男人决定去死，书评/" style="font-size: 15px;">一个叫欧维的男人决定去死，书评</a> <a href="/tags/星际穿越，影评/" style="font-size: 15px;">星际穿越，影评</a> <a href="/tags/图像处理的数学原理/" style="font-size: 15px;">图像处理的数学原理</a> <a href="/tags/社交网络，影评/" style="font-size: 15px;">社交网络，影评</a> <a href="/tags/秒速五厘米/" style="font-size: 15px;">秒速五厘米</a> <a href="/tags/海涛之声，影评/" style="font-size: 15px;">海涛之声，影评</a> <a href="/tags/白日梦想家，影评/" style="font-size: 15px;">白日梦想家，影评</a> <a href="/tags/饥荒/" style="font-size: 15px;">饥荒</a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-file-o"> 最新文章</i></div><ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/2018/08/22/tensorflow-二十七弹/">tensorflow-二十七弹</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/08/13/tensorflow-二十六弹/">tensorflow-二十六弹</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/07/22/RPC（共线条件方程）校正迭代方法分析/">RPC（共线条件方程）校正迭代方法分析</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/07/15/混合高斯分布/">混合高斯分布</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/07/08/python修饰符/">python修饰符</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/07/04/向往的生活/">向往的生活</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/06/25/空房子/">空房子</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/06/20/Flask服务器linux离线配置/">Flask服务器linux离线配置</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/06/17/月亮与六便士/">月亮与六便士</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/06/17/狗子日记十一月四号/">狗子日记十一月四号</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-external-link"> 友情链接</i></div><ul></ul><a href="https://github.com/RemoteSensingFrank" title="Github" target="_blank">Github</a><ul></ul><a href="http://www.example2.com/" title="site-name2" target="_blank">site-name2</a><ul></ul><a href="http://www.example3.com/" title="site-name3" target="_blank">site-name3</a></div></div></div><div class="pure-u-1 pure-u-md-3-4"><div id="footer">© <a href="/." rel="nofollow">吴蔚.转载请注明</a></div></div></div><a id="rocket" href="#top" class="show"></a><script type="text/javascript" src="/js/totop.js?v=0.0.0" async></script><script type="text/javascript" src="//cdn.bootcss.com/fancybox/2.1.5/jquery.fancybox.pack.js" async></script><script type="text/javascript" src="/js/fancybox.js?v=0.0.0" async></script><link rel="stylesheet" type="text/css" href="/css/jquery.fancybox.css?v=0.0.0"><script>(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
ga('create','UA-107160167-1','auto');ga('send','pageview');
</script><script type="text/javascript" src="/js/codeblock-resizer.js?v=0.0.0"></script><script type="text/javascript" src="/js/smartresize.js?v=0.0.0"></script></div></body></html>