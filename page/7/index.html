<!DOCTYPE html><html lang="zh-CN"><head><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description" content="随感，遥感，机器学习....想到什么写什么"><title>吴蔚 | 生命不息，折腾不止！</title><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/normalize/5.0.0/normalize.min.css"><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/pure/0.6.2/pure-min.css"><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/pure/0.6.2/grids-responsive-min.css"><link rel="stylesheet" type="text/css" href="/css/style.css?v=0.0.0"><link rel="stylesheet" href="//cdn.bootcss.com/font-awesome/4.7.0/css/font-awesome.min.css"><script type="text/javascript" src="//cdn.bootcss.com/jquery/3.1.1/jquery.min.js"></script><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.ico"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/apple-touch-icon.png"><link rel="alternate" type="application/atom+xml" href="/atom.xml"></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">吴蔚</h1><a id="logo" href="/.">吴蔚</a><p class="description">生命不息，折腾不止！</p></div><div id="nav-menu"><a href="/." class="current"><i class="fa fa-home"> 首页</i></a><a href="/archives/"><i class="fa fa-archive"> 归档</i></a><a href="/about/"><i class="fa fa-user"> 关于</i></a><a href="/atom.xml"><i class="fa fa-rss"> 订阅</i></a></div></div><div id="layout" class="pure-g"><div class="pure-u-1 pure-u-md-3-4"><div class="content_container"><div class="post"><h2 class="post-title"><a href="/2017/09/22/tensorflow-十六弹/">tensorflow-十六弹</a></h2><div class="post-meta">2017-09-22</div><div class="post-content"><p>时隔这么久，又回到了深度学习，其实这一段时间一直有在学习，不过都不系统，想要根据自己的数据训练出一个学习库针对需要的应用进行学习，但是实际上数据获取了，但是数据格式的转换把我难住了，如何进行数据格式的转换，使得转换后的数据格式满足tensorflow/Models的输入需要是要面临的巨大问题，另外数据的标注也是一个不得不面对的问题，由于需要的数据量比较大，则对数据进行标注也是一个很耗费时间的工作，所以就先放一放了，计划是采用Fast-RCNN进行目标的检测，实际上结合了RNN和CNN的深度网络能够 比较高校和准确的在影像上识别出目标，而且需要的训练样本相对较少，在应用中具有很大的优势，但是这次我们从基础开始，讲一讲传说中的RNN。<br>经过以上一段时间的学习相信大家对CNN都有所了解，主要分为：<br>$$[输入]\to[卷积，池化，映射]^n \to [全连接]^n\to[输出]$$<br>简单的来说包含输入层，中间的卷积层池化层，全连接层以及输出层这几个部分，其中卷积池化层可以多次进行，全连接层实际上也是一个神经网络，由此构建了一个深度CNN网络，在CNN中有一个重要的概念就是权值共享，通过权值共享缩小了参数规模，使得学习规模变小，从而使得深度学习成为可能。<br>这一次我们所提的RNN(循环神经网络)和CNN在理念上存在着一些相似性，具体的东西在下面会交代，实际上RNN主要用于解决输入数据存在着序列相关的问题，在CNN的过程中每次输入都是独立的，因此每一层只与其上一层和下一层有关而在RNN中每一层不仅与上一层和下一层有关，另外还与本层的上一次输入有关，由此使得RNN能够对序列数据进行处理，使得RNN在NLP上具有重要的应用。<br>RNN模型图：<br><img src="https://lh3.googleusercontent.com/-sD4LDKBV2CQ/WcSD4gBW8XI/AAAAAAAACX0/OOlvuN52Ve0RSHr-9O3YJLV3ok89t286gCLcBGAs/s0/rnn.png" alt="RNN模型" title="rnn.png"><br>通过以上模型可以看出整个RNN的运行过程，左侧为pack的模型，右侧为unpack的模型，上图的模型中$x_i$为输入数据$A$为隐含层的网络单元结构，$h_i$为输出数据，我们看到每一个隐含层的网络单元结构不仅要与输入和输出相连还要与它本身相连，由此构成了一个循环神经网络。<br>实际上在应用过程中，简单的RNN由于其结构比较简单，在进行循环的过程中通过上一次的输入调整$A$然而实际上在网络深度较大的情况下，早期的数据已经无法对$A$产生影响，或者说影响已经被抹去了，因此传统简单的RNN神经元只有短时记忆而不具备长期记忆功能，对于深度较大的应用来说有着明显的缺陷，因此需要对神经元的结构进行调整，由此产生了LSTM结构：<br><img src="https://lh3.googleusercontent.com/-IO7_OIIy5kA/WcS1fjQGSCI/AAAAAAAACYI/-LKRPcuDwQIG2W5-s2CQ--Cd8JSiv7LNQCLcBGAs/s0/LSTM.png" alt="简单神经元与LSTM对比" title="LSTM.png"><br>从以上对比可以看出LSTM在神经元的循环过程中有着更复杂的结构,另外由于LSTM能够克服传统RNN无法处理的短时记忆的问题，因此在网络中同时包含了短期记忆和长期记忆的两种情况，下面我们仔细谈谈LSTM的神经元结构：<br><img src="https://lh3.googleusercontent.com/-KRiyDpdgBwU/WcS3qvMeTEI/AAAAAAAACYY/R5-UN4xOeNEZlY2yltpfWSP3trDQNxaBwCLcBGAs/s0/LSTMShort.png" alt="LSTM步骤详细分析" title="LSTMShort.png"><br>OK，这样的话我们先分析上面四幅图中的图a，图a指示的是从上次和输入获取的信息中无用的部分，就是需要放弃的部分，计算方法为：<br>$$f_t=\sigma(W_f\cdot[h_m,x_t]+b_f)(1)$$<br>在记忆过程中并不是所有的信息都是有意义的，放弃那些没有意义的输入对于进行长期的记忆具有重要的作用，图b表示获取需要存储的信息，这一里包含了两个部分：</p></div><p class="readmore"><a href="/2017/09/22/tensorflow-十六弹/">阅读更多</a></p></div><div class="post"><h2 class="post-title"><a href="/2017/09/20/公式反推/">公式反推</a></h2><div class="post-meta">2017-09-20</div><div class="post-content"><p>正常情况下通过前方交会求解公式为：<br>$$x=PX(1)$$<br>其中矩阵$P$包含内参矩阵，旋转和平移矩阵，即$P=K[R|t]$,在此情况下进行求解，则必须将像点坐标 $x$和空间点坐标$X$转换为齐次式，求解过程为：<br>根据公式$x=PX$,则有 $[x \times P]X=0$ 求解以上齐次线性方程组，通过SVD分解进行分解，在有两个同名点的情况下得到$V$矩阵为求解结果。<br>然而对于几何校正的应用来说，需要根据矩阵$P$计算地面点的坐标，然而$x=PX$中有三个未知数但是只有两个方程组，是一个欠定问题，无法进行求解，实际上在真实求解过程中一般都是假设地面的高程已知，在高程已知的情况下需要求解的参数就只有2个，能够通过以上方程进行求解了，在此探讨整个求解过程，虽然求解过程比较简单，但是在计算机视觉中似乎没有人提到应该如何进行求解。<br>我们将式（1）展开，则有:<br>$$<br>\begin{bmatrix}<br>x\\<br>y\\<br>1\<br>\end{bmatrix}=<br>\begin{bmatrix} a_1 &amp;a_2&amp;a_3&amp;a_4\\ b_1 &amp;b_2&amp;b_3&amp;b_4\\ c_1 &amp;c_2&amp;c_3&amp;c_4 \end{bmatrix}\cdot<br>\begin{bmatrix} X\\ Y\\ Z\\ 1 \end{bmatrix}<br>(2)<br>$$<br>根据式（2）可知三个线性方程组，现在目标在于在已知$Z$的情况下求解$X$和$Y$,因此我们可以将(2)式乘开，有：<br>$$<br>x=a_1X+a_2Y+a_3Z+a_4(3)\\<br>y=b_1X+b_2Y+b_3Z+b_4(4)\\<br>1=c_1X+c_2Y+c_3Z+c_4(5)\\<br>$$<br>实际上对于式(3)-(5)有用的能够用来计算的式子只有(3)(4),下面我们重点考虑(3)(4)式，在计算过程中由于矩阵$P$是已知的，因此$a_i,b_i,c_i$都是<br>已知值，$Z$也是已知的，则式(3)(4)可以转换为：<br>$$<br>x-a_3Za_4=a_1X+a_2Y(6)\\<br>y-b_3Z-b_4=b_1X+b_2Y(7)\\<br>$$<br>将上式写成矩阵的形式则有：<br>$$<br>\begin{bmatrix}\hat{x}\\ \hat{y}\end{bmatrix}=<br>\begin{bmatrix}a_1&amp;a_2\\ b_1&amp;b_2\end{bmatrix}\cdot<br>\begin{bmatrix}X\\ Y\end{bmatrix}(8)<br>$$<br>则直接对上式进行求解可以得到$X,Y$，以上都是在理想情况下的数学推到结果，得到的结论也是显而易见的，为了验证结论的正确性需要在程序中同时通过正解和反解的方式进行验证，经过思考发现实际上计算结果与式(3)-(5)有差异，实际上在计算过程中齐次坐标可能并不是１，因此将式(3)-(5)修改为：<br>$$<br>kx=a_1X+a_2Y+a_3Z+a_4(9)\\<br>ky=b_1X+b_2Y+b_3Z+b_4(10)\\<br>k=c_1X+c_2Y+c_3Z+c_4(11)<br>$$<br>其中$k$为比例系数，则采用x=(9)/(11),y=(10)/(11),在已知$P,Z$的情况下上式转换为：<br>$$<br>x=\frac{a_1X+a_2Y+a_3Z+a_4}{c_1X+c_2Y+c_3Z+c_4}(12)\\<br>y=\frac{b_1X+b_2Y+b_3Z+b_4}{c_1X+c_2Y+c_3Z+c_4}(13)\\<br>$$<br>由式(12)(13)解算得到Ｘ，Ｙ，最后求解结果用矩阵表示为：<br>$$<br>\begin{bmatrix}\hat{a_1}&amp; \hat{a_2} \\ \hat{b_1}&amp;\hat{b_2}\end{bmatrix}\cdot<br>\begin{bmatrix}X\\ Y\end{bmatrix}=<br>\begin{bmatrix}T1\\ T2\end{bmatrix}(14)<br>$$<br>求解式(14)可得$X,Y$</p></div><p class="readmore"><a href="/2017/09/20/公式反推/">阅读更多</a></p></div><div class="post"><h2 class="post-title"><a href="/2017/09/19/婚礼月/">婚礼月</a></h2><div class="post-meta">2017-09-19</div><div class="post-content"><p>今天周一，工作这么久没有一天像今天这样不想工作，总觉得应该做点别的东西让自己舒坦一下，可是也不是一件容易的事情，不过好在国企好像压力也不是那么大，总是还留着一点时间可以打打野，可以想想自己的事情，但是话又说回来了，我现在有什么好想得呢，是不是太矫情了一点。不过好像一直很矫情在这里也就不要再吐槽我了，至少我还是有些自知之明的…..不过看到QQ空间和朋友圈的状态还是有些感叹。<br>原来大家都已经到了应该和可以结婚的年龄了，谁说不是呢，十年前的时候我们进高中，觉得大学是距离我们多么遥远的事情，大学是多么幸福高中时多么痛苦，不过有些事情也是只有经历过了才知道，其中的种种都是难以言说的。不过随着时间的推移，什么事情都在慢慢变化，或是变得厚重或是变得轻松，有在意的变得不在意，有不在意的变得在意，有更加深厚的友谊，有渐渐少去的问候。有朋友在大城市的CBD奋斗，也有朋友过上了陪孩子的美好，总之我们都在变化，都在面临成长和选择，没有人能够拒绝时间，所以只能咬着牙接受了。毕业工作了几个月，不知道自己会面临怎么样的选择，也不知道自己会做出怎么样的选择，只是想着，如果年轻的时候不努力一把，也许真的无法坦然面对以后碌碌无为的自己，好了说了这么多不过就是心理矛盾罢了，大概都是大家晒婚礼的照片晒的吧。<br>九月十月难道是传中的结婚月，很多的同学朋友选择在这两个月结婚，或许是十一长假的缘故吧，大家都选在这个时候，其实也挺好的至少能借着这个机会聚聚吧，毕竟总是聚少离多，朋友们婚礼都送上最诚挚的祝福，希望他们在面对生活的琐碎的时候都能够安静坦然，不知道生活应该怎么样才会快乐，不过我想如果被琐碎的事情麻烦而变得无法沟通应该是不会快乐的吧，我倒是很相信我的小伙伴们，他们都是有着生活智慧的人，相信他们了解自己做出的选择，也相信他们能够简直自己的选择获得幸福。<br>平时也不是这样，可能是这个周末没有休息好，而今天也有些累了，这样的身体上的疲劳会让自己的精神变得松懈然后失去奋斗的动力吧，不过让我真正的休息的时候又会有一种无所适从的紧迫感，总是感觉一天不学习不奋斗就会滑入堕落的深渊，就会被别人甩在身后。也不知道这样的焦虑是从何而来，可是他就是在这里，在我的脑海里，像恶魔一样折磨着我，在我放松的时候，在我休息的时候，总是有一个声音告诉我应该努力，这么去玩耍浪费光阴永远都只能是一个loser，这个念头让我很紧张，让我变得只想逃避，想躲到一个无人的孤岛过每天只为口腹奋斗，然后等待着明天的到来，这样多好是不是，感觉这样的焦虑时间长了会变成精神分裂的。<br>有时候很希望有一个人跟我说，其实你不用这么奋斗，你其实已经超过大家的期望了，你就这样就好，可是并没有，大家总是在说你可以做得更好，你应该做得更好，如果你…那么你肯定能够做得更好，这样我觉得这不是鼓励，这已经变成了煎熬了，为什么就没有人跟我说其实我待在老家挣个差不多的工资，开开心心的就好呢！所以我只能选择更大的城市，选择更大的平台，在这里不停的奋斗，生怕一旦有了松懈的念头就会灰溜溜的回去，让所有人失望，让所有人觉得其实我已经无法做的更好了，其实已经超过我的能力了，我可以么？所以只能羡慕别人的悠闲生活然后回过头来自己继续吧。</p></div><p class="readmore"><a href="/2017/09/19/婚礼月/">阅读更多</a></p></div><div class="post"><h2 class="post-title"><a href="/2017/09/14/无人机影像处理代码效果展示/">无人机影像处理代码效果展示</a></h2><div class="post-meta">2017-09-14</div><div class="post-content"><h2 id="光束法平差结果展示："><a href="#光束法平差结果展示：" class="headerlink" title="光束法平差结果展示："></a>光束法平差结果展示：</h2><p><hr>Dataset info:<br> #views: 10<br> #poses: 10<br> #intrinsics: 10<br> #tracks: 2920<br> #residuals: 11216<br><hr><table border="1"><tr><td>IdView</td><td>Basename</td><td>#Observations</td><td>Residuals min</td><td>Residuals median</td><td>Residuals mean</td><td>Residuals max</td></tr><tr><td>0</td><td>DSC01681</td><td>600</td><td>0.00083602</td><td>0.407472</td><td>0.578636</td><td>3.7219</td></tr><tr><td>1</td><td>DSC01682</td><td>919</td><td>8.32022e-05</td><td>0.458603</td><td>0.615874</td><td>3.6788</td></tr><tr><td>2</td><td>DSC01683</td><td>1088</td><td>0.000115459</td><td>0.417227</td><td>0.569578</td><td>3.73954</td></tr><tr><td>3</td><td>DSC01684</td><td>1287</td><td>0.000208581</td><td>0.420897</td><td>0.597463</td><td>3.63912</td></tr><tr><td>4</td><td>DSC01685</td><td>1425</td><td>0.000283019</td><td>0.429967</td><td>0.606205</td><td>4.13164</td></tr><tr><td>5</td><td>DSC01686</td><td>1521</td><td>0.00038597</td><td>0.435394</td><td>0.597527</td><td>3.88993</td></tr><tr><td>6</td><td>DSC01687</td><td>1521</td><td>5.28791e-06</td><td>0.420268</td><td>0.589389</td><td>3.96556</td></tr><tr><td>7</td><td>DSC01688</td><td>1309</td><td>0.000498331</td><td>0.433224</td><td>0.609728</td><td>4.3141</td></tr><tr><td>8</td><td>DSC01689</td><td>1103</td><td>3.36415e-05</td><td>0.437597</td><td>0.607724</td><td>3.97541</td></tr><tr><td>9</td><td>DSC01690</td><td>443</td><td>0.000554143</td><td>0.410359</td><td>0.574652</td><td>3.97483</td></tr></table><hr><br>Residuals histogram<br><img src="http://blogimage-1251632003.cosgz.myqcloud.com/residuals_histogram.png"></p></div><p class="readmore"><a href="/2017/09/14/无人机影像处理代码效果展示/">阅读更多</a></p></div><div class="post"><h2 class="post-title"><a href="/2017/09/10/周末在家/">周末在家</a></h2><div class="post-meta">2017-09-10</div><div class="post-content"><p>这个周末没有怎么加班，周六上午去做了一下晓玲让帮忙做的小功能，本来准备花一天时间去做的，万万没有想到最后一上午就做完了，然后去公司蹭顿中饭之后就默默回来了，回来之后什么都没有干睡了一觉看了两部电影，都是没有什么脑子的搞笑片，不知道为啥，在家呆着就是不太想做什么费脑子的事情，感觉在公司已经快把脑细胞耗尽了，在家什么都不想干，不过如果什么都不干又觉得特别害怕．真的特别害怕，害怕自己一天不学习，一天的放纵就会赶不上别人，就会堕落，也不知道这些压力从哪里来的．<br>昨天中午做梦，梦见自己在乡下老家建了一所房子，有小小的房子和大大的院子，有花有草有水有鱼，每天做着简单的工作然后回家打扫打扫卫生，做饭钓鱼，一切都是那么的美好，可是那不过都是梦而已，醒来之后脑袋特别痛，想着要不看看书吧，可是书上那些小字怎么都看不进去，脑袋里默默想起了海子的诗句＂有一所房子，面朝大海，春暖花开！＂可是我们怎么能够有一所房子面朝大海，春暖花开呢，不过是一场美丽的幻想罢了，回到现实中算一算如果自己要做一个自由职业者，要有一所美丽的房子那得需要多少钱呀，可能我这一辈子都挣不够这么多钱吧，回到现实中后果然心里冰凉冰凉的，在这个秋老虎还在肆虐的时候让我如坠冰窟．好吧，如果现实不如我们想象的美好应该怎么办呢？<br>有一个问题我不止一次的问过自己，如果重来一次我还希望做自己么？我想我是愿意做自己的，虽然我也希望自己生活在一个更加有钱的人家，最好是永远不需要担心钱的问题，可是要我因为钱放弃现在的自己似乎又不是那么愿意，虽然现在我有很多不完美，有很多假如的幻想，可是我对自己还是满意的吧，虽然我家里不是很有钱，可是也不曾缺少我的吃穿；虽然老爸老妈很难交流，可是也会倾听我的意见；找了一个自己目前为止还比较满意的工作，在工作上也没有什么解不开的难题，领导同事们也能够很好的相处，我想我应该是满意的吧．有时候快乐这种事情是无法言说的，我们总是把痛苦把槽点挂在嘴边，可是谁把快乐和幸福挂在嘴边呢，我想大概还比较满意，可是又有很多假如这才是生活的常态吧，生活本来就不完美，我们又何必要去苛求呢，如果对现在还比较满意那就好了吧．</p></div><p class="readmore"><a href="/2017/09/10/周末在家/">阅读更多</a></p></div><div class="post"><h2 class="post-title"><a href="/2017/09/10/tensorflow-十五弹/">tensorflow-十五弹</a></h2><div class="post-meta">2017-09-10</div><div class="post-content"><p>介绍了很多关于神经网络的理论的东西，当然理论的介绍还差得远，但是再介绍理论上的东西未免显得太无聊了，所以我们中间穿插一些关于tensorflow的小技巧的介绍以及相关算法的简单说明：</p>
<ul>
<li>自定义损失函数：在神经网络的学习过程中我们使用损失函数去判别学习好坏，实际上损失函数的定义有很多种，tensorflow中也内置了很多种类的损失函数，一般来说这些损失函数已经能够训练出比较好的效果了，但是对于实际问题我们常常会有着不同的要求，在此情况下所需的损失函数也会有所差异，在tensorflow中可以使用自定义的损失函数来进行约束，使得求解结果满足要求．示例代码如下：<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> numpy.random <span class="keyword">import</span> RandomState</span><br><span class="line"></span><br><span class="line"><span class="comment">#y=wx</span></span><br><span class="line">batch_size = <span class="number">8</span></span><br><span class="line">x = tf.placeholder(tf.float32,shape=(<span class="keyword">None</span>,<span class="number">2</span>),name=<span class="string">'x-input'</span>)</span><br><span class="line">y_= tf.placeholder(tf.float32,shape=(<span class="keyword">None</span>,<span class="number">1</span>),name=<span class="string">'y-input'</span>)</span><br><span class="line">w1= tf.Variable(tf.random_normal([<span class="number">2</span>,<span class="number">1</span>],stddev=<span class="number">1</span>,seed=<span class="number">1</span>))</span><br><span class="line">y = tf.matmul(x,w1);</span><br><span class="line"></span><br><span class="line"><span class="comment">#loss</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">loss_less = 10</span></span><br><span class="line"><span class="string">loss_more = 1</span></span><br><span class="line"><span class="string">loss = tf.reduce_sum(tf.where(tf.greater(y, y_), (y - y_) * loss_more, (y_ - y) * loss_less))</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">loss_less = 1</span></span><br><span class="line"><span class="string">loss_more = 10</span></span><br><span class="line"><span class="string">loss = tf.reduce_sum(tf.where(tf.greater(y, y_), (y - y_) * loss_more, (y_ - y) * loss_less))</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line">loss = tf.losses.mean_squared_error(y,y_)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">##</span></span><br><span class="line">train_step = tf.train.AdamOptimizer(<span class="number">0.001</span>).minimize(loss)</span><br><span class="line"></span><br><span class="line"><span class="comment">#data</span></span><br><span class="line">rdm = RandomState(<span class="number">1</span>)</span><br><span class="line">X = rdm.rand(<span class="number">128</span>,<span class="number">2</span>)</span><br><span class="line">Y = [[x1+x2+(rdm.rand()/<span class="number">10.0</span><span class="number">-0.05</span>)] <span class="keyword">for</span> (x1,x2) <span class="keyword">in</span> X]</span><br><span class="line"></span><br><span class="line"><span class="comment">#train</span></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    init_op = tf.global_variables_initializer()</span><br><span class="line">    sess.run(init_op)</span><br><span class="line"></span><br><span class="line">    STEPS = <span class="number">5000</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(STEPS):</span><br><span class="line">        start = (i*batch_size)%<span class="number">128</span></span><br><span class="line">        end   = (i*batch_size)%<span class="number">128</span>+batch_size</span><br><span class="line">        sess.run(train_step,feed_dict=&#123;x:X[start:end],y_:Y[start:end]&#125;)</span><br><span class="line">        <span class="keyword">if</span>(i%<span class="number">1000</span>==<span class="number">0</span>):</span><br><span class="line">            print(<span class="string">'After %d train step(s) ,w1 is: '</span>%(i))</span><br><span class="line">            <span class="keyword">print</span> sess.run(w1), <span class="string">'\n'</span></span><br><span class="line">    <span class="keyword">print</span> <span class="string">'finnal w1 is: \n'</span>,sess.run(w1)</span><br></pre></td></tr></table></figure></div><p class="readmore"><a href="/2017/09/10/tensorflow-十五弹/">阅读更多</a></p></div><div class="post"><h2 class="post-title"><a href="/2017/09/07/CAD二次开发问题集中解决/">CAD二次开发问题集中解决</a></h2><div class="post-meta">2017-09-07</div><div class="post-content"><p>使用了一段时间的AutoCAD后遇到了很多问题，感谢一些网上的大神给我提出了很多宝贵的意见和指导，虽然有时候会很麻烦，有时候会让我绞尽脑汁，有时候会很抓狂但是问题总是在一步一步的解决，当然啦对于新的帮助最大的还是万能的百度，但是有时候网上的东西写的比较简略或者由于版本过于陈旧而缺少了参考的价值，而自己获得了很多帮助所以也想着写一些东西出来为后来学习的人做贡献。<br>好了废话不多说，直接上干活，说说自己在开发的过程中所面临的问题以及自己的解决方式，当然我提出的解决方式可能不是最好的额，不过也可以让大家作为参考：  </p></div><p class="readmore"><a href="/2017/09/07/CAD二次开发问题集中解决/">阅读更多</a></p></div><div class="post"><h2 class="post-title"><a href="/2017/09/06/有罪或无罪/">有罪或无罪</a></h2><div class="post-meta">2017-09-06</div><div class="post-content"><p>什么都没有做就在找电影看，看到了＜十二公民＞这部电影，内心还有有一些震撼，电影的场景很简单，就是一群家长学生请来的家长或者是亲朋来开一个会模拟陪审团来决定一个影响很大的杀人事件中一个当事人是否有罪，案件比较简单，一位富二代从小被生父抛弃，后来长大后与生父发生争执，在争执过程中有证人说他听见富二代对他的生父说要＂我要杀了你＂另外还有一位证人从城际列车上看见杀人的过程，同时有位女证人是目击证人．刚开始的时候大家一致认为富二代是有罪的，除了一位，刚开始的时候大家都试图去说服他，可是随着案件进一步分析陪审团成员一个个被说服，然后该投了无罪的一个故事，全片的场景非常简单，在一个破体育馆内大家围成一个圈各自抒发自己的看法以及争执的过程．<br>虽然剧情很简单，但是其中哦你有很多值得思考的问题，有地域歧视，有仇富，有各种矛盾等，这些都是本片的亮点，不过让我最留下最深刻印象的却是整个推理的过程，看似简单的证词实际上有多么的矛盾，真相往往藏在无数谎言编织的外衣之下，而我们所说的话又能有多少是真的呢？老人的证词言之凿凿的说自己听见富二代和生父发生了争执，然后说出＂我要杀了你＂这句话，然后十几秒的时间之后他推开门看见富二代杀人后逃窜．可是通过案情分析我们发现并不是这样，到底老人是为什么说谎，是无意还是有意．在片中给我们的解释是老人想要获得他人的重视，一个生活艰难的老人，一辈子不被重视，不管说什么都没有人听没有人在意，好不容易又一个上电视的机会，他渴望被重视所以才用肯定的语气说出了那些自己都不确定的事情么？想想我们自己，是不是也曾是这样，为了面子，为了各种各样的原因说了许多我们自己都无法确定的事情，而这些事情往往有着重大的意义．另外那个目击姑娘的证词同样存在着这个问题，既然看不清到底谁是杀人犯，为什么会确定就是那个富二代呢？也许每一个人都渴望被重视，好不容易得到这个机会的情况下我们为了体现自己的重要性而往往言之凿凿的说出一些我们自己都不确定的事情．<br>另外，我们为什么一听别人的言论，一听所谓的证词就会去偏信一些事情呢？是因为我们有我们自己生活的背景，我们总是用我们自己生活的圈子来观察别人，总是会带着我们自己的偏见，而被偏见蒙蔽了双眼让我们无法去看清楚事实，实际上多数不一定是正确，我们需要有自己的判断，在面对问题，特别是重大问题的过程中我们不能够仅仅凭借着我们自己的经验去做出判断，但是很多时候我们就是这样做的，可是实际上我们做出的决定对当事人来说却是一万分的重要不是么？<br>看了这部电影还是有很大的触动的，其实那些关于社会关于各个阶层的区分以及他们对于事情不同的看法我倒是没有什么特别的感慨，反而是那些人在一步步走向真理的过程让我倍感幸福，是的我们每一个人都值得被保护，每一个真相都值得刨根问底的弄个明白，而我们也应该学会向真理低头．</p></div><p class="readmore"><a href="/2017/09/06/有罪或无罪/">阅读更多</a></p></div><div class="post"><h2 class="post-title"><a href="/2017/09/04/tensorflow-十四弹/">tensorflow-十四弹</a></h2><div class="post-meta">2017-09-04</div><div class="post-content"><p>在上一讲中我们已经结束了传统学习算法的学习，马上要转入深度学习，那么在这里就不得不进行一些铺垫的工作，为什么我们要提深度学习，传统的学习方法究竟在哪些地方不如深度学习，是否有可能这些缺点会被克服以及深度学习的所面对的问题．<br>总的来说，促使机器学习发展的部分原因在于传统学习算法对于高维，复杂函数的泛化性较差，在高维空间中的复杂函数的学习传统的学习方法往往面临着巨大的计算代价，而促进机器学习发展的主要有两点，１．维度灾难；２．流型学习．下面分别就这两个问题进行进一步的描述：</p></div><p class="readmore"><a href="/2017/09/04/tensorflow-十四弹/">阅读更多</a></p></div><div class="post"><h2 class="post-title"><a href="/2017/09/03/tensorflow-十三弹/">tensorflow-十三弹</a></h2><div class="post-meta">2017-09-03</div><div class="post-content"><p>上一讲中提到了一些机器学习的概念,现在我们讲讲机器学习的算法,机器学习的算法总的来说包括两大类,一类为监督学习方法,另一类为无监督学习方法,监督学习就是在具有训练样本的情况下对样本进行学习的过程,而无监督的学习方法针对没有训练样本的情况,下面就这两大类方法分别介绍:  </p>
<h2 id="1-监督学习方法"><a href="#1-监督学习方法" class="headerlink" title="1 监督学习方法"></a>1 监督学习方法</h2><h3 id="1-1逻辑回归方法"><a href="#1-1逻辑回归方法" class="headerlink" title="1.1逻辑回归方法:"></a>1.1逻辑回归方法:</h3><p>对于明确的概率分布　$p(y|x)$ 使用最大似然估计找到对于有参数分布族　$p(y|x;\theta)$　其中 $\theta$ 为参数向量，则线性回归族为：<br>$p(y|x;\theta)＝N(y;\theta^T x,I)$<br>进一步，对于分类问题来说，是求解ｘ属于某一个类别的概率，假设求解ｘ属于类别１的概率，则为：<br>$p(y=1|x;\theta)=\delta(\theta^T x)$<br>其中 $\delta$　为sigmoid函数，使用sigmoid函数的主要原因在于将线性变换的概率映射到０－１之间，实际上为什只求解ｘ属于类别１的概率，因为对于一个二类分类的结果来说，ｘ只有两种情况，在求解得到ｘ属于类别１的概率后ｘ属于类别２的概率也能够获取到了．</p></div><p class="readmore"><a href="/2017/09/03/tensorflow-十三弹/">阅读更多</a></p></div><nav class="page-navigator"><a class="extend prev" rel="prev" href="/page/6/">上一页</a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/5/">5</a><a class="page-number" href="/page/6/">6</a><span class="page-number current">7</span><a class="page-number" href="/page/8/">8</a><a class="page-number" href="/page/9/">9</a><span class="space">&hellip;</span><a class="page-number" href="/page/12/">12</a><a class="extend next" rel="next" href="/page/8/">下一页</a></nav></div></div><div class="pure-u-1-4 hidden_mid_and_down"><div id="sidebar"><div class="widget"><form action="//www.google.com/search" method="get" accept-charset="utf-8" target="_blank" class="search-form"><input type="text" name="q" maxlength="20" placeholder="Search"/><input type="hidden" name="sitesearch" value="http://www.wuweiblog.com"/></form></div><div class="widget"><div class="widget-title"><i class="fa fa-folder-o"> 分类</i></div><ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/书评/">书评</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/图像处理/">图像处理</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/学习/">学习</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/小说/">小说</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/影评/">影评</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/数学/">数学</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/游戏/">游戏</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/随感/">随感</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-star-o"> 标签</i></div><div class="tagcloud"><a href="/tags/喜剧之王，影评/" style="font-size: 15px;">喜剧之王，影评</a> <a href="/tags/开发/" style="font-size: 15px;">开发</a> <a href="/tags/学习/" style="font-size: 15px;">学习</a> <a href="/tags/ArcGIS环境配置/" style="font-size: 15px;">ArcGIS环境配置</a> <a href="/tags/图像处理数学原理/" style="font-size: 15px;">图像处理数学原理</a> <a href="/tags/数学/" style="font-size: 15px;">数学</a> <a href="/tags/机器学习/" style="font-size: 15px;">机器学习</a> <a href="/tags/效率/" style="font-size: 15px;">效率</a> <a href="/tags/tensorflow学习/" style="font-size: 15px;">tensorflow学习</a> <a href="/tags/机器学习，图像处理/" style="font-size: 15px;">机器学习，图像处理</a> <a href="/tags/随感/" style="font-size: 15px;">随感</a> <a href="/tags/Mary-and-Max，影评/" style="font-size: 15px;">Mary and Max，影评</a> <a href="/tags/将夜-书评/" style="font-size: 15px;">将夜,书评</a> <a href="/tags/沉默的大多数，书评/" style="font-size: 15px;">沉默的大多数，书评</a> <a href="/tags/未来简史-书评/" style="font-size: 15px;">未来简史,书评</a> <a href="/tags/断舍离，书评/" style="font-size: 15px;">断舍离，书评</a> <a href="/tags/雪中悍刀行-书评/" style="font-size: 15px;">雪中悍刀行,书评</a> <a href="/tags/潜规则-书评/" style="font-size: 15px;">潜规则,书评</a> <a href="/tags/呼兰河传-书评/" style="font-size: 15px;">呼兰河传,书评</a> <a href="/tags/你好疯子，影评/" style="font-size: 15px;">你好疯子，影评</a> <a href="/tags/图像处理/" style="font-size: 15px;">图像处理</a> <a href="/tags/openMVG-openMVS学习/" style="font-size: 15px;">openMVG openMVS学习</a> <a href="/tags/linux-学习/" style="font-size: 15px;">linux 学习</a> <a href="/tags/随感-摄影测量/" style="font-size: 15px;">随感-摄影测量</a> <a href="/tags/The-Legend-of-1900/" style="font-size: 15px;">The Legend of 1900</a> <a href="/tags/随感，毕业/" style="font-size: 15px;">随感，毕业</a> <a href="/tags/校正方法，控制点，光束法平差/" style="font-size: 15px;">校正方法，控制点，光束法平差</a> <a href="/tags/linux学习/" style="font-size: 15px;">linux学习</a> <a href="/tags/电影十二公民/" style="font-size: 15px;">电影十二公民</a> <a href="/tags/月亮与六便士/" style="font-size: 15px;">月亮与六便士</a> <a href="/tags/V字仇杀队-浪潮，影评/" style="font-size: 15px;">V字仇杀队,浪潮，影评</a> <a href="/tags/随感－代码重构/" style="font-size: 15px;">随感－代码重构</a> <a href="/tags/小说/" style="font-size: 15px;">小说</a> <a href="/tags/爱乐之城，影评/" style="font-size: 15px;">爱乐之城，影评</a> <a href="/tags/狗子日记/" style="font-size: 15px;">狗子日记</a> <a href="/tags/R-学习/" style="font-size: 15px;">R 学习</a> <a href="/tags/星际穿越，影评/" style="font-size: 15px;">星际穿越，影评</a> <a href="/tags/图像处理的数学原理/" style="font-size: 15px;">图像处理的数学原理</a> <a href="/tags/社交网络，影评/" style="font-size: 15px;">社交网络，影评</a> <a href="/tags/白日梦想家，影评/" style="font-size: 15px;">白日梦想家，影评</a> <a href="/tags/秒速五厘米/" style="font-size: 15px;">秒速五厘米</a> <a href="/tags/饥荒/" style="font-size: 15px;">饥荒</a> <a href="/tags/海涛之声，影评/" style="font-size: 15px;">海涛之声，影评</a> <a href="/tags/一个叫欧维的男人决定去死，书评/" style="font-size: 15px;">一个叫欧维的男人决定去死，书评</a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-file-o"> 最新文章</i></div><ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/2018/11/08/选择/">选择</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/10/31/不读书(七)/">不读书(七)</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/10/28/Cesium搭建自己的GIS服务器/">Cesium搭建自己的GIS服务器</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/10/14/关于PCA变换及其应用的梳理/">关于PCA变换及其应用的梳理</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/10/05/有趣的人/">有趣的人</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/09/08/一切随缘/">一切随缘</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/08/30/一座孤岛/">一座孤岛</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/08/29/不读书(六)/">不读书(六)</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/08/22/tensorflow-二十七弹/">tensorflow-二十七弹</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/08/13/tensorflow-二十六弹/">tensorflow-二十六弹</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-external-link"> 友情链接</i></div><ul></ul><a href="https://github.com/RemoteSensingFrank" title="Github" target="_blank">Github</a><ul></ul><a href="http://www.example2.com/" title="site-name2" target="_blank">site-name2</a><ul></ul><a href="http://www.example3.com/" title="site-name3" target="_blank">site-name3</a></div></div></div><div class="pure-u-1 pure-u-md-3-4"><div id="footer">© <a href="/." rel="nofollow">吴蔚.转载请注明</a></div></div></div><a id="rocket" href="#top" class="show"></a><script type="text/javascript" src="/js/totop.js?v=0.0.0" async></script><script type="text/javascript" src="//cdn.bootcss.com/fancybox/2.1.5/jquery.fancybox.pack.js" async></script><script type="text/javascript" src="/js/fancybox.js?v=0.0.0" async></script><link rel="stylesheet" type="text/css" href="/css/jquery.fancybox.css?v=0.0.0"><script>(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
ga('create','UA-107160167-1','auto');ga('send','pageview');
</script><script type="text/javascript" src="/js/codeblock-resizer.js?v=0.0.0"></script><script type="text/javascript" src="/js/smartresize.js?v=0.0.0"></script></div></body></html>